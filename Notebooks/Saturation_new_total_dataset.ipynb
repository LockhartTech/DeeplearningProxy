{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Saturation_new_total_dataset.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLl3zZJC2/mdrbbFrPPGVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-srm3018/DeeplearningProxy/blob/main/Notebooks/Saturation_new_total_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9PQE47vGAl_",
        "outputId": "7b8c601c-85f4-469f-b142-949e88bfda4b"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 22 19:49:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi27NRCrGRba",
        "outputId": "5573f270-9de9-456b-9989-9c544786ba2c"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "9D0wgpehaCVX",
        "outputId": "07062195-f8a7-4b76-b5d6-4ab7903790d2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0e741b3-4e0e-4e4c-8739-04150e131948\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0e741b3-4e0e-4e4c-8739-04150e131948\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving layers.py to layers (4).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers.py': b'\"\"\"Import required libraries and modules.\"\"\"\\r\\n\\r\\nimport tensorflow as tf\\r\\nfrom keras import backend as K\\r\\nfrom keras.engine.topology import Layer\\r\\nfrom keras.layers.merge import add\\r\\n# from keras.engine import InputSpec\\r\\nfrom keras.layers import InputSpec\\r\\nfrom keras.layers.core import Activation\\r\\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\\r\\nfrom keras.layers import BatchNormalization, ConvLSTM2D\\r\\nfrom keras.layers import TimeDistributed, Reshape, RepeatVector\\r\\nfrom keras import regularizers\\r\\n\\r\\n\\r\\nreg_weights = 0.00001\\r\\n\\r\\n\\r\\ndef conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride,\\r\\n                   padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(\"relu\")(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef time_conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                            strides=stride,\\r\\n                            padding=\\'same\\',\\r\\n                            kernel_regularizer=regularizers.\\r\\n                            l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(\"relu\"))(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    *default = (1,1)\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        a = BatchNormalization()(a)\\r\\n        a = Activation(\"relu\")(a)\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(a)\\r\\n        y = BatchNormalization()(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef time_res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        a = TimeDistributed(BatchNormalization())(a)\\r\\n        a = TimeDistributed(Activation(\"relu\"))(a)\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        y = TimeDistributed(BatchNormalization())(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef dconv_bn_nolinear(nb_filter, nb_row, nb_col, stride=(2, 2),\\r\\n                      activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = UnPooling2D(size=stride)(x)\\r\\n        x = ReflectionPadding2D(padding=(int(nb_row/2), int(nb_col/2)))(x)\\r\\n        x = Conv2D(nb_filter, (nb_row, nb_col), padding=\\'valid\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(activation)(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\ndef time_dconv_bn_nolinear(nb_filter, nb_row, nb_col,\\r\\n                           stride=(2, 2), activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create time convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = TimeDistributed(UnPooling2D(size=stride))(x)\\r\\n        x = TimeDistributed(ReflectionPadding2D(padding=(int(nb_row/2),\\r\\n                            int(nb_col/2))))(x)\\r\\n        x = TimeDistributed(Conv2D(nb_filter, (nb_row, nb_col),\\r\\n                                   padding=\\'valid\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(activation))(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\nclass ReflectionPadding2D(Layer):\\r\\n    \"\"\"class for reflectionPadding2D.\"\"\"\\r\\n\\r\\n    def __init__(self, padding=(1, 1), data_format=\"channels_last\", **kwargs):\\r\\n        \"\"\"\\r\\n        Construct class parameters.\\r\\n\\r\\n        parameters:\\r\\n        -------\\r\\n        padding\\r\\n        dim_ordering\\r\\n        \"\"\"\\r\\n        super(ReflectionPadding2D, self).__init__(**kwargs)\\r\\n\\r\\n        if data_format == \\'channels_last\\':\\r\\n            dim_ordering = K.image_data_format()\\r\\n\\r\\n        self.padding = padding\\r\\n        if isinstance(padding, dict):\\r\\n            if set(padding.keys()) <= {\\'top_pad\\', \\'bottom_pad\\',\\r\\n                                       \\'left_pad\\', \\'right_pad\\'}:\\r\\n                self.top_pad = padding.get(\\'top_pad\\', 0)\\r\\n                self.bottom_pad = padding.get(\\'bottom_pad\\', 0)\\r\\n                self.left_pad = padding.get(\\'left_pad\\', 0)\\r\\n                self.right_pad = padding.get(\\'right_pad\\', 0)\\r\\n            else:\\r\\n                raise ValueError(\\'Unexpected key\\'\\r\\n                                 \\'found in `padding` dictionary.\\'\\r\\n                                 \\'Keys have to be in {\"top_pad\", \"bottom_pad\",\\'\\r\\n                                 \\'\"left_pad\", \"right_pad\"}.\\'\\r\\n                                 \\'Found: \\' + str(padding.keys()))\\r\\n        else:\\r\\n            padding = tuple(padding)\\r\\n            if len(padding) == 2:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[0]\\r\\n                self.left_pad = padding[1]\\r\\n                self.right_pad = padding[1]\\r\\n            elif len(padding) == 4:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[1]\\r\\n                self.left_pad = padding[2]\\r\\n                self.right_pad = padding[3]\\r\\n            else:\\r\\n                raise TypeError(\\'`padding` should be tuple of int \\'\\r\\n                                \\'of length 2 or 4, or dict. \\'\\r\\n                                \\'Found: \\' + str(padding))\\r\\n\\r\\n        # if data_format not in {\\'channels_last\\'}:\\r\\n        #     raise ValueError(\\'data_format must be in {\"channels_last\"}.\\')\\r\\n        self.data_format = data_format\\r\\n        self.input_spec = [InputSpec(ndim=4)]\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call x to apply padding.\"\"\"\\r\\n        top_pad = self.top_pad\\r\\n        bottom_pad = self.bottom_pad\\r\\n        left_pad = self.left_pad\\r\\n        right_pad = self.right_pad\\r\\n\\r\\n        paddings = [[0, 0], [left_pad, right_pad],\\r\\n                    [top_pad, bottom_pad], [0, 0]]\\r\\n\\r\\n        return tf.pad(x, paddings, mode=\\'REFLECT\\', name=None)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"\\r\\n        Compute the shape of output.\\r\\n\\r\\n        Parameters:\\r\\n        --------\\r\\n        input_shape: Tuple\\r\\n        shape of input\\r\\n        \"\"\"\\r\\n        if self.data_format == \\'channels_last\\':\\r\\n            rows = input_shape[1] + self.top_pad + self.bottom_pad\\r\\n            cols = input_shape[2] + self.left_pad + self.right_pad\\r\\n\\r\\n            return (input_shape[0],\\r\\n                    rows,\\r\\n                    cols,\\r\\n                    input_shape[3])\\r\\n        else:\\r\\n            raise ValueError(\\'Invalid data_format:\\', self.data_format)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get the Configure.\"\"\"\\r\\n        config = {\\'padding\\': self.padding}\\r\\n        base_config = super(ReflectionPadding2D, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n\\r\\n\\r\\nclass UnPooling2D(UpSampling2D):\\r\\n    \"\"\"Unpool 2D from 2D upsampling.\"\"\"\\r\\n\\r\\n    def __init__(self, size=(2, 2)):\\r\\n        \"\"\"Construct size.\"\"\"\\r\\n        super(UnPooling2D, self).__init__(size)\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call th x data.\"\"\"\\r\\n        shapes = x.get_shape().as_list()\\r\\n        w = self.size[0] * shapes[1]\\r\\n        h = self.size[1] * shapes[2]\\r\\n        return tf.image.resize(x, (w, h))\\r\\n\\r\\n\\r\\nclass InstanceNormalize(Layer):\\r\\n    \"\"\"Normalization Instance of class.\"\"\"\\r\\n\\r\\n    def __init__(self, **kwargs):\\r\\n        \"\"\"Initialize the keyaarguments.\"\"\"\\r\\n        super(InstanceNormalize, self).__init__(**kwargs)\\r\\n        self.epsilon = 1e-3\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call mean and variance for normalization.\"\"\"\\r\\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\\r\\n        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute the shape of output.\"\"\"\\r\\n        return input_shape\\r\\n\\r\\n\\r\\nclass RepeatConv(Layer):\\r\\n    \"\"\"\\r\\n    Repeats the input n times.\\r\\n\\r\\n    Example:\\r\\n    -------\\r\\n        model = Sequential()\\r\\n        model.add(Dense(32, input_dim=32))\\r\\n        now: model.output_shape == (None, 32)\\r\\n        note: `None` is the batch dimension\\r\\n        model.add(RepeatVector(3))\\r\\n        now: model.output_shape == (None, 3, 32)\\r\\n\\r\\n    Arguments\\r\\n    ---------\\r\\n        n: integer, repetition factor.\\r\\n    Input shape\\r\\n    ----------\\r\\n        4D tensor of shape `(num_samples, w, h, c)`.\\r\\n    Output shape\\r\\n    -----------\\r\\n        5D tensor of shape `(num_samples, n, w, h, c)`.\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, n, **kwargs):\\r\\n        \"\"\"Initialize the class parameters.\"\"\"\\r\\n        super(RepeatConv, self).__init__(**kwargs)\\r\\n        self.n = n\\r\\n        self.input_spec = InputSpec(ndim=4)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute output shape.\"\"\"\\r\\n        return (input_shape[0], self.n, input_shape[1],\\r\\n                input_shape[2], input_shape[3])\\r\\n\\r\\n    def call(self, inputs):\\r\\n        \"\"\"Call the inputs.\"\"\"\\r\\n        x = K.expand_dims(inputs, 1)\\r\\n        pattern = tf.stack([1, self.n, 1, 1, 1])\\r\\n        return K.tile(x, pattern)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get configure.\"\"\"\\r\\n        config = {\\'n\\': self.n}\\r\\n        base_config = super(RepeatConv, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrWDL1qVfMw"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import unet_uae as vae_util\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf \n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zr0u9vQzf5R",
        "outputId": "05b62fe8-9064-4829-be9b-29e327f3cdd9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlbNGMtBA_Fs"
      },
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Function to load datasets in format .NPY\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    path : string\n",
        "        The absolute path of where data saved in local system\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    loaded_data : ndarray\n",
        "        The data which was loaded\n",
        "    \"\"\"\n",
        "    loaded_data = np.load(path)\n",
        "    return loaded_data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cErfklo76F21"
      },
      "source": [
        "# Two common methods for feature scaling is : 1-Normalization & 2-Standardaisation\n",
        "\n",
        "def normalize(data):\n",
        "    \"\"\"\n",
        "    this function used for Max-Min Normalization (Min-Max scaling) by re-scaling\n",
        "    features with a distribution value between 0 and 1. For every feature,the minimum\n",
        "    value of that feature gets transformed into 0, and the maximum value \n",
        "    gets transformed into 1\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    norm_data : ndarray\n",
        "        The normalized data which transformed into 0 and 1\n",
        "    \"\"\"\n",
        "    max_p = np.max(data[:, :, :, :])\n",
        "    min_p = np.min(data[:, :, :, :])\n",
        "    norm_data = (data - min_p)/(max_p - min_p)\n",
        "    return norm_data\n",
        "\n",
        "def standardize(data):\n",
        "    \"\"\"\n",
        "    this function used for rescaling faetures to ensure the mean\n",
        "    and the standard deviation to be 0 and 1, respectively.\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The standardized data which the mean\n",
        "    and the standard deviation to be 0 and 1\n",
        "    \"\"\"\n",
        "    data_mean = np.mean(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    data_std = np.std(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    std_data = (data - data_mean)/(data_std)\n",
        "    return std_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uviUO--fC_pp"
      },
      "source": [
        "# define the absolute path of training datatsat\n",
        "path_perm = '/content/gdrive/MyDrive/perm.npy'\n",
        "path_sat = '/content/gdrive/MyDrive/saturation.npy'\n",
        "# use load_data function nd above path to loading data\n",
        "X_data= load_data(path_perm)\n",
        "target_data = load_data(path_sat)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yq6Ouns_EIN",
        "outputId": "50383e3f-958d-4a74-a0de-7ee92e695657"
      },
      "source": [
        "target_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 10, 100, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5s3_Zex6sQs"
      },
      "source": [
        "# Normalize data using abov normalize function\n",
        "train_nr = 2250\n",
        "test_nr = 750"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q5C0284nFxJ",
        "outputId": "a16eba74-9403-4808-83b5-b9ba3964b1e9"
      },
      "source": [
        "input_shape=(100, 100, 2)\n",
        "depth = 10\n",
        "vae_model,_ = vae_util.create_vae(input_shape, depth)\n",
        "vae_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output shape is  (None, 10, 100, 100, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 100, 100, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 50, 50, 16)   304         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 50, 50, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 50, 50, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 50, 50, 32)   4640        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 50, 50, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 50, 50, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 25, 25, 64)   18496       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 25, 25, 128)  73856       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 25, 25, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 25, 25, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_4 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)       (None, 10, 25, 25, 1 1180160     repeat_conv_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 10, 25, 25, 1 147584      conv_lst_m2d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 10, 25, 25, 1 0           conv_lst_m2d[0][0]               \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 10, 25, 25, 1 147584      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 10, 25, 25, 1 0           add_3[0][0]                      \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_13 (TimeDistri (None, 10, 25, 25, 1 147584      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_14 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_3 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 10, 25, 25, 1 0           add_4[0][0]                      \n",
            "                                                                 time_distributed_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 25, 25, 2 0           repeat_conv_3[0][0]              \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_15 (TimeDistri (None, 10, 25, 25, 2 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 10, 27, 27, 2 0           time_distributed_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 10, 25, 25, 1 295040      time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_2 (RepeatConv)      (None, 10, 25, 25, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_19 (TimeDistri (None, 10, 25, 25, 1 0           time_distributed_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 25, 25, 1 0           repeat_conv_2[0][0]              \n",
            "                                                                 time_distributed_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_20 (TimeDistri (None, 10, 50, 50, 1 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_21 (TimeDistri (None, 10, 52, 52, 1 0           time_distributed_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_22 (TimeDistri (None, 10, 50, 50, 6 110656      time_distributed_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_23 (TimeDistri (None, 10, 50, 50, 6 256         time_distributed_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_1 (RepeatConv)      (None, 10, 50, 50, 3 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_24 (TimeDistri (None, 10, 50, 50, 6 0           time_distributed_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 50, 50, 9 0           repeat_conv_1[0][0]              \n",
            "                                                                 time_distributed_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_25 (TimeDistri (None, 10, 50, 50, 9 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_26 (TimeDistri (None, 10, 52, 52, 9 0           time_distributed_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_27 (TimeDistri (None, 10, 50, 50, 3 27680       time_distributed_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_28 (TimeDistri (None, 10, 50, 50, 3 128         time_distributed_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv (RepeatConv)        (None, 10, 50, 50, 1 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_29 (TimeDistri (None, 10, 50, 50, 3 0           time_distributed_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 50, 50, 4 0           repeat_conv[0][0]                \n",
            "                                                                 time_distributed_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_30 (TimeDistri (None, 10, 100, 100, 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_31 (TimeDistri (None, 10, 102, 102, 0           time_distributed_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_32 (TimeDistri (None, 10, 100, 100, 6928        time_distributed_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_33 (TimeDistri (None, 10, 100, 100, 64          time_distributed_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_34 (TimeDistri (None, 10, 100, 100, 0           time_distributed_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_35 (TimeDistri (None, 10, 100, 100, 145         time_distributed_34[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 2,164,113\n",
            "Trainable params: 2,162,385\n",
            "Non-trainable params: 1,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4PDSq-qnONZ",
        "outputId": "9448a228-d583-4e38-e242-f7db7a001d93"
      },
      "source": [
        "depth = 10\n",
        "nr = X_data.shape[0]\n",
        "train_nr = 2250\n",
        "test_nr = 750\n",
        "train_x = np.concatenate([X_data[:train_nr,[0], ...],target_data[:train_nr,[0], ...]], axis = 1)\n",
        "train_y = target_data[:train_nr, ...]\n",
        "\n",
        "test_x = np.concatenate([X_data[nr-test_nr:,[0], ...], target_data[nr-test_nr:, [0], ...]], axis = 1)\n",
        "test_y = target_data[nr-test_nr:,...]\n",
        "\n",
        "\n",
        "train_x = train_x.transpose(0,2,3,1)\n",
        "train_y = train_y[:,:,:,:,None]\n",
        "test_x = test_x.transpose(0,2,3,1)\n",
        "test_y = test_y[:,:,:,:,None]\n",
        "#test_y = test_y.transpose(0,2,3,1)\n",
        "print('train_x shape is ', train_x.shape)\n",
        "print('train_y shape is ', train_y.shape)\n",
        "print('test_x shape is ', test_x.shape)\n",
        "print('test_y shape is ', test_y.shape)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape is  (2250, 100, 100, 2)\n",
            "train_y shape is  (2250, 10, 100, 100, 1)\n",
            "test_x shape is  (750, 100, 100, 2)\n",
            "test_y shape is  (750, 10, 100, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsuLGQpniCs"
      },
      "source": [
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/saved_models/'\n",
        "epochs = 300\n",
        "batch_size = 8\n",
        "num_batch = int(train_nr/batch_size) "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EqIECRbnoTX"
      },
      "source": [
        "def vae_loss(x, t_decoded):\n",
        "    '''Total loss for the plain UAE'''\n",
        "    return K.mean(reconstruction_loss(x, t_decoded))\n",
        "\n",
        "\n",
        "def reconstruction_loss(x, t_decoded):\n",
        "    '''Reconstruction loss for the plain UAE'''\n",
        "\n",
        "    return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
        "\n",
        "def relative_error(x, t_decoded):\n",
        "    return K.mean(K.abs(x - t_decoded) / x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m46Y8eknsmA"
      },
      "source": [
        "opt = Adam(learning_rate=3e-3)\n",
        "vae_model.compile(loss = vae_loss, optimizer = opt, metrics = [relative_error])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lttghynSnzzT"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "lrScheduler = ReduceLROnPlateau(monitor = 'loss', factor = 0.5, patience = 15, cooldown = 1, verbose = 1, min_lr = 1e-6)\n",
        "filePath = 'saved-model-{epoch:03d}-{val_loss:.2f}.h5'\n",
        "checkPoint = ModelCheckpoint(filePath, monitor = 'val_loss', verbose = 1, save_best_only = False, \\\n",
        "                             save_weights_only = True, mode = 'auto', save_freq = 20)\n",
        "\n",
        "callbacks_list = [lrScheduler, checkPoint]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClmgwTn9n7Dv",
        "outputId": "f8459b8c-cf6f-403b-f1a4-af906f9cd5a0"
      },
      "source": [
        "history = vae_model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, \\\n",
        "                        verbose = 1, validation_data = (test_x, test_y))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "282/282 [==============================] - 113s 222ms/step - loss: 4285.1809 - relative_error: 0.2310 - val_loss: 1142.0175 - val_relative_error: 0.1773\n",
            "Epoch 2/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 281.7523 - relative_error: 0.0846 - val_loss: 682.7005 - val_relative_error: 0.1460\n",
            "Epoch 3/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 181.8478 - relative_error: 0.0618 - val_loss: 242.3855 - val_relative_error: 0.0841\n",
            "Epoch 4/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 144.1413 - relative_error: 0.0523 - val_loss: 219.3401 - val_relative_error: 0.0705\n",
            "Epoch 5/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 120.6083 - relative_error: 0.0457 - val_loss: 267.0657 - val_relative_error: 0.0827\n",
            "Epoch 6/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 115.6128 - relative_error: 0.0439 - val_loss: 236.9305 - val_relative_error: 0.0637\n",
            "Epoch 7/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 97.4689 - relative_error: 0.0388 - val_loss: 179.6339 - val_relative_error: 0.0496\n",
            "Epoch 8/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 84.1380 - relative_error: 0.0361 - val_loss: 192.0091 - val_relative_error: 0.0576\n",
            "Epoch 9/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 75.7777 - relative_error: 0.0327 - val_loss: 277.3298 - val_relative_error: 0.0640\n",
            "Epoch 10/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 70.6670 - relative_error: 0.0310 - val_loss: 83.2362 - val_relative_error: 0.0324\n",
            "Epoch 11/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 60.2051 - relative_error: 0.0277 - val_loss: 90.8393 - val_relative_error: 0.0344\n",
            "Epoch 12/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 57.7609 - relative_error: 0.0277 - val_loss: 65.5802 - val_relative_error: 0.0253\n",
            "Epoch 13/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 48.7392 - relative_error: 0.0240 - val_loss: 60.6246 - val_relative_error: 0.0285\n",
            "Epoch 14/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 49.4342 - relative_error: 0.0241 - val_loss: 90.8644 - val_relative_error: 0.0285\n",
            "Epoch 15/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 45.0337 - relative_error: 0.0222 - val_loss: 178.6366 - val_relative_error: 0.0433\n",
            "Epoch 16/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 45.1325 - relative_error: 0.0232 - val_loss: 50.0930 - val_relative_error: 0.0261\n",
            "Epoch 17/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 40.7622 - relative_error: 0.0215 - val_loss: 59.0036 - val_relative_error: 0.0338\n",
            "Epoch 18/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 39.2695 - relative_error: 0.0209 - val_loss: 86.4011 - val_relative_error: 0.0381\n",
            "Epoch 19/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 39.9091 - relative_error: 0.0210 - val_loss: 45.0187 - val_relative_error: 0.0199\n",
            "Epoch 20/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 36.4197 - relative_error: 0.0189 - val_loss: 55.5770 - val_relative_error: 0.0246\n",
            "Epoch 21/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 35.5403 - relative_error: 0.0192 - val_loss: 71.7715 - val_relative_error: 0.0265\n",
            "Epoch 22/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 34.7223 - relative_error: 0.0191 - val_loss: 47.4469 - val_relative_error: 0.0213\n",
            "Epoch 23/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 30.4083 - relative_error: 0.0162 - val_loss: 62.2472 - val_relative_error: 0.0250\n",
            "Epoch 24/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 33.2748 - relative_error: 0.0174 - val_loss: 53.9779 - val_relative_error: 0.0209\n",
            "Epoch 25/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 33.8547 - relative_error: 0.0202 - val_loss: 49.7290 - val_relative_error: 0.0216\n",
            "Epoch 26/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 31.4859 - relative_error: 0.0168 - val_loss: 39.5159 - val_relative_error: 0.0192\n",
            "Epoch 27/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 29.4568 - relative_error: 0.0169 - val_loss: 37.0150 - val_relative_error: 0.0147\n",
            "Epoch 28/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 27.8993 - relative_error: 0.0152 - val_loss: 31.4879 - val_relative_error: 0.0135\n",
            "Epoch 29/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 27.3007 - relative_error: 0.0150 - val_loss: 41.8953 - val_relative_error: 0.0144\n",
            "Epoch 30/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 28.6823 - relative_error: 0.0155 - val_loss: 31.6197 - val_relative_error: 0.0143\n",
            "Epoch 31/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 24.8493 - relative_error: 0.0137 - val_loss: 35.3203 - val_relative_error: 0.0146\n",
            "Epoch 32/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 23.3741 - relative_error: 0.0135 - val_loss: 81.0333 - val_relative_error: 0.0258\n",
            "Epoch 33/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 281.9840 - relative_error: 0.0718 - val_loss: 185.3389 - val_relative_error: 0.0566\n",
            "Epoch 34/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 111.8145 - relative_error: 0.0357 - val_loss: 161.6321 - val_relative_error: 0.0455\n",
            "Epoch 35/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 94.2441 - relative_error: 0.0313 - val_loss: 148.5322 - val_relative_error: 0.0383\n",
            "Epoch 36/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 79.5758 - relative_error: 0.0283 - val_loss: 266.9610 - val_relative_error: 0.0714\n",
            "Epoch 37/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 73.0535 - relative_error: 0.0262 - val_loss: 169.7254 - val_relative_error: 0.0370\n",
            "Epoch 38/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 114.6703 - relative_error: 0.0378 - val_loss: 192.5205 - val_relative_error: 0.0524\n",
            "Epoch 39/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 62.3524 - relative_error: 0.0230 - val_loss: 172.2502 - val_relative_error: 0.0392\n",
            "Epoch 40/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 54.1093 - relative_error: 0.0213 - val_loss: 163.8931 - val_relative_error: 0.0420\n",
            "Epoch 41/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 47.6342 - relative_error: 0.0196 - val_loss: 70.1106 - val_relative_error: 0.0209\n",
            "Epoch 42/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 41.8040 - relative_error: 0.0176 - val_loss: 49.8908 - val_relative_error: 0.0182\n",
            "Epoch 43/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 38.6589 - relative_error: 0.0168 - val_loss: 51.2958 - val_relative_error: 0.0197\n",
            "Epoch 44/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 37.9984 - relative_error: 0.0167 - val_loss: 49.7523 - val_relative_error: 0.0198\n",
            "Epoch 45/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 35.5106 - relative_error: 0.0165 - val_loss: 45.2854 - val_relative_error: 0.0195\n",
            "Epoch 46/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 33.5586 - relative_error: 0.0161 - val_loss: 38.9038 - val_relative_error: 0.0157\n",
            "Epoch 47/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 32.3594 - relative_error: 0.0157 - val_loss: 34.0889 - val_relative_error: 0.0128\n",
            "Epoch 48/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 29.1963 - relative_error: 0.0137 - val_loss: 36.5873 - val_relative_error: 0.0178\n",
            "Epoch 49/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 28.9865 - relative_error: 0.0138 - val_loss: 41.3894 - val_relative_error: 0.0132\n",
            "Epoch 50/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 27.9929 - relative_error: 0.0135 - val_loss: 37.1632 - val_relative_error: 0.0150\n",
            "Epoch 51/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 27.5133 - relative_error: 0.0137 - val_loss: 41.0574 - val_relative_error: 0.0182\n",
            "Epoch 52/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 26.8422 - relative_error: 0.0140 - val_loss: 39.5523 - val_relative_error: 0.0235\n",
            "Epoch 53/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 25.7589 - relative_error: 0.0144 - val_loss: 33.1788 - val_relative_error: 0.0134\n",
            "Epoch 54/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 25.2151 - relative_error: 0.0142 - val_loss: 31.0240 - val_relative_error: 0.0128\n",
            "Epoch 55/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 23.5218 - relative_error: 0.0122 - val_loss: 32.8733 - val_relative_error: 0.0141\n",
            "Epoch 56/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 25.1080 - relative_error: 0.0133 - val_loss: 41.7917 - val_relative_error: 0.0161\n",
            "Epoch 57/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 23.8893 - relative_error: 0.0135 - val_loss: 38.8341 - val_relative_error: 0.0180\n",
            "Epoch 58/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 21.0299 - relative_error: 0.0120 - val_loss: 42.4711 - val_relative_error: 0.0225\n",
            "Epoch 59/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 21.3400 - relative_error: 0.0124 - val_loss: 28.3006 - val_relative_error: 0.0102\n",
            "Epoch 60/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 21.0954 - relative_error: 0.0119 - val_loss: 31.5829 - val_relative_error: 0.0132\n",
            "Epoch 61/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 19.1937 - relative_error: 0.0113 - val_loss: 31.6958 - val_relative_error: 0.0186\n",
            "Epoch 62/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 19.4879 - relative_error: 0.0124 - val_loss: 37.9454 - val_relative_error: 0.0239\n",
            "Epoch 63/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 19.6204 - relative_error: 0.0130 - val_loss: 42.6824 - val_relative_error: 0.0133\n",
            "Epoch 64/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 18.2898 - relative_error: 0.0113 - val_loss: 34.7555 - val_relative_error: 0.0156\n",
            "Epoch 65/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 19.1635 - relative_error: 0.0115 - val_loss: 29.1397 - val_relative_error: 0.0132\n",
            "Epoch 66/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 16.4646 - relative_error: 0.0099 - val_loss: 32.4084 - val_relative_error: 0.0181\n",
            "Epoch 67/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 15.9912 - relative_error: 0.0102 - val_loss: 30.5471 - val_relative_error: 0.0138\n",
            "Epoch 68/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 15.7006 - relative_error: 0.0102 - val_loss: 37.4077 - val_relative_error: 0.0146\n",
            "Epoch 69/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 15.9652 - relative_error: 0.0102 - val_loss: 37.5264 - val_relative_error: 0.0231\n",
            "Epoch 70/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 16.2347 - relative_error: 0.0105 - val_loss: 27.4320 - val_relative_error: 0.0123\n",
            "Epoch 71/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 14.3819 - relative_error: 0.0096 - val_loss: 34.0907 - val_relative_error: 0.0145\n",
            "Epoch 72/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 13.3726 - relative_error: 0.0090 - val_loss: 35.5039 - val_relative_error: 0.0116\n",
            "Epoch 73/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 13.8974 - relative_error: 0.0095 - val_loss: 27.0083 - val_relative_error: 0.0108\n",
            "Epoch 74/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 13.5484 - relative_error: 0.0093 - val_loss: 29.0376 - val_relative_error: 0.0116\n",
            "Epoch 75/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 14.1424 - relative_error: 0.0098 - val_loss: 38.1972 - val_relative_error: 0.0186\n",
            "Epoch 76/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 13.2518 - relative_error: 0.0093 - val_loss: 31.0989 - val_relative_error: 0.0149\n",
            "Epoch 77/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 13.8130 - relative_error: 0.0099 - val_loss: 29.1330 - val_relative_error: 0.0126\n",
            "Epoch 78/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 12.2801 - relative_error: 0.0093 - val_loss: 27.5500 - val_relative_error: 0.0117\n",
            "Epoch 79/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.6926 - relative_error: 0.0083 - val_loss: 31.0069 - val_relative_error: 0.0190\n",
            "Epoch 80/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.7879 - relative_error: 0.0088 - val_loss: 33.9722 - val_relative_error: 0.0145\n",
            "Epoch 81/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.8179 - relative_error: 0.0087 - val_loss: 26.2762 - val_relative_error: 0.0099\n",
            "Epoch 82/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.2636 - relative_error: 0.0084 - val_loss: 34.0934 - val_relative_error: 0.0168\n",
            "Epoch 83/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.3618 - relative_error: 0.0088 - val_loss: 26.4627 - val_relative_error: 0.0120\n",
            "Epoch 84/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 10.7916 - relative_error: 0.0084 - val_loss: 29.9023 - val_relative_error: 0.0121\n",
            "Epoch 85/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.1352 - relative_error: 0.0083 - val_loss: 28.9896 - val_relative_error: 0.0148\n",
            "Epoch 86/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 10.5839 - relative_error: 0.0077 - val_loss: 27.3950 - val_relative_error: 0.0120\n",
            "Epoch 87/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 10.4295 - relative_error: 0.0079 - val_loss: 27.8475 - val_relative_error: 0.0122\n",
            "Epoch 88/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.0378 - relative_error: 0.0086 - val_loss: 25.4610 - val_relative_error: 0.0108\n",
            "Epoch 89/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 10.5010 - relative_error: 0.0086 - val_loss: 27.4028 - val_relative_error: 0.0110\n",
            "Epoch 90/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.0009 - relative_error: 0.0088 - val_loss: 27.4611 - val_relative_error: 0.0137\n",
            "Epoch 91/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 10.8356 - relative_error: 0.0087 - val_loss: 26.0297 - val_relative_error: 0.0087\n",
            "Epoch 92/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 11.0256 - relative_error: 0.0094 - val_loss: 28.5080 - val_relative_error: 0.0125\n",
            "Epoch 93/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 9.4944 - relative_error: 0.0076 - val_loss: 28.8034 - val_relative_error: 0.0166\n",
            "Epoch 94/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 8.9105 - relative_error: 0.0071 - val_loss: 25.2057 - val_relative_error: 0.0105\n",
            "Epoch 95/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 9.2014 - relative_error: 0.0073 - val_loss: 26.6869 - val_relative_error: 0.0091\n",
            "Epoch 96/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 9.0746 - relative_error: 0.0074 - val_loss: 25.7190 - val_relative_error: 0.0095\n",
            "Epoch 97/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 9.2427 - relative_error: 0.0083 - val_loss: 28.1617 - val_relative_error: 0.0139\n",
            "Epoch 98/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 12.2846 - relative_error: 0.0097 - val_loss: 28.1122 - val_relative_error: 0.0106\n",
            "Epoch 99/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 9.2516 - relative_error: 0.0075 - val_loss: 24.6721 - val_relative_error: 0.0094\n",
            "Epoch 100/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 8.5545 - relative_error: 0.0076 - val_loss: 26.0241 - val_relative_error: 0.0088\n",
            "Epoch 101/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 8.0850 - relative_error: 0.0068 - val_loss: 28.0819 - val_relative_error: 0.0129\n",
            "Epoch 102/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 8.4032 - relative_error: 0.0069 - val_loss: 25.7886 - val_relative_error: 0.0124\n",
            "Epoch 103/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 8.2548 - relative_error: 0.0070 - val_loss: 25.2245 - val_relative_error: 0.0086\n",
            "Epoch 104/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 8.6652 - relative_error: 0.0071 - val_loss: 28.0567 - val_relative_error: 0.0157\n",
            "Epoch 105/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 8.5145 - relative_error: 0.0080 - val_loss: 25.5148 - val_relative_error: 0.0095\n",
            "Epoch 106/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 8.6136 - relative_error: 0.0077 - val_loss: 27.7215 - val_relative_error: 0.0116\n",
            "Epoch 107/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 8.6606 - relative_error: 0.0077 - val_loss: 25.1747 - val_relative_error: 0.0093\n",
            "Epoch 108/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 9.2605 - relative_error: 0.0078 - val_loss: 48.1819 - val_relative_error: 0.0190\n",
            "Epoch 109/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 11.7053 - relative_error: 0.0099 - val_loss: 26.9046 - val_relative_error: 0.0102\n",
            "Epoch 110/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 8.7604 - relative_error: 0.0075 - val_loss: 25.4085 - val_relative_error: 0.0106\n",
            "Epoch 111/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.4069 - relative_error: 0.0063 - val_loss: 27.2942 - val_relative_error: 0.0117\n",
            "Epoch 112/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 7.2949 - relative_error: 0.0068 - val_loss: 27.1148 - val_relative_error: 0.0132\n",
            "Epoch 113/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.6259 - relative_error: 0.0070 - val_loss: 29.8786 - val_relative_error: 0.0129\n",
            "Epoch 114/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 7.9036 - relative_error: 0.0076 - val_loss: 27.8962 - val_relative_error: 0.0134\n",
            "Epoch 115/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.3665 - relative_error: 0.0063 - val_loss: 26.0355 - val_relative_error: 0.0111\n",
            "Epoch 116/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.7635 - relative_error: 0.0071 - val_loss: 25.7899 - val_relative_error: 0.0094\n",
            "Epoch 117/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 7.7356 - relative_error: 0.0072 - val_loss: 25.4339 - val_relative_error: 0.0097\n",
            "Epoch 118/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.6636 - relative_error: 0.0068 - val_loss: 25.9146 - val_relative_error: 0.0123\n",
            "Epoch 119/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.1005 - relative_error: 0.0065 - val_loss: 26.0488 - val_relative_error: 0.0094\n",
            "Epoch 120/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.8340 - relative_error: 0.0067 - val_loss: 28.1061 - val_relative_error: 0.0150\n",
            "Epoch 121/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.5366 - relative_error: 0.0074 - val_loss: 25.9458 - val_relative_error: 0.0092\n",
            "Epoch 122/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.5252 - relative_error: 0.0071 - val_loss: 26.5212 - val_relative_error: 0.0135\n",
            "Epoch 123/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.2574 - relative_error: 0.0070 - val_loss: 25.8106 - val_relative_error: 0.0084\n",
            "Epoch 124/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 6.6256 - relative_error: 0.0060 - val_loss: 25.2647 - val_relative_error: 0.0094\n",
            "Epoch 125/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 7.0444 - relative_error: 0.0066 - val_loss: 25.8570 - val_relative_error: 0.0087\n",
            "Epoch 126/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 6.9051 - relative_error: 0.0068 - val_loss: 28.1089 - val_relative_error: 0.0155\n",
            "Epoch 127/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 6.8686 - relative_error: 0.0062 - val_loss: 25.4924 - val_relative_error: 0.0090\n",
            "Epoch 128/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 6.5393 - relative_error: 0.0062 - val_loss: 26.8410 - val_relative_error: 0.0117\n",
            "Epoch 129/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 6.7514 - relative_error: 0.0061 - val_loss: 25.4448 - val_relative_error: 0.0089\n",
            "Epoch 130/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.8291 - relative_error: 0.0062 - val_loss: 31.2729 - val_relative_error: 0.0212\n",
            "Epoch 131/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 7.3096 - relative_error: 0.0073 - val_loss: 26.4033 - val_relative_error: 0.0090\n",
            "Epoch 132/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.7247 - relative_error: 0.0064 - val_loss: 25.2229 - val_relative_error: 0.0100\n",
            "Epoch 133/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.6921 - relative_error: 0.0065 - val_loss: 25.5953 - val_relative_error: 0.0092\n",
            "Epoch 134/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.3125 - relative_error: 0.0058 - val_loss: 27.0476 - val_relative_error: 0.0129\n",
            "Epoch 135/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.5834 - relative_error: 0.0066 - val_loss: 26.8089 - val_relative_error: 0.0103\n",
            "Epoch 136/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 7.2201 - relative_error: 0.0067 - val_loss: 25.3180 - val_relative_error: 0.0081\n",
            "Epoch 137/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.3843 - relative_error: 0.0060 - val_loss: 25.9149 - val_relative_error: 0.0109\n",
            "Epoch 138/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.6664 - relative_error: 0.0057 - val_loss: 26.9835 - val_relative_error: 0.0092\n",
            "Epoch 139/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.3224 - relative_error: 0.0059 - val_loss: 26.4945 - val_relative_error: 0.0118\n",
            "Epoch 140/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.8549 - relative_error: 0.0066 - val_loss: 25.4512 - val_relative_error: 0.0080\n",
            "Epoch 141/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.2223 - relative_error: 0.0061 - val_loss: 28.1327 - val_relative_error: 0.0152\n",
            "Epoch 142/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.3150 - relative_error: 0.0065 - val_loss: 25.5800 - val_relative_error: 0.0092\n",
            "Epoch 143/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.9447 - relative_error: 0.0059 - val_loss: 26.5355 - val_relative_error: 0.0125\n",
            "Epoch 144/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.1343 - relative_error: 0.0060 - val_loss: 25.4481 - val_relative_error: 0.0093\n",
            "Epoch 145/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.0524 - relative_error: 0.0059 - val_loss: 25.8532 - val_relative_error: 0.0117\n",
            "Epoch 146/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.8009 - relative_error: 0.0061 - val_loss: 25.9395 - val_relative_error: 0.0112\n",
            "Epoch 147/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.6459 - relative_error: 0.0059 - val_loss: 25.1462 - val_relative_error: 0.0077\n",
            "Epoch 148/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 6.1940 - relative_error: 0.0060 - val_loss: 25.1649 - val_relative_error: 0.0092\n",
            "Epoch 149/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.0368 - relative_error: 0.0058 - val_loss: 25.4427 - val_relative_error: 0.0096\n",
            "Epoch 150/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.7720 - relative_error: 0.0060 - val_loss: 25.1961 - val_relative_error: 0.0098\n",
            "Epoch 151/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.9375 - relative_error: 0.0061 - val_loss: 26.0627 - val_relative_error: 0.0115\n",
            "Epoch 152/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.0661 - relative_error: 0.0058 - val_loss: 25.5902 - val_relative_error: 0.0111\n",
            "Epoch 153/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.6929 - relative_error: 0.0057 - val_loss: 27.1888 - val_relative_error: 0.0121\n",
            "Epoch 154/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.9243 - relative_error: 0.0061 - val_loss: 25.7140 - val_relative_error: 0.0083\n",
            "Epoch 155/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.4422 - relative_error: 0.0054 - val_loss: 27.0901 - val_relative_error: 0.0101\n",
            "Epoch 156/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.7572 - relative_error: 0.0058 - val_loss: 28.9117 - val_relative_error: 0.0137\n",
            "Epoch 157/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.9166 - relative_error: 0.0063 - val_loss: 26.0530 - val_relative_error: 0.0098\n",
            "Epoch 158/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 6.0953 - relative_error: 0.0064 - val_loss: 26.3778 - val_relative_error: 0.0128\n",
            "Epoch 159/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.4482 - relative_error: 0.0056 - val_loss: 25.9776 - val_relative_error: 0.0095\n",
            "Epoch 160/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.3106 - relative_error: 0.0058 - val_loss: 27.6843 - val_relative_error: 0.0144\n",
            "Epoch 161/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 6.4165 - relative_error: 0.0065 - val_loss: 25.7543 - val_relative_error: 0.0091\n",
            "Epoch 162/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.6510 - relative_error: 0.0055 - val_loss: 26.3543 - val_relative_error: 0.0098\n",
            "Epoch 163/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.5535 - relative_error: 0.0059 - val_loss: 25.5740 - val_relative_error: 0.0085\n",
            "Epoch 164/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.0804 - relative_error: 0.0053 - val_loss: 26.8642 - val_relative_error: 0.0083\n",
            "Epoch 165/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.2754 - relative_error: 0.0056 - val_loss: 25.8762 - val_relative_error: 0.0098\n",
            "Epoch 166/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.0798 - relative_error: 0.0053 - val_loss: 26.3414 - val_relative_error: 0.0092\n",
            "Epoch 167/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.3254 - relative_error: 0.0056 - val_loss: 26.1592 - val_relative_error: 0.0097\n",
            "Epoch 168/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.6059 - relative_error: 0.0057 - val_loss: 24.7331 - val_relative_error: 0.0079\n",
            "Epoch 169/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.9818 - relative_error: 0.0055 - val_loss: 26.1508 - val_relative_error: 0.0125\n",
            "Epoch 170/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 5.6154 - relative_error: 0.0056 - val_loss: 25.3170 - val_relative_error: 0.0092\n",
            "Epoch 171/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.0651 - relative_error: 0.0055 - val_loss: 25.5597 - val_relative_error: 0.0087\n",
            "Epoch 172/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.0361 - relative_error: 0.0053 - val_loss: 25.9663 - val_relative_error: 0.0108\n",
            "Epoch 173/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.0808 - relative_error: 0.0057 - val_loss: 25.5143 - val_relative_error: 0.0087\n",
            "Epoch 174/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.5205 - relative_error: 0.0056 - val_loss: 26.9154 - val_relative_error: 0.0122\n",
            "Epoch 175/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.2828 - relative_error: 0.0059 - val_loss: 26.9814 - val_relative_error: 0.0128\n",
            "Epoch 176/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.1456 - relative_error: 0.0056 - val_loss: 25.4740 - val_relative_error: 0.0086\n",
            "Epoch 177/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.9231 - relative_error: 0.0052 - val_loss: 26.6247 - val_relative_error: 0.0101\n",
            "Epoch 178/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.9432 - relative_error: 0.0057 - val_loss: 24.9289 - val_relative_error: 0.0092\n",
            "Epoch 179/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 5.8644 - relative_error: 0.0062 - val_loss: 26.2283 - val_relative_error: 0.0119\n",
            "Epoch 180/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.3341 - relative_error: 0.0063 - val_loss: 25.2249 - val_relative_error: 0.0086\n",
            "Epoch 181/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 5.0495 - relative_error: 0.0052 - val_loss: 25.8444 - val_relative_error: 0.0100\n",
            "Epoch 182/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.8536 - relative_error: 0.0053 - val_loss: 25.1877 - val_relative_error: 0.0084\n",
            "Epoch 183/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.7885 - relative_error: 0.0050 - val_loss: 25.8240 - val_relative_error: 0.0081\n",
            "Epoch 184/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.8668 - relative_error: 0.0053 - val_loss: 25.4093 - val_relative_error: 0.0096\n",
            "Epoch 185/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.7115 - relative_error: 0.0051 - val_loss: 27.0415 - val_relative_error: 0.0134\n",
            "Epoch 186/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.9321 - relative_error: 0.0057 - val_loss: 25.8189 - val_relative_error: 0.0086\n",
            "Epoch 187/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.0073 - relative_error: 0.0053 - val_loss: 25.6519 - val_relative_error: 0.0097\n",
            "Epoch 188/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.7457 - relative_error: 0.0055 - val_loss: 26.4789 - val_relative_error: 0.0090\n",
            "Epoch 189/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 4.9466 - relative_error: 0.0050 - val_loss: 25.0920 - val_relative_error: 0.0090\n",
            "Epoch 190/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.6796 - relative_error: 0.0050 - val_loss: 26.0095 - val_relative_error: 0.0087\n",
            "Epoch 191/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 5.1447 - relative_error: 0.0057 - val_loss: 24.9113 - val_relative_error: 0.0075\n",
            "Epoch 192/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.8644 - relative_error: 0.0053 - val_loss: 25.8968 - val_relative_error: 0.0105\n",
            "Epoch 193/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.6470 - relative_error: 0.0050 - val_loss: 26.4286 - val_relative_error: 0.0096\n",
            "Epoch 194/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.8361 - relative_error: 0.0051 - val_loss: 25.8346 - val_relative_error: 0.0087\n",
            "Epoch 195/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.7805 - relative_error: 0.0051 - val_loss: 26.6798 - val_relative_error: 0.0106\n",
            "Epoch 196/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 4.7795 - relative_error: 0.0050 - val_loss: 26.2860 - val_relative_error: 0.0080\n",
            "Epoch 197/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.4872 - relative_error: 0.0050 - val_loss: 26.5264 - val_relative_error: 0.0083\n",
            "Epoch 198/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 5.2092 - relative_error: 0.0057 - val_loss: 25.8777 - val_relative_error: 0.0102\n",
            "Epoch 199/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.7903 - relative_error: 0.0053 - val_loss: 25.2707 - val_relative_error: 0.0084\n",
            "Epoch 200/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.4407 - relative_error: 0.0047 - val_loss: 25.3521 - val_relative_error: 0.0091\n",
            "Epoch 201/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 5.1532 - relative_error: 0.0051 - val_loss: 25.4793 - val_relative_error: 0.0082\n",
            "Epoch 202/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.7599 - relative_error: 0.0053 - val_loss: 25.1269 - val_relative_error: 0.0093\n",
            "Epoch 203/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.7375 - relative_error: 0.0050 - val_loss: 25.1704 - val_relative_error: 0.0080\n",
            "Epoch 204/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 4.5064 - relative_error: 0.0049 - val_loss: 25.2800 - val_relative_error: 0.0095\n",
            "Epoch 205/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.3814 - relative_error: 0.0048 - val_loss: 25.4716 - val_relative_error: 0.0086\n",
            "Epoch 206/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.3078 - relative_error: 0.0048 - val_loss: 25.8800 - val_relative_error: 0.0101\n",
            "Epoch 207/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.7094 - relative_error: 0.0054 - val_loss: 24.9691 - val_relative_error: 0.0079\n",
            "Epoch 208/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.6694 - relative_error: 0.0050 - val_loss: 25.5526 - val_relative_error: 0.0091\n",
            "Epoch 209/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.6559 - relative_error: 0.0049 - val_loss: 25.4054 - val_relative_error: 0.0081\n",
            "Epoch 210/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.3258 - relative_error: 0.0048 - val_loss: 25.7460 - val_relative_error: 0.0085\n",
            "Epoch 211/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.5554 - relative_error: 0.0051 - val_loss: 25.3952 - val_relative_error: 0.0095\n",
            "Epoch 212/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.1475 - relative_error: 0.0048 - val_loss: 26.6594 - val_relative_error: 0.0084\n",
            "Epoch 213/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.5058 - relative_error: 0.0050 - val_loss: 27.2666 - val_relative_error: 0.0112\n",
            "Epoch 214/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 5.4247 - relative_error: 0.0058 - val_loss: 25.5571 - val_relative_error: 0.0076\n",
            "Epoch 215/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.6008 - relative_error: 0.0049 - val_loss: 25.4206 - val_relative_error: 0.0083\n",
            "Epoch 216/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.3530 - relative_error: 0.0048 - val_loss: 26.9026 - val_relative_error: 0.0108\n",
            "Epoch 217/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.4352 - relative_error: 0.0048 - val_loss: 25.6127 - val_relative_error: 0.0100\n",
            "Epoch 218/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.6412 - relative_error: 0.0055 - val_loss: 25.6881 - val_relative_error: 0.0097\n",
            "Epoch 219/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.5493 - relative_error: 0.0051 - val_loss: 26.0612 - val_relative_error: 0.0078\n",
            "Epoch 220/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.5403 - relative_error: 0.0051 - val_loss: 25.8338 - val_relative_error: 0.0089\n",
            "Epoch 221/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.2828 - relative_error: 0.0051 - val_loss: 25.5189 - val_relative_error: 0.0088\n",
            "Epoch 222/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1579 - relative_error: 0.0046 - val_loss: 26.5417 - val_relative_error: 0.0091\n",
            "Epoch 223/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1743 - relative_error: 0.0047 - val_loss: 25.3057 - val_relative_error: 0.0079\n",
            "Epoch 224/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.6876 - relative_error: 0.0051 - val_loss: 25.8909 - val_relative_error: 0.0107\n",
            "Epoch 225/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4390 - relative_error: 0.0050 - val_loss: 25.8646 - val_relative_error: 0.0089\n",
            "Epoch 226/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.2634 - relative_error: 0.0048 - val_loss: 25.8604 - val_relative_error: 0.0081\n",
            "Epoch 227/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.2827 - relative_error: 0.0050 - val_loss: 25.1889 - val_relative_error: 0.0092\n",
            "Epoch 228/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0173 - relative_error: 0.0043 - val_loss: 25.4554 - val_relative_error: 0.0102\n",
            "Epoch 229/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.5066 - relative_error: 0.0051 - val_loss: 25.7136 - val_relative_error: 0.0101\n",
            "Epoch 230/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0892 - relative_error: 0.0046 - val_loss: 26.6491 - val_relative_error: 0.0086\n",
            "Epoch 231/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4904 - relative_error: 0.0051 - val_loss: 25.0341 - val_relative_error: 0.0083\n",
            "Epoch 232/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1769 - relative_error: 0.0049 - val_loss: 25.9255 - val_relative_error: 0.0088\n",
            "Epoch 233/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1752 - relative_error: 0.0047 - val_loss: 27.1062 - val_relative_error: 0.0092\n",
            "Epoch 234/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4669 - relative_error: 0.0052 - val_loss: 26.6181 - val_relative_error: 0.0103\n",
            "Epoch 235/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1209 - relative_error: 0.0046 - val_loss: 26.6831 - val_relative_error: 0.0094\n",
            "Epoch 236/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 4.5370 - relative_error: 0.0050 - val_loss: 25.6481 - val_relative_error: 0.0082\n",
            "Epoch 237/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.6705 - relative_error: 0.0049 - val_loss: 25.4784 - val_relative_error: 0.0095\n",
            "Epoch 238/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.6007 - relative_error: 0.0053 - val_loss: 25.5778 - val_relative_error: 0.0090\n",
            "Epoch 239/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9429 - relative_error: 0.0047 - val_loss: 25.3444 - val_relative_error: 0.0086\n",
            "Epoch 240/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.8697 - relative_error: 0.0046 - val_loss: 25.8984 - val_relative_error: 0.0101\n",
            "Epoch 241/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9929 - relative_error: 0.0046 - val_loss: 25.1483 - val_relative_error: 0.0077\n",
            "Epoch 242/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.6581 - relative_error: 0.0041 - val_loss: 26.2301 - val_relative_error: 0.0112\n",
            "Epoch 243/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1503 - relative_error: 0.0049 - val_loss: 26.1760 - val_relative_error: 0.0096\n",
            "Epoch 244/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.2665 - relative_error: 0.0047 - val_loss: 25.5480 - val_relative_error: 0.0097\n",
            "Epoch 245/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1193 - relative_error: 0.0045 - val_loss: 25.4970 - val_relative_error: 0.0082\n",
            "Epoch 246/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4154 - relative_error: 0.0050 - val_loss: 25.5067 - val_relative_error: 0.0080\n",
            "Epoch 247/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9230 - relative_error: 0.0045 - val_loss: 25.8441 - val_relative_error: 0.0082\n",
            "Epoch 248/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.8534 - relative_error: 0.0044 - val_loss: 27.7152 - val_relative_error: 0.0095\n",
            "Epoch 249/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0874 - relative_error: 0.0049 - val_loss: 25.3087 - val_relative_error: 0.0083\n",
            "Epoch 250/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1075 - relative_error: 0.0046 - val_loss: 26.0568 - val_relative_error: 0.0092\n",
            "Epoch 251/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0905 - relative_error: 0.0047 - val_loss: 25.8895 - val_relative_error: 0.0083\n",
            "Epoch 252/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.7562 - relative_error: 0.0042 - val_loss: 25.8728 - val_relative_error: 0.0077\n",
            "Epoch 253/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.2636 - relative_error: 0.0050 - val_loss: 26.4156 - val_relative_error: 0.0102\n",
            "Epoch 254/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4082 - relative_error: 0.0051 - val_loss: 25.4964 - val_relative_error: 0.0079\n",
            "Epoch 255/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.6706 - relative_error: 0.0041 - val_loss: 25.4385 - val_relative_error: 0.0080\n",
            "Epoch 256/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.7441 - relative_error: 0.0043 - val_loss: 25.3801 - val_relative_error: 0.0092\n",
            "Epoch 257/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.7927 - relative_error: 0.0046 - val_loss: 25.2550 - val_relative_error: 0.0086\n",
            "Epoch 258/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.7261 - relative_error: 0.0045 - val_loss: 25.5837 - val_relative_error: 0.0080\n",
            "Epoch 259/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.7472 - relative_error: 0.0043 - val_loss: 25.9277 - val_relative_error: 0.0094\n",
            "Epoch 260/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.8451 - relative_error: 0.0047 - val_loss: 25.5735 - val_relative_error: 0.0105\n",
            "Epoch 261/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1204 - relative_error: 0.0048 - val_loss: 25.5212 - val_relative_error: 0.0075\n",
            "Epoch 262/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9015 - relative_error: 0.0044 - val_loss: 26.0697 - val_relative_error: 0.0082\n",
            "Epoch 263/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.4346 - relative_error: 0.0047 - val_loss: 25.7300 - val_relative_error: 0.0104\n",
            "Epoch 264/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9029 - relative_error: 0.0045 - val_loss: 24.9713 - val_relative_error: 0.0083\n",
            "Epoch 265/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1988 - relative_error: 0.0051 - val_loss: 25.6599 - val_relative_error: 0.0094\n",
            "Epoch 266/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.8302 - relative_error: 0.0046 - val_loss: 26.0868 - val_relative_error: 0.0093\n",
            "Epoch 267/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9134 - relative_error: 0.0045 - val_loss: 25.6207 - val_relative_error: 0.0077\n",
            "Epoch 268/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 3.8471 - relative_error: 0.0046 - val_loss: 25.6456 - val_relative_error: 0.0093\n",
            "Epoch 269/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 4.7118 - relative_error: 0.0050 - val_loss: 685.8736 - val_relative_error: 0.0663\n",
            "Epoch 270/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 14.8344 - relative_error: 0.0103 - val_loss: 28.7066 - val_relative_error: 0.0128\n",
            "Epoch 271/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 5.8811 - relative_error: 0.0056 - val_loss: 26.9095 - val_relative_error: 0.0090\n",
            "Epoch 272/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 5.6313 - relative_error: 0.0057 - val_loss: 25.6955 - val_relative_error: 0.0086\n",
            "Epoch 273/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 4.5036 - relative_error: 0.0049 - val_loss: 25.7865 - val_relative_error: 0.0092\n",
            "Epoch 274/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0443 - relative_error: 0.0042 - val_loss: 25.7610 - val_relative_error: 0.0085\n",
            "Epoch 275/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1414 - relative_error: 0.0048 - val_loss: 26.0718 - val_relative_error: 0.0087\n",
            "Epoch 276/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.3166 - relative_error: 0.0047 - val_loss: 25.4263 - val_relative_error: 0.0088\n",
            "Epoch 277/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9860 - relative_error: 0.0045 - val_loss: 26.0043 - val_relative_error: 0.0087\n",
            "Epoch 278/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9511 - relative_error: 0.0044 - val_loss: 25.5169 - val_relative_error: 0.0076\n",
            "Epoch 279/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 4.1214 - relative_error: 0.0048 - val_loss: 25.4890 - val_relative_error: 0.0083\n",
            "Epoch 280/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0224 - relative_error: 0.0045 - val_loss: 25.2589 - val_relative_error: 0.0080\n",
            "Epoch 281/300\n",
            "282/282 [==============================] - 58s 207ms/step - loss: 3.9890 - relative_error: 0.0043 - val_loss: 26.1140 - val_relative_error: 0.0094\n",
            "Epoch 282/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.1328 - relative_error: 0.0049 - val_loss: 25.4803 - val_relative_error: 0.0080\n",
            "Epoch 283/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 3.9534 - relative_error: 0.0043 - val_loss: 25.8274 - val_relative_error: 0.0080\n",
            "Epoch 284/300\n",
            "282/282 [==============================] - 58s 206ms/step - loss: 4.0657 - relative_error: 0.0044 - val_loss: 26.4215 - val_relative_error: 0.0073\n",
            "Epoch 285/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1236 - relative_error: 0.0043 - val_loss: 25.5953 - val_relative_error: 0.0087\n",
            "Epoch 286/300\n",
            "282/282 [==============================] - 57s 204ms/step - loss: 4.1277 - relative_error: 0.0046 - val_loss: 25.5728 - val_relative_error: 0.0097\n",
            "Epoch 287/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.0294 - relative_error: 0.0046 - val_loss: 25.1911 - val_relative_error: 0.0091\n",
            "Epoch 288/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.9007 - relative_error: 0.0045 - val_loss: 26.3100 - val_relative_error: 0.0073\n",
            "Epoch 289/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 3.8826 - relative_error: 0.0043 - val_loss: 25.9767 - val_relative_error: 0.0096\n",
            "Epoch 290/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1296 - relative_error: 0.0045 - val_loss: 25.7809 - val_relative_error: 0.0098\n",
            "Epoch 291/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.8500 - relative_error: 0.0044 - val_loss: 25.3814 - val_relative_error: 0.0075\n",
            "Epoch 292/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.0565 - relative_error: 0.0044 - val_loss: 25.7549 - val_relative_error: 0.0088\n",
            "Epoch 293/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.8981 - relative_error: 0.0044 - val_loss: 25.5450 - val_relative_error: 0.0088\n",
            "Epoch 294/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.1282 - relative_error: 0.0046 - val_loss: 25.1617 - val_relative_error: 0.0080\n",
            "Epoch 295/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.8914 - relative_error: 0.0043 - val_loss: 26.5226 - val_relative_error: 0.0111\n",
            "Epoch 296/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.0003 - relative_error: 0.0045 - val_loss: 26.1915 - val_relative_error: 0.0095\n",
            "Epoch 297/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.9615 - relative_error: 0.0044 - val_loss: 25.9610 - val_relative_error: 0.0084\n",
            "Epoch 298/300\n",
            "282/282 [==============================] - 58s 204ms/step - loss: 4.1502 - relative_error: 0.0047 - val_loss: 26.0756 - val_relative_error: 0.0092\n",
            "Epoch 299/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 4.3620 - relative_error: 0.0049 - val_loss: 25.8160 - val_relative_error: 0.0079\n",
            "Epoch 300/300\n",
            "282/282 [==============================] - 58s 205ms/step - loss: 3.7447 - relative_error: 0.0041 - val_loss: 25.7105 - val_relative_error: 0.0085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "ZWrMqIqWce2j",
        "outputId": "a9bab34e-1af8-4fe0-a667-ea19d397bbb8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['relative_error'])\n",
        "plt.plot(history.history['val_relative_error'])\n",
        "plt.title('model relative error')\n",
        "plt.ylabel('relative error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'relative_error', 'val_loss', 'val_relative_error'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c9T1Xt39nRCFiBBAyQga0AYQNEou4CCiIoyjiP6G53RcWSEGdeZcQZn3HVQg2REQQRBR2YEDTs6soVFCBDIQkI6a2fp7qT3rnp+f5xT3dXVnXS609WVrv6+X4S6de6te59bN7lPnXPuPdfcHRERkb1JFDoAERE58ClZiIjIgJQsRERkQEoWIiIyICULEREZkJKFiIgMSMlCZJiZ2Y/N7F/2cdm1Zva2/V2PSL4pWYiIyICULEREZEBKFjImxeafq83sOTNrNrMbzWy6md1jZrvM7D4zm5S1/IVm9oKZNZjZQ2Y2P2ve8Wb2dPzcbUBFzrYuMLNn42f/aGbHDDHmj5jZKjPbYWZ3mdnMWG5m9k0z22pmTWb2vJkdHeedZ2Yvxtg2mNlnhvSFyZinZCFj2SXA24HDgXcA9wD/ANQS/m38DYCZHQ7cCnwqzrsb+B8zKzOzMuC/gZ8Ck4FfxPUSP3s8sAT4KDAF+CFwl5mVDyZQM3sr8G/AZcAMYB3w8zj7LOBNcT8mxGW2x3k3Ah9193HA0cADg9muSIaShYxl33X3Le6+Afg98Li7P+PubcCvgOPjcu8BfuPu97p7J/A1oBL4M+AUoBT4lrt3uvsdwJNZ27gK+KG7P+7uKXe/CWiPnxuM9wNL3P1pd28HrgVONbM5QCcwDjgSMHd/yd03xc91AgvMbLy773T3pwe5XRFAyULGti1Z0639vK+J0zMJv+QBcPc0sB6YFedt8N4jcq7Lmj4U+LvYBNVgZg3AwfFzg5Ebw25C7WGWuz8AfA/4T2CrmS02s/Fx0UuA84B1ZvawmZ06yO2KAEoWIvtiI+GkD4Q+AsIJfwOwCZgVyzIOyZpeD3zF3Sdm/aly91v3M4ZqQrPWBgB3/467nwgsIDRHXR3Ln3T3i4BphOay2we5XRFAyUJkX9wOnG9mi8ysFPg7QlPSH4FHgS7gb8ys1MzeBZyc9dkbgI+Z2RtjR3S1mZ1vZuMGGcOtwIfM7LjY3/GvhGaztWZ2Ulx/KdAMtAHp2KfyfjObEJvPmoD0fnwPMoYpWYgMwN1fBq4AvgtsI3SGv8PdO9y9A3gX8OfADkL/xi+zPrsM+AihmWgnsCouO9gY7gM+D9xJqM28Drg8zh5PSEo7CU1V24H/iPM+AKw1sybgY4S+D5FBMz38SEREBqKahYiIDEjJQkREBqRkISIiA1KyEBGRAZUUOoB8mDp1qs+ZM6fQYYiIjCpPPfXUNnev7W9eUSaLOXPmsGzZskKHISIyqpjZuj3NUzOUiIgMSMlCREQGpGQhIiIDKso+i/50dnZSV1dHW1tboUPJu4qKCmbPnk1paWmhQxGRIjFmkkVdXR3jxo1jzpw59B4gtLi4O9u3b6euro65c+cWOhwRKRJjphmqra2NKVOmFHWiADAzpkyZMiZqUCIycsZMsgCKPlFkjJX9FJGRM6aSxUBSaWdzYxst7V2FDkVE5ICiZJHF3dm6q42WzlRe1t/Q0MD1118/6M+dd955NDQ05CEiEZF9k7dkYWZLzGyrmS3PKvsPM1thZs+Z2a/MbGLWvGvNbJWZvWxmZ2eVnxPLVpnZNfmKdyTsKVl0de29JnP33XczceLEvS4jIpJP+axZ/Bg4J6fsXuBodz8GeAW4FsDMFhCe+nVU/Mz1ZpY0syThIfTnEp4t/N64bH7l6XlQ11xzDatXr+a4447jpJNO4owzzuDCCy9kwYKwSxdffDEnnngiRx11FIsXL+7+3Jw5c9i2bRtr165l/vz5fOQjH+Goo47irLPOorW1NT/Biohkyduls+7+iJnNySlbmvX2MeDSOH0R8HN3bwdeNbNV9DzHeJW7rwEws5/HZV/cn9i+/D8v8OLGpr4xAy3tXZSVJChNDi6PLpg5ni++46i9LnPdddexfPlynn32WR566CHOP/98li9f3n2J65IlS5g8eTKtra2cdNJJXHLJJUyZMqXXOlauXMmtt97KDTfcwGWXXcadd97JFVdcMahYRUQGq5B9Fn8B3BOnZwHrs+bVxbI9lfdhZleZ2TIzW1ZfXz+kgEb6GqKTTz65170Q3/nOdzj22GM55ZRTWL9+PStXruzzmblz53LccccBcOKJJ7J27dqRCldExrCC3JRnZv8IdAG3DNc63X0xsBhg4cKFe21I2lMNIJV2XtjYyIwJFdSOqxiu0Paourq6e/qhhx7ivvvu49FHH6Wqqoozzzyz33slysvLu6eTyaSaoURkRIx4sjCzPwcuABa5e+akvgE4OGux2bGMvZSPOuPGjWPXrl39zmtsbGTSpElUVVWxYsUKHnvssRGOTkRkz0Y0WZjZOcDfA29295asWXcBPzOzbwAzgXnAE4SWoXlmNpeQJC4H3pe3+OJrnvq3mTJlCqeddhpHH300lZWVTJ8+vXveOeecww9+8APmz5/PEUccwSmnnJKnKEREBi9vycLMbgXOBKaaWR3wRcLVT+XAvfEu48fc/WPu/oKZ3U7ouO4CPu7uqbieTwC/A5LAEnd/IV8xj4Sf/exn/ZaXl5dzzz339Dsv0y8xdepUli/vvhKZz3zmM8Men4hIf/J5NdR7+ym+cS/LfwX4Sj/ldwN3D2Noe5bvqoWIyCilO7j7oVwhItKbkkUWDb8nItI/JYscJaQwTxc6DBGRA4qSRRZLd7Eg8RoVnY2FDkVE5ICiZNGLerhFRPqjZJGtu9MiP8liqEOUA3zrW9+ipaVl4AVFRPJAyaIXy/r/8FOyEJHRqiBjQx24Yprw/NQssocof/vb3860adO4/fbbaW9v553vfCdf/vKXaW5u5rLLLqOuro5UKsXnP/95tmzZwsaNG3nLW97C1KlTefDBB/MSn4jInozNZHHPNbD5+X5mOHTsptJKoXSQAwke9AY497q9LpI9RPnSpUu54447eOKJJ3B3LrzwQh555BHq6+uZOXMmv/nNb4AwZtSECRP4xje+wYMPPsjUqVMHF5eIyDBQM1SBLF26lKVLl3L88cdzwgknsGLFClauXMkb3vAG7r33Xj772c/y+9//ngkTJhQ6VBGRMVqz2EsNwDc8Q2vZFMbVHpLXENyda6+9lo9+9KN95j399NPcfffdfO5zn2PRokV84QtfyGssIiIDUc0ihwOWpytns4coP/vss1myZAm7d+8GYMOGDWzdupWNGzdSVVXFFVdcwdVXX83TTz/d57MiIiNtbNYs9srI16Wz2UOUn3vuubzvfe/j1FNPBaCmpoabb76ZVatWcfXVV5NIJCgtLeX73/8+AFdddRXnnHMOM2fOVAe3iIw48zxd+VNICxcu9GXLlvUqe+mll5g/f/6An01tfJbWkonUTJuTp+hGxr7ur4hIhpk95e4L+5unZqg+NJygiEguJYscnvV/EREJxlSy2JcmN8fy1sE9UoqxaVFECmvMJIuKigq2b9++jyfS0XuydXe2b99ORcUgbyoUEdmLMXM11OzZs6mrq6O+vn6vy6UatpJKNFC2o32EIht+FRUVzJ49u9BhiEgRGTPJorS0lLlz5w64XN2XLmLrxGOZ/6lfjEBUIiKjw5hphtpXaRIkPFXoMEREDihKFjlSltRjVUVEcihZ5EiTwLyr0GGIiBxQ8pYszGyJmW01s+VZZZPN7F4zWxlfJ8VyM7PvmNkqM3vOzE7I+syVcfmVZnZlvuLNSJFUM5SISI581ix+DJyTU3YNcL+7zwPuj+8BzgXmxT9XAd+HkFyALwJvBE4GvphJMPmSsiQJNUOJiPSSt2Th7o8AO3KKLwJuitM3ARdnlf/Eg8eAiWY2AzgbuNfdd7j7TuBe+iagYZUmgaGahYhItpHus5ju7pvi9GZgepyeBazPWq4ulu2pPG/SaoYSEemjYB3cHm6lHrZbpc3sKjNbZmbLBrrxbm906ayISF8jnSy2xOYl4uvWWL4BODhrudmxbE/lfbj7Yndf6O4La2trhxxgypIY6rMQEck20sniLiBzRdOVwK+zyj8Yr4o6BWiMzVW/A84ys0mxY/usWJY3rpqFiEgfeRvuw8xuBc4EpppZHeGqpuuA283sw8A64LK4+N3AecAqoAX4EIC77zCzfwaejMv9k7vndpoPq3A1lJKFiEi2vCULd3/vHmYt6mdZBz6+h/UsAZYMY2h7pQ5uEZG+dAd3jrQlSOjSWRGRXpQscqTQ2FAiIrmULHK4JUiqGUpEpBclixwpkmqGEhHJoWSRI60hykVE+lCyyOGWJKmahYhIL0oWOcIQ5apZiIhkU7LI4bp0VkSkDyWLHGndwS0i0oeSRQ4nSUIDCYqI9KJkkSOl+yxERPpQssjhVqKahYhIDiWLHBobSkSkLyWLHE6SJGnwYXuIn4jIqKdkkSNt8StJq3YhIpKhZJEjbfERH+rkFhHppmSRw7trFl2FDURE5ACiZJEjbck4oWQhIpKhZJHDM81Q6rMQEemmZJHD1cEtItKHkkWO7mYodXCLiHRTssihDm4Rkb6ULHL09FkoWYiIZChZ5PDuq6HUDCUiklGQZGFmf2tmL5jZcjO71cwqzGyumT1uZqvM7DYzK4vLlsf3q+L8OfmMLa1kISLSx4gnCzObBfwNsNDdjwaSwOXAV4FvuvvrgZ3Ah+NHPgzsjOXfjMvljauDW0Skj0I1Q5UAlWZWAlQBm4C3AnfE+TcBF8fpi+J74vxFZmb5Csx1U56ISB8jnizcfQPwNeA1QpJoBJ4CGtw9c4auA2bF6VnA+vjZrrj8lNz1mtlVZrbMzJbV19cPPT41Q4mI9FGIZqhJhNrCXGAmUA2cs7/rdffF7r7Q3RfW1tbuR4BKFiIiuQrRDPU24FV3r3f3TuCXwGnAxNgsBTAb2BCnNwAHA8T5E4Dt+QounVAzlIhIrkIki9eAU8ysKvY9LAJeBB4ELo3LXAn8Ok7fFd8T5z/gnr8nE7mGKBcR6aMQfRaPEzqqnwaejzEsBj4LfNrMVhH6JG6MH7kRmBLLPw1ck9f4dAe3iEgfJQMvMvzc/YvAF3OK1wAn97NsG/DukYgLAI06KyLSh+7gzpXQqLMiIrmULHKkNTaUiEgfSha5Mn0W6uAWEemmZJFDl86KiPSlZJFLHdwiIn0oWeRK6A5uEZFcShY59FhVEZG+lCxy6WooEZE+lCxyqYNbRKQPJYscrj4LEZE+lCxyaYhyEZE+lCxyeEKjzorIftixptAR5IWSRS6NOisiQ1X3FHzneNi6otCRDDsli1wJXQ0lIkPUuiO+7ixsHHmgZJFLfRYiMlSZ80YRNmMrWeTS1VAiMlSe7v1aRJQscsWahasZSkQGK1OjKMIfm0oWOSxhdHmiKA+2iOTZWK9ZmNknzWy8BTea2dNmdla+gyuEhBkpkriShYgMVney8MLGkQf7WrP4C3dvAs4CJgEfAK7LW1QFlDBIkVAzlIgMnjq4sfh6HvBTd38hq6yomBldJPB0Z6FDEZHRJlOjGKvNUMBTZraUkCx+Z2bjgOL7NgAzSJGEdFHunojkUxF3cJfs43IfBo4D1rh7i5lNBj6Uv7AKxzBSJCClZigRGaSx3sENnAq87O4NZnYF8DmgcagbNbOJZnaHma0ws5fM7FQzm2xm95rZyvg6KS5rZvYdM1tlZs+Z2QlD3e6+6O6zKMI2RxHJM/VZ8H2gxcyOBf4OWA38ZD+2+23gt+5+JHAs8BJwDXC/u88D7o/vAc4F5sU/V8VY8iZcDZXQcB8iMniqWdDl7g5cBHzP3f8TGDeUDZrZBOBNwI0A7t7h7g1x3TfFxW4CLo7TFwE/8eAxYKKZzRjKtvctPkh5ElczlIgMViZJFGGf574mi11mdi3hktnfmFkCKB3iNucC9cB/mdkzZvYjM6sGprv7prjMZmB6nJ4FrM/6fF0s68XMrjKzZWa2rL6+foihhauhUiSKshopInmWOW+M4ZrFe4B2wv0Wm4HZwH8McZslwAnA9939eKCZniYnAGItZlB3tbj7Yndf6O4La2trhxhaT5+FOrhFZNDG+qWzMUHcAkwwswuANncfap9FHVDn7o/H93cQkseWTPNSfN0a528ADs76/OxYlhcGdJFUzUJEBm+sd3Cb2WXAE8C7gcuAx83s0qFsMCae9WZ2RCxaBLwI3AVcGcuuBH4dp+8CPhivijoFaMxqrhp2iYSRVge3iAxFEXdw7+t9Fv8InOTuWwHMrBa4j1ArGIq/Bm4xszJgDeGejQRwu5l9GFhHSEoAdxNuBlwFtJDn+zt67uAuvoMtInmmm/JIZBJFtJ39GLHW3Z8FFvYza1E/yzrw8aFua7AMdOmsiAyNahb81sx+B9wa37+H8Iu/6GRGnVWyEJFBSxfv1VD7lCzc/WozuwQ4LRYtdvdf5S+swrHM1VBF2EElInmmmgW4+53AnXmM5YCQiDflFWObo4jkWebS2SI8f+w1WZjZLvq/38EI3Qnj8xJVAWU6uIvxYItInhXxTXl7TRbuPqQhPUYzA106KyJD090MVXw/NvUM7hwJM7pIYkoWIjJYRdzBrWSRI5GINYsi/GUgInlWxB3cShY5DPVZiMgQdd+Up2RR9Lofq6qahYgMlmoWY4cefiQiQ5ZWB/eYYUZshiq+XwYikmeqWYwdCTPSnsBcNQsRGaTuJ+WpZlH0up9noWYoERmsIr4pT8kih1l4noUV4cEWkTzTTXljR6K7z0I1CxEZpO6b8gb1VOhRQckih8UhylWzEJFBUwf32JEwPfxIRIZIHdxjR6K7ZlF8B1tE8kw1izHEIIVhRfjLQETyrLvPovjOH0oWObpHndV9FiIyWKpZjB0Jg7THr0V3cYvIYKjPYuwIo84mwxt1covIYLgunR0zuq+GgqJsdxSRPNJNeWNIdrJQzUJEBiOtPothZ2ZJM3vGzP43vp9rZo+b2Sozu83MymJ5eXy/Ks6fk8+4MpfOAkoWIjI46uDOi08CL2W9/yrwTXd/PbAT+HAs/zCwM5Z/My6XN93PswB1cIvI4HQ/KU/NUMPCzGYD5wM/iu8NeCtwR1zkJuDiOH1RfE+cvygun6fY1AwlIkOkmsWw+xbw90DmG50CNLh339xQB8yK07OA9QBxfmNcvhczu8rMlpnZsvr6+iEHFgYSjM1QRdhJJSJ5pA7u4WNmFwBb3f2p4Vyvuy9294XuvrC2tnbI67FezVCqWYjIIBTxqLMlBdjmacCFZnYeUAGMB74NTDSzklh7mA1siMtvAA4G6sysBJgAbM9XcAakum/KK75fByKSR7opb/i4+7XuPtvd5wCXAw+4+/uBB4FL42JXAr+O03fF98T5D7jnL2337uAuvgMuInmkPosR8Vng02a2itAncWMsvxGYEss/DVyTzyBCB7cunRWRISjiPotCNEN1c/eHgIfi9Brg5H6WaQPePVIx9apZFOEBF5E8SusZ3GOGLp0VkSFTn8XY0XsgweI74CKSR66axZiRSEBaHdwiMhTdfRbFd+mskkWOzMOPADVDicjgFHEHt5JFDkNDlIvIEGnU2bHDzLJuylPNQkQGQR3cY0evhx8V4QEXkTxSB/fYYb2eZ6FkISKDoD6LsSOMOqs+CxEZAt2UN3YYlnXprPosRGQQuvsslCyKnmU/z0LJQkQGQwMJjh2JhEadFZEhUrIYO3rdZ9Ffskh1FmUVU0SGgTq4x45ed3D3d8D/eSr898dGNigRGR3UwT12JAzS2TflvbIUmreF96nYh/HcbSMTTOtOWHX/yGxLRPafbsobQ7I7uNsa4WeXwR++Gd83jGwsz9wMN18CbU0ju10RGRov3mdwK1nkSFjWpbPbVwMOG54O71t2jGwwrTvD9kc6SYnI0KjPYuwIfRbxa9mxOrxuejY0QbXGZFFSMTLBtO8Kr61KFiKjggYSHDuMrOdZbF8TXjtbYNvLPTWL0qqRCSaTLFSzEBkdirjPoqDP4D4QJcxopZyUlZBsqiOkj9gUZTGJlFWPTDDdyaJxZLYnIvtHAwmOHdXlSTopYcP440LB9KOhpBLqV/Q0Q5VWjkwwaoYSGV10U97YUZJMMLGqlBVVC0PB5DkwfiY0bexphkqUjkwwqlmIjC7q4B5bJleX8VRJrFlMmtOTLDI1i1T7yATSsTu8qs9CZHRI69LZYWNmB5vZg2b2opm9YGafjOWTzexeM1sZXyfFcjOz75jZKjN7zsxOyHeMU6vL+VPnIbDwL+Cod8L4Wb1rFp1t+Q4hUDOUyOjhDsQkUYQd3IWoWXQBf+fuC4BTgI+b2QLgGuB+d58H3B/fA5wLzIt/rgK+n+8AJ1eXsb2lCy74Jsw6MdQsdm2Elu1xD0YqWWRqFmqGEjngZfdTqM9i/7n7Jnd/Ok7vAl4CZgEXATfFxW4CLo7TFwE/8eAxYKKZzchnjJNrytjR3NFTMH5mGPpj2yvhfdcINEOl09ChS2dFRo1eyUI1i2FlZnOA44HHgenuvinO2gxMj9OzgPVZH6uLZbnrusrMlpnZsvr6+v2Ka2p1GTtaOkilY5VyfNxcc1zvSNQsMv0VAE2bYM3D+d+miAxdd9OTqWYxnMysBrgT+JS79xr8yD278W/fuPtid1/o7gtra2v3K7bJ1WW4Q0NLrF2Mn9l7gXRn/toku9ph1+beyWLL8/CTC6H+5fxsU0T2XyZBJEvVZzFczKyUkChucfdfxuItmeal+Lo1lm8ADs76+OxYljeTa8oBepqiJszumXnSR8Jrbu2iqz0862I/7frVp+HrR4SEAVBW0zNzxW/2e/0ikieZZJEoBbzorogqxNVQBtwIvOTu38iadRdwZZy+Evh1VvkH41VRpwCNWc1VeTG1ugyA7ZlkUTUlvB72Fpg6L0zn9lvcfAn87h/2e9udq0NzU9fLS0NB+fiemUoWIgeuTD9FMg6MUWRNUYWoWZwGfAB4q5k9G/+cB1wHvN3MVgJvi+8B7gbWAKuAG4C/yneAk2tistgdk4UZXL0GrrgTSkKto0/NYttK2PHqfm97c8XrAPCX/icUtMRnaRx6OmxYBnVP7fc2RCQPetUsKLpkMeJjQ7n7HwgDLvVnUT/LO/DxvAaVY3J3zSKr9lAdaxeZEWc7W3vmuYcb9rL7GYYqPmCptH55eH/+N2DtH+Csf4Eb3gq/+HP462U9SUtEDgzprD4LKLpkoTu4+zGlupwJlaUs39DP/Q3dNYusRNLZAqmOnvsi9kNZ167eBXNOh3f9EGpq4czPQuNrsHPdfm9HRIZZd80i/gYvsk5uJYt+JBPGaa+fwu9XbsNzO6lK4iCC2c1QrTvD6zDULMpTu3kyfXhWwbie6epp4VX3XYgceDJ9Fgn1WYwpb5pXy6bGNlbX5ySA/vosMsOADEOyqEjt5jWfxvopp4WC7A7uyknhVcN/iBx4PLcZSjWLMeGMw8O9Gg+9nHODX6bPot+aRfN+b7cy3cwur+L2eV+DT78EJWVZMyeGV9UsRA48mWanIu3gVrLYg1kTK1kwYzy/eT7nKt3++iwyyaKzZf/aKdNpqryZJqrY1pLqezNgRUwWqlmIHHi6axaZPgslizHjHcfO5JnXGli/o6WnsLS/PosdPdP7U7vo2E0CZ5dXsaO5n/GnVLMQOXAV+aWzShZ7ccExYbzCmx/PuvooU7Po7KcZCvav36I9jHrSRHXPPR7ZkqVQWt17eyJyYFCfxdh18OQqLj1xNjc8soZnXosn6L31WcD+1SziUOS7vLL3qLfZKieqGUrkQJR76axqFmPLF96xgMnVZVz/0OpQ0J0sspqJWrKSRXvOfRKD0RZqFruo6hlqJFflJDVDiRyI0rp0dkwbX1HKJSfO5oEVW9na1JaVLLLu4B6mmkU61hjakjU0tnayqbG170IVqlmIHJBym6F0U97Yc9nCg0mlnTuf3rCHq6F29IwOux99Fp0tIQmcftRhJBPGkj+Esaa27mrjp4+uDTcIVk5UzULkQOS6dHbMe11tDSfPmczty9bjFr+yh/4NHl8cplu2w4Q4inr77vDLfwjNUZ3NoYYy86CDuOCYGdzy+Gus3LKLnz66js//+gVW1zerZiFyoMq9dFbJYmy67KSDeXVbM0+8mnWZ7EP/Fq6K2rEGZh4Xyjp2w8/fB7/62KC30dUSOrgTlRP4+3OOpLq8hA/9+EkeXR2e/f3SpibVLEQOVLopTwDOe8NBjK8o4dv3r8Tf+LFQk2hrgHX/F57PPfdNYcHmenjtMdj0p0Fvw5u30+pllFdWMWtiJf9+yTHU7Wxl2bpQ4+hOFp0tI/MccBHZd5lx5NRnMbZVlZVwzbnz+ePq7Xyv7C/xC78Xfjk8dn1YYM7p4fXVR0LbZeN66GiB338dHvy3fdpGonEd672WytIkAG86vJZp43qGIn9xU5Pu4hY5UOnSWcm4/KSDOf+YGXz93lf41+fHhermqvugamqoaZRUwtrf93xg07PwyNdCQtldD017f8BfaeNa1vlBVJaFZJFMGBcfPwuAUw6bHGoWEw8NC295Pi/7KCJD1P2kPN2UN+YlEsb33ns8f/5nc7jhsc08VPu+MGP6gvA0vfJ4RVRmGPOHrgtNRu1N8IPT4PpToOG1/lfuTvmu11jn07prFgCfXDSPmz/8Rt42fzpbmtp5vuwYKK2CFXfncU9FZNBUs5BsZsY/nDefs4+azlV15/LRxJd49Kgv0dDSEforAE6ND/Z79WGYNCdM794S+jhuvhQ291Mr2LWZZKqNdT69u2YBUF1ewunzpnLJCbOZMaGCT/ziJdrnvAVe+h94+N9DzabIHgwvMiqpg1tylZUk+OEHFnL3J8/g0fQC3nvHZs782kNsHbeAVGk1nW/6bM/C53wVph0V7sO4/GchYdz0Dmje3rOMO+wM91S85tN71SwyJlWX8b33Hc+Wpja+vuEoaN4KD34Fbr4Ebr28++7vfdZfB3n77tGXeBo3wL+/Ljx6VqSQinzU2RF/Bncxef20Gh74zJm8sLGJHxLZHnIAABIvSURBVD68mjNW/z2OMfvbf+S7s97NZG+g5tBF1JxTgbXvhiPPh0lz4YdnwK//Cs79Kjz7M1i5FI57PwBrc2oW2U48dDI/+uBJfOJnCX7b/k1OPeZI3pN4iONf/jp2y6Xw3p/D6gfgwX+Fw88OV2pNmgNv+xJMPixc4vvjC0IM6x+Hc6+Dk/4yrPxPt8H/fgqOvADetTg0qwHseBXqX4ZD/wwqxvcOaOtL4XXa/GH/bvfZK7+Flm3w3O09FxmIFEKR35SnZLGfptaU8+bDa3nz4bWsqd/N06818I2lL3P+6neGBb60lMrSJEfNnMxbNq9iY0OKM6b/JWevvAF79fd4Vyvmadi5lrQl2eBT+61ZZJw+byoPX/1WvvvASm5+fB23dR7LOYlP8N3138O+Np+SdBvpqloSj10PtfNh9UNQdwG85R/gyR+FGkjDa+HE/4dvhSSy9g/hqq2Jh8Dzt8OkQ6H2yDCMydLPh6FNZhwHc8+A5++AuW+Gt30RlpwDqU644g445NSw/GPXgyXCere+BIu+GJ5PXlqRnwOw+oHwunJpqBVlklyqCxLJnvdD0VgX9iX3uSIi/SnyUWetzzOmi8DChQt92bJlBdt+ZypNc3sXL25s4vkNjWzd1c6DK7ayZlszNeUlpNLOpM4t/Fflt5iU3smT6cN5e8kz3FzzIf6p/s2s/Mq5lCYHbiFMp50123Zzz/ObWfnMwyxqvJMn0kdyW+pM3nVwC6uZxYlVm/nrzZ9jXPsWPFlO58WLKT36Iuzlu8PNgxnz3wGXLAm1i2dv6SmffBgs/AtY+rnwftaJsOEpqJkeEk/VFGiqgwmHhGTRkXPnes1BsHtzuOR3/EyYenioiWx9MXTUJ0th7f+F54Qcc1lYp6dDAiuthGMvh51rwzbnnB4+s/pBOOho2LYSXvzvEEPLdnjzNbBrY3jM7cql4Qq1I86FcTPCSd8SYbTg9Y+HspppYfiWmoNC+asPhzjHHRTunXl8cYjlzVfD+Fmw7RXYvTXsx8RDQqxtjSEp7d4avofZJ0OqPSyfGZm4elpP0upsheZtYf071sD4GVD3FGxfFWqD5eMBh/oVMP0owEJZ42vhqrtkaWgbnzQn9IOVVYck2b4rfPftu8L4ZBMPga6OsI+pDkiWhTgTpWHbnoaGdeF1wuywLyUV4UeEe7hYo2F92AezcKl2aRWUVcXX6rBOT4f1pVPxtSt08NZMi6MYWPjeUx2hqXX3FjjomPDZba+Eq/sSydAs2tUWlkt1hv1LJMMFIt0/AuJ3uHtLOAapzrAOd6iYEO5BqpgQtp9ZT1lNWEdbA1gy/oBIhmW2rwrHaubxoQk28zlPhb8TEI4FhHV1tYcnV1oifF+JktDkZIlwmfyO1bBsCbz18/DAP8PrFsHrF4W/x+lUiNNT4TsrKQ+PGygpC1dKllaG9bc1hMFCKyaGY920IWy7vCbsiyXDOpJl0LQxLJ8sD+tJlofPjDsotGIMgZk95e4L+52nZDEy3J32rjQlCaO5I8Uvlq3nude2M6PKKa2s5pePrWRjWyk15SUs//LZQ9rG+h0tPLO+gZc3N/Hb5ZuZWlPOhoZWNu5s5ghbzzqfTgsVlCSMo2dU8/H0LTSNez3tM0+hbOpcaipKGV+epHblz6mdOYeJk6bClNdD9VS457Mwbjqc9rfw3/8v3Edy2qdg1gmhs33VfeEkcvrfwh+/C9tehulHw7o/wtHvCifwpo1Q92Tob5n8unC3e1sjvP5toblr6wt0nxAOfiPs2hROaJYIJ48da8K8munxRFkT1nHxD0LNaPvK8I+sciLMOSNs79VHIN3Z+4uacEi4GKErZ6DG6niC62oDPCS2ykkhuUCIo3JySEzk/LtJlIZlm7fu+wErrYbO5vBdTJgdYs2sd9zMkPiy15+7H/1JlocTz77c5W+x5pXu2rf1pvbzRlBLhMSXiS1RsodtG32+3z0pqwknyLbGwTf7lFSG5LE/z6DJZQn46CPw6PXhx0fThuFbd/8bpM93Nfsk+Mv7hra2YkgWZnYO8G0gCfzI3a/b07IHYrIYSHN7F3+qa2BydRlHHjR+4A8MQlNbJys27eLlzU00tXXR1NrJc3WNbGlqY3tzB42tfU9CZjCuvISpNeVMqSmjvCRJRWmCGRMqOWhCBcmEkTSjoixJOu1UliWZN62Gjq40Hak0U6vLqKkoJZkwaipKqCxNYoClu7CO3VjVpJAWPB3+waZT4R/WuJnhl2DF+PCLatOfYMrrwol4d324DHnyYaEWUzkJb9rAC7vHcejkSsZ1bguJJJHVjJf5xerp8MsuUQLl48K6zcL83VtC+YTZ4Zc/hJNY+bjwj3/X5rDd8TNDWWtDSBgVE8LJr6st/NorGxdOWqUVYZ2lVWG/MlfJmYVfhNXTQjzVU8N6Kyb0xGrJ8Ou2rCpsp6Q8rLN6WjypediP7atC7airLexv+fhw4sw8s715W0gaTRtDjSHdFbaf6uz5dV05KcTTuiPUFDqaw/67hyQ24eCeX8XlNeG1syUs09Ec4kzEX9aJkp71puKVgZn9codEAsbP7vm1nO4K62+uByzEXVIRkhIefiRg4Tu0RNzveAyrpoSm1NLKcEzMQmdyx+6QiDwdkmuyNNaYMt95Kv7CT4Xly2tCHM314btIp+L+WKgpZmozZmG/Sip6ah7l48N2Up3htawq1OSSJXjFRMwsxNq8LSxviZ7kbBaW7WwOx7xqavjhUlIZvrPWnT0/WsbNCH+f2neH/fN0rCG3h78/1bVhH7raw590Z5hfM21I54pRnyzMLAm8ArwdqAOeBN7r7i/2t/xoTBaF1NTWSUNzJ7vaO9nV1kXanWVrd7J9dztbd7XT0NJJe1eK1s4063e0sLt9H36J7qPSpDGhsoxJVaVMqCylvDRBKu1saGilsaWTN8yewPiKUkqTCcpKEiTNSCQgYYYZvLx5F0+u3Ul5SYIzj6hlwYwJVJQmMMssYxiQsHCfTGkyQVNrJw5UlSWpKiuhqixJ2p3WjhQJM6rKknSmnVQ6TUkiEbcd1pVKOSl3ShLGuIpSKkuTOB5aGIBU2ulKpelKOx2pUJOcUl1O5tyRWRZ6v3dC7TNhxvjKUMNsbO3AzCgvSVBekqSsJIG7k0o7aQ83bSYTRknCSOT0zexLV03uMkbfdSTMSFi4ZFwG9tvlm/j07X8iacY15x3JBW+YyfjKklHz/RVDsjgV+JK7nx3fXwvg7v2Oo6FkkT/u4SSYTkNnOk1bR4pEwtjZ3MH6nS2UJsOJbdvudna3d5FKO83tXbR2pOIJsfcJsqMrTWNrBzubO2lq66SjKzQlzJpUSVVZkhc2NtHakaIzlaajK03aIeWOezhhjq8o4fKTD2FzYxtLX9jMxsa2vcYvQ9edgAmvhP+6E3dmXqZlJJMAe68jnDS7T53W66Vn/p7Ks2LJXqLv8pn3e18fucsP8LmB9mH9jlaOOGgc1eVJHlsTBh2tLA218pJkgkTWivpLzjmr7DfWPX4mTi+YMZ4ffqDf8/2A9pYsRsvVULOA9Vnv64A3FiiWMS380g3NPJUkGV8RrvyYWlPOvOnjChkaX7rwKDq60vGXd+YP4JD2UCPo6EozrqKEkkSC5o6QxJo7ukKTWmmoYbR0pCiJtZDOVJrOlNOZSpNyJ2nh13xnKs3u9i7aOtOheS3TWmFGWTLURkqSRmdXuvsRuRbPpPE8213r6f4sRtqdxtZOdrd3MbGqrLuvq70rTXtnqPkkE+HXftqhK9Zksk/J/f3+85x27X35jZhOe3dtyeOH0vG7TMekn0kKmWXT7t390Za1ryGG3tvOxJQbSybBDLR87nxyam378pnu76XP/D3F0P/8zMTJcyfzmbOOYGJVGQ+s2Mq67c1sbmyjvStNVzrdJ76syLP2P7ucfstzP5M9eciUKvJhtCSLAZnZVcBVAIccckiBo5FCKSvZ9/tM93Q/i8hwePuC6YUOYViNlju4NwAHZ72fHcu6uftid1/o7gtra2tHNDgRkWI3WpLFk8A8M5trZmXA5cBdBY5JRGTMGBXNUO7eZWafAH5HuHR2ibu/UOCwRETGjFGRLADc/W5A43KLiBTAaGmGEhGRAlKyEBGRASlZiIjIgJQsRERkQKNiuI/BMrN6YN1+rGIqsG2Ywim0YtmXYtkP0L4cqLQvcKi793ujWlEmi/1lZsv2ND7KaFMs+1Is+wHalwOV9mXv1AwlIiIDUrIQEZEBKVn0b3GhAxhGxbIvxbIfoH05UGlf9kJ9FiIiMiDVLEREZEBKFiIiMiAliyxmdo6ZvWxmq8zsmkLHM1hmttbMnjezZ81sWSybbGb3mtnK+Dqp0HH2x8yWmNlWM1ueVdZv7BZ8Jx6n58zshMJF3tce9uVLZrYhHptnzey8rHnXxn152czOLkzU/TOzg83sQTN70cxeMLNPxvJRdWz2sh+j7riYWYWZPWFmf4r78uVYPtfMHo8x3xYf54CZlcf3q+L8OUPasMfnGY/1P4Shz1cDhwFlwJ+ABYWOa5D7sBaYmlP278A1cfoa4KuFjnMPsb8JOAFYPlDswHnAPYQndp4CPF7o+PdhX74EfKafZRfEv2vlwNz4dzBZ6H3Iim8GcEKcHge8EmMeVcdmL/sx6o5L/G5r4nQp8Hj8rm8HLo/lPwD+X5z+K+AHcfpy4LahbFc1ix4nA6vcfY27dwA/By4qcEzD4SLgpjh9E3BxAWPZI3d/BNiRU7yn2C8CfuLBY8BEM5sxMpEObA/7sicXAT9393Z3fxVYRfi7eEBw903u/nSc3gW8BMxilB2bvezHnhywxyV+t7vj29L4x4G3AnfE8txjkjlWdwCLzCzzaPR9pmTRYxawPut9HXv/y3QgcmCpmT0Vn0kOMN3dN8XpzcBoejDwnmIfrcfqE7FpZklWc+Co2ZfYfHE84ZfsqD02OfsBo/C4mFnSzJ4FtgL3Emo+De7eFRfJjrd7X+L8RmDKYLepZFFcTnf3E4BzgY+b2ZuyZ3qoh47Ka6VHc+zR94HXAccBm4CvFzacwTGzGuBO4FPu3pQ9bzQdm372Y1QeF3dPuftxwGxCjefIfG9TyaLHBuDgrPezY9mo4e4b4utW4FeEv0RbMs0A8XVr4SIctD3FPuqOlbtvif/A08AN9DRpHPD7YmalhBPsLe7+y1g86o5Nf/sxmo8LgLs3AA8CpxKa/DJPP82Ot3tf4vwJwPbBbkvJoseTwLx4RUEZoSPorgLHtM/MrNrMxmWmgbOA5YR9uDIudiXw68JEOCR7iv0u4IPxyptTgMasJpEDUk67/TsJxwbCvlwer1iZC8wDnhjp+PYktm3fCLzk7t/ImjWqjs2e9mM0HhczqzWziXG6Eng7oQ/mQeDSuFjuMckcq0uBB2JtcHAK3bN/IP0hXMnxCqH97x8LHc8gYz+McPXGn4AXMvET2ibvB1YC9wGTCx3rHuK/ldAM0Elob/3wnmInXA3yn/E4PQ8sLHT8+7AvP42xPhf/8c7IWv4f4768DJxb6Phz9uV0QhPTc8Cz8c95o+3Y7GU/Rt1xAY4BnokxLwe+EMsPIyS0VcAvgPJYXhHfr4rzDxvKdjXch4iIDEjNUCIiMiAlCxERGZCShYiIDEjJQkREBqRkISIiA1KyEDnAmNmZZva/hY5DJJuShYiIDEjJQmSIzOyK+FyBZ83sh3Fwt91m9s34nIH7zaw2LnucmT0WB6z7VdbzH15vZvfFZxM8bWavi6uvMbM7zGyFmd0ylFFCRYaTkoXIEJjZfOA9wGkeBnRLAe8HqoFl7n4U8DDwxfiRnwCfdfdjCHcMZ8pvAf7T3Y8F/oxw5zeEUVE/RXiuwmHAaXnfKZG9KBl4ERHpxyLgRODJ+KO/kjCYXhq4LS5zM/BLM5sATHT3h2P5TcAv4lhes9z9VwDu3gYQ1/eEu9fF988Cc4A/5H+3RPqnZCEyNAbc5O7X9io0+3zOckMdT6c9azqF/q1KgakZSmRo7gcuNbNp0P1M6kMJ/6YyI3++D/iDuzcCO83sjFj+AeBhD09sqzOzi+M6ys2sakT3QmQf6deKyBC4+4tm9jnCkwkThBFmPw40AyfHeVsJ/RoQhoj+QUwGa4APxfIPAD80s3+K63j3CO6GyD7TqLMiw8jMdrt7TaHjEBluaoYSEZEBqWYhIiIDUs1CREQGpGQhIiIDUrIQEZEBKVmIiMiAlCxERGRA/x/V2vDQaPMIggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0-oZjd1NP1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282742ed-4caa-4f55-bae1-b795cb65b9cc"
      },
      "source": [
        "vae_model.save('/content/gdrive/MyDrive/saturation_10_points.h5')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc6EqII-2K1a"
      },
      "source": [
        "vae_model.load_weights('/content/gdrive/MyDrive/saturation_10_points.h5')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdhzP1M92eLt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}