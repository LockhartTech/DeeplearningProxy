{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pressure_Grid_Search_lr1e-4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-srm3018/DeeplearningProxy/blob/main/Gridsearch/Pressure_Grid_Search_lr1e_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9PQE47vGAl_",
        "outputId": "96f063a3-bf1a-4640-e207-17fd99a4ba79"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 25 09:28:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi27NRCrGRba",
        "outputId": "e3abab1b-f6b8-44f6-eb28-145f74b4d8c1"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "9D0wgpehaCVX",
        "outputId": "18ba604c-8808-4b7f-e003-7bb01773a638"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2947b9d9-a23e-4c58-9ea7-aaf0761f86e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2947b9d9-a23e-4c58-9ea7-aaf0761f86e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving layers.py to layers.py\n",
            "Saving unet_uae.py to unet_uae.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers.py': b'\"\"\"Import required libraries and modules.\"\"\"\\r\\n\\r\\nimport tensorflow as tf\\r\\nfrom keras import backend as K\\r\\nfrom keras.engine.topology import Layer\\r\\nfrom keras.layers.merge import add\\r\\n# from keras.engine import InputSpec\\r\\nfrom keras.layers import InputSpec\\r\\nfrom keras.layers.core import Activation\\r\\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\\r\\nfrom keras.layers import BatchNormalization, ConvLSTM2D\\r\\nfrom keras.layers import TimeDistributed, Reshape, RepeatVector\\r\\nfrom keras import regularizers\\r\\n\\r\\n\\r\\nreg_weights = 0.00001\\r\\n\\r\\n\\r\\ndef conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride,\\r\\n                   padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(\"relu\")(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef time_conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                            strides=stride,\\r\\n                            padding=\\'same\\',\\r\\n                            kernel_regularizer=regularizers.\\r\\n                            l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(\"relu\"))(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    *default = (1,1)\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        a = BatchNormalization()(a)\\r\\n        a = Activation(\"relu\")(a)\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(a)\\r\\n        y = BatchNormalization()(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef time_res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        a = TimeDistributed(BatchNormalization())(a)\\r\\n        a = TimeDistributed(Activation(\"relu\"))(a)\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        y = TimeDistributed(BatchNormalization())(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef dconv_bn_nolinear(nb_filter, nb_row, nb_col, stride=(2, 2),\\r\\n                      activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = UnPooling2D(size=stride)(x)\\r\\n        x = ReflectionPadding2D(padding=(int(nb_row/2), int(nb_col/2)))(x)\\r\\n        x = Conv2D(nb_filter, (nb_row, nb_col), padding=\\'valid\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(activation)(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\ndef time_dconv_bn_nolinear(nb_filter, nb_row, nb_col,\\r\\n                           stride=(2, 2), activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create time convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = TimeDistributed(UnPooling2D(size=stride))(x)\\r\\n        x = TimeDistributed(ReflectionPadding2D(padding=(int(nb_row/2),\\r\\n                            int(nb_col/2))))(x)\\r\\n        x = TimeDistributed(Conv2D(nb_filter, (nb_row, nb_col),\\r\\n                                   padding=\\'valid\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(activation))(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\nclass ReflectionPadding2D(Layer):\\r\\n    \"\"\"class for reflectionPadding2D.\"\"\"\\r\\n\\r\\n    def __init__(self, padding=(1, 1), data_format=\"channels_last\", **kwargs):\\r\\n        \"\"\"\\r\\n        Construct class parameters.\\r\\n\\r\\n        parameters:\\r\\n        -------\\r\\n        padding\\r\\n        dim_ordering\\r\\n        \"\"\"\\r\\n        super(ReflectionPadding2D, self).__init__(**kwargs)\\r\\n\\r\\n        if data_format == \\'channels_last\\':\\r\\n            dim_ordering = K.image_data_format()\\r\\n\\r\\n        self.padding = padding\\r\\n        if isinstance(padding, dict):\\r\\n            if set(padding.keys()) <= {\\'top_pad\\', \\'bottom_pad\\',\\r\\n                                       \\'left_pad\\', \\'right_pad\\'}:\\r\\n                self.top_pad = padding.get(\\'top_pad\\', 0)\\r\\n                self.bottom_pad = padding.get(\\'bottom_pad\\', 0)\\r\\n                self.left_pad = padding.get(\\'left_pad\\', 0)\\r\\n                self.right_pad = padding.get(\\'right_pad\\', 0)\\r\\n            else:\\r\\n                raise ValueError(\\'Unexpected key\\'\\r\\n                                 \\'found in `padding` dictionary.\\'\\r\\n                                 \\'Keys have to be in {\"top_pad\", \"bottom_pad\",\\'\\r\\n                                 \\'\"left_pad\", \"right_pad\"}.\\'\\r\\n                                 \\'Found: \\' + str(padding.keys()))\\r\\n        else:\\r\\n            padding = tuple(padding)\\r\\n            if len(padding) == 2:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[0]\\r\\n                self.left_pad = padding[1]\\r\\n                self.right_pad = padding[1]\\r\\n            elif len(padding) == 4:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[1]\\r\\n                self.left_pad = padding[2]\\r\\n                self.right_pad = padding[3]\\r\\n            else:\\r\\n                raise TypeError(\\'`padding` should be tuple of int \\'\\r\\n                                \\'of length 2 or 4, or dict. \\'\\r\\n                                \\'Found: \\' + str(padding))\\r\\n\\r\\n        # if data_format not in {\\'channels_last\\'}:\\r\\n        #     raise ValueError(\\'data_format must be in {\"channels_last\"}.\\')\\r\\n        self.data_format = data_format\\r\\n        self.input_spec = [InputSpec(ndim=4)]\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call x to apply padding.\"\"\"\\r\\n        top_pad = self.top_pad\\r\\n        bottom_pad = self.bottom_pad\\r\\n        left_pad = self.left_pad\\r\\n        right_pad = self.right_pad\\r\\n\\r\\n        paddings = [[0, 0], [left_pad, right_pad],\\r\\n                    [top_pad, bottom_pad], [0, 0]]\\r\\n\\r\\n        return tf.pad(x, paddings, mode=\\'REFLECT\\', name=None)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"\\r\\n        Compute the shape of output.\\r\\n\\r\\n        Parameters:\\r\\n        --------\\r\\n        input_shape: Tuple\\r\\n        shape of input\\r\\n        \"\"\"\\r\\n        if self.data_format == \\'channels_last\\':\\r\\n            rows = input_shape[1] + self.top_pad + self.bottom_pad\\r\\n            cols = input_shape[2] + self.left_pad + self.right_pad\\r\\n\\r\\n            return (input_shape[0],\\r\\n                    rows,\\r\\n                    cols,\\r\\n                    input_shape[3])\\r\\n        else:\\r\\n            raise ValueError(\\'Invalid data_format:\\', self.data_format)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get the Configure.\"\"\"\\r\\n        config = {\\'padding\\': self.padding}\\r\\n        base_config = super(ReflectionPadding2D, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n\\r\\n\\r\\nclass UnPooling2D(UpSampling2D):\\r\\n    \"\"\"Unpool 2D from 2D upsampling.\"\"\"\\r\\n\\r\\n    def __init__(self, size=(2, 2)):\\r\\n        \"\"\"Construct size.\"\"\"\\r\\n        super(UnPooling2D, self).__init__(size)\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call th x data.\"\"\"\\r\\n        shapes = x.get_shape().as_list()\\r\\n        w = self.size[0] * shapes[1]\\r\\n        h = self.size[1] * shapes[2]\\r\\n        return tf.image.resize(x, (w, h))\\r\\n\\r\\n\\r\\nclass InstanceNormalize(Layer):\\r\\n    \"\"\"Normalization Instance of class.\"\"\"\\r\\n\\r\\n    def __init__(self, **kwargs):\\r\\n        \"\"\"Initialize the keyaarguments.\"\"\"\\r\\n        super(InstanceNormalize, self).__init__(**kwargs)\\r\\n        self.epsilon = 1e-3\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call mean and variance for normalization.\"\"\"\\r\\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\\r\\n        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute the shape of output.\"\"\"\\r\\n        return input_shape\\r\\n\\r\\n\\r\\nclass RepeatConv(Layer):\\r\\n    \"\"\"\\r\\n    Repeats the input n times.\\r\\n\\r\\n    Example:\\r\\n    -------\\r\\n        model = Sequential()\\r\\n        model.add(Dense(32, input_dim=32))\\r\\n        now: model.output_shape == (None, 32)\\r\\n        note: `None` is the batch dimension\\r\\n        model.add(RepeatVector(3))\\r\\n        now: model.output_shape == (None, 3, 32)\\r\\n\\r\\n    Arguments\\r\\n    ---------\\r\\n        n: integer, repetition factor.\\r\\n    Input shape\\r\\n    ----------\\r\\n        4D tensor of shape `(num_samples, w, h, c)`.\\r\\n    Output shape\\r\\n    -----------\\r\\n        5D tensor of shape `(num_samples, n, w, h, c)`.\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, n, **kwargs):\\r\\n        \"\"\"Initialize the class parameters.\"\"\"\\r\\n        super(RepeatConv, self).__init__(**kwargs)\\r\\n        self.n = n\\r\\n        self.input_spec = InputSpec(ndim=4)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute output shape.\"\"\"\\r\\n        return (input_shape[0], self.n, input_shape[1],\\r\\n                input_shape[2], input_shape[3])\\r\\n\\r\\n    def call(self, inputs):\\r\\n        \"\"\"Call the inputs.\"\"\"\\r\\n        x = K.expand_dims(inputs, 1)\\r\\n        pattern = tf.stack([1, self.n, 1, 1, 1])\\r\\n        return K.tile(x, pattern)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get configure.\"\"\"\\r\\n        config = {\\'n\\': self.n}\\r\\n        base_config = super(RepeatConv, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n',\n",
              " 'unet_uae.py': b'\"\"\"Path hack to make tests work.\"\"\"\\r\\nfrom layers import *\\r\\nfrom keras import backend as K\\r\\nfrom keras.layers import Input, Flatten, Dense, Lambda, Reshape\\r\\nfrom keras.layers import concatenate, TimeDistributed, RepeatVector, ConvLSTM2D\\r\\nfrom keras.models import Model\\r\\nimport numpy as np\\r\\n\\r\\n\\r\\ndef create_vae(input_shape, depth):\\r\\n    \"\"\"\\r\\n    Create VAE to create something new.\\r\\n\\r\\n    Parameters:\\r\\n    -------\\r\\n    input_shape : Tuple\\r\\n    depth : int\\r\\n    Returns:\\r\\n    -------\\r\\n    encoder: encoder\\r\\n    model : recurrnet R-UNET model\\r\\n\\r\\n    \"\"\"\\r\\n    # Encoder\\r\\n    input = Input(shape=input_shape, name=\\'image\\')\\r\\n\\r\\n    enc1 = conv_bn_relu(16, 3, 3, stride=(2, 2))(input)\\r\\n    time_enc1 = RepeatConv(depth)(enc1)\\r\\n    enc2 = conv_bn_relu(32, 3, 3, stride=(1, 1))(enc1)\\r\\n    time_enc2 = RepeatConv(depth)(enc2)\\r\\n    enc3 = conv_bn_relu(64, 3, 3, stride=(2, 2))(enc2)\\r\\n    time_enc3 = RepeatConv(depth)(enc3)\\r\\n    enc4 = conv_bn_relu(128, 3, 3, stride=(1, 1))(enc3)\\r\\n    time_enc4 = RepeatConv(depth)(enc4)\\r\\n\\r\\n    x = res_conv(128, 3, 3)(enc4)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n\\r\\n    encoder = Model(input, x, name=\\'encoder\\')\\r\\n\\r\\n    x = RepeatConv(depth)(enc4)\\r\\n    x = ConvLSTM2D(128, (3, 3), strides=(1, 1),\\r\\n                   padding=\\'same\\', activation=\\'relu\\',\\r\\n                   return_sequences=True)(x)\\r\\n    # x = ConvLSTM2D(64, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    # x = ConvLSTM2D(128, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    dec4 = time_res_conv(128, 3, 3)(x)\\r\\n\\r\\n    merge4 = concatenate([time_enc4, dec4], axis=4)\\r\\n    dec3 = time_dconv_bn_nolinear(128, 3, 3, stride=(1, 1))(merge4)\\r\\n    merge3 = concatenate([time_enc3, dec3], axis=4)\\r\\n    dec2 = time_dconv_bn_nolinear(64, 3, 3, stride=(2, 2))(merge3)\\r\\n    merge2 = concatenate([time_enc2, dec2], axis=4)\\r\\n    dec1 = time_dconv_bn_nolinear(32, 3, 3, stride=(1, 1))(merge2)\\r\\n    merge1 = concatenate([time_enc1, dec1], axis=4)\\r\\n    dec0 = time_dconv_bn_nolinear(16, 3, 3, stride=(2, 2))(merge1)\\r\\n\\r\\n    output = TimeDistributed(Conv2D(1, (3, 3), padding=\\'same\\',\\r\\n                                    activation=None))(dec0)\\r\\n    print(\\'output shape is \\', K.int_shape(output))\\r\\n    # Full net\\r\\n    full_model = Model(input, output)\\r\\n\\r\\n    return full_model, encoder\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrWDL1qVfMw"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import unet_uae as vae_util\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf \n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zr0u9vQzf5R",
        "outputId": "a788c6db-a886-4ceb-cbb3-d7277aceb901"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlbNGMtBA_Fs"
      },
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Function to load datasets in format .NPY\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    path : string\n",
        "        The absolute path of where data saved in local system\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    loaded_data : ndarray\n",
        "        The data which was loaded\n",
        "    \"\"\"\n",
        "    loaded_data = np.load(path)\n",
        "    return loaded_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cErfklo76F21"
      },
      "source": [
        "# Two common methods for feature scaling is : 1-Normalization & 2-Standardaisation\n",
        "\n",
        "def normalize(data):\n",
        "    \"\"\"\n",
        "    this function used for Max-Min Normalization (Min-Max scaling) by re-scaling\n",
        "    features with a distribution value between 0 and 1. For every feature,the minimum\n",
        "    value of that feature gets transformed into 0, and the maximum value \n",
        "    gets transformed into 1\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    norm_data : ndarray\n",
        "        The normalized data which transformed into 0 and 1\n",
        "    \"\"\"\n",
        "    max_p = np.max(data[:, :, :, :])\n",
        "    min_p = np.min(data[:, :, :, :])\n",
        "    norm_data = (data - min_p)/(max_p - min_p)\n",
        "    return norm_data\n",
        "\n",
        "def standardize(data):\n",
        "    \"\"\"\n",
        "    this function used for rescaling faetures to ensure the mean\n",
        "    and the standard deviation to be 0 and 1, respectively.\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The standardized data which the mean\n",
        "    and the standard deviation to be 0 and 1\n",
        "    \"\"\"\n",
        "    data_mean = np.mean(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    data_std = np.std(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    std_data = (data - data_mean)/(data_std)\n",
        "    return std_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uviUO--fC_pp"
      },
      "source": [
        "# define the absolute path of training datatsat\n",
        "path_perm = '/content/gdrive/MyDrive/perm.npy'\n",
        "path_press = '/content/gdrive/MyDrive/pressure.npy'\n",
        "# use load_data function nd above path to loading data\n",
        "X_data= load_data(path_perm)\n",
        "target_data = load_data(path_press)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5s3_Zex6sQs"
      },
      "source": [
        "# Normalize data using abov normalize function\n",
        "train_nr = 2250\n",
        "test_nr = 750"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yq6Ouns_EIN",
        "outputId": "48b094a7-a580-4c68-ba55-be5f19dffaed"
      },
      "source": [
        "p_t_mean = np.mean(target_data[:train_nr, ...], axis = 0, keepdims = True)\n",
        "target_data = target_data - p_t_mean\n",
        "print('max p is ', np.max(target_data[:train_nr, ...]), ', min p is ', np.min(target_data[:train_nr, ...]))\n",
        "max_p = np.max(target_data[:train_nr, ...])\n",
        "min_p = np.min(target_data[:train_nr, ...])\n",
        "target_data = (target_data - min_p)/(max_p -min_p) - 0.5\n",
        "print('max p is ', np.max(target_data), ', min p is ', np.min(target_data))\n",
        "print('max p train is ', np.max(target_data[:train_nr, ...]), ', min p train is ', np.min(target_data[:train_nr, ...]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max p is  516.6753418240017 , min p is  -135.6267126736111\n",
            "max p is  0.5039995270719013 , min p is  -0.500166926837037\n",
            "max p train is  0.5 , min p train is  -0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q5C0284nFxJ",
        "outputId": "ee8bf178-35a2-449e-a49c-c2f2840725e9"
      },
      "source": [
        "input_shape=(100, 100, 2)\n",
        "depth = 10\n",
        "vae_model,_ = vae_util.create_vae(input_shape, depth)\n",
        "vae_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output shape is  (None, 10, 100, 100, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 100, 100, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 50, 50, 16)   304         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 50, 50, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 50, 50, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 50, 50, 32)   4640        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 50, 50, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 50, 50, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 25, 25, 64)   18496       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 25, 25, 128)  73856       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 25, 25, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 25, 25, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_4 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)       (None, 10, 25, 25, 1 1180160     repeat_conv_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 10, 25, 25, 1 147584      conv_lst_m2d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 10, 25, 25, 1 0           conv_lst_m2d[0][0]               \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 10, 25, 25, 1 147584      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 10, 25, 25, 1 0           add_3[0][0]                      \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_13 (TimeDistri (None, 10, 25, 25, 1 147584      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_14 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_3 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 10, 25, 25, 1 0           add_4[0][0]                      \n",
            "                                                                 time_distributed_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 25, 25, 2 0           repeat_conv_3[0][0]              \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_15 (TimeDistri (None, 10, 25, 25, 2 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 10, 27, 27, 2 0           time_distributed_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 10, 25, 25, 1 295040      time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_2 (RepeatConv)      (None, 10, 25, 25, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_19 (TimeDistri (None, 10, 25, 25, 1 0           time_distributed_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 25, 25, 1 0           repeat_conv_2[0][0]              \n",
            "                                                                 time_distributed_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_20 (TimeDistri (None, 10, 50, 50, 1 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_21 (TimeDistri (None, 10, 52, 52, 1 0           time_distributed_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_22 (TimeDistri (None, 10, 50, 50, 6 110656      time_distributed_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_23 (TimeDistri (None, 10, 50, 50, 6 256         time_distributed_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_1 (RepeatConv)      (None, 10, 50, 50, 3 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_24 (TimeDistri (None, 10, 50, 50, 6 0           time_distributed_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 50, 50, 9 0           repeat_conv_1[0][0]              \n",
            "                                                                 time_distributed_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_25 (TimeDistri (None, 10, 50, 50, 9 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_26 (TimeDistri (None, 10, 52, 52, 9 0           time_distributed_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_27 (TimeDistri (None, 10, 50, 50, 3 27680       time_distributed_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_28 (TimeDistri (None, 10, 50, 50, 3 128         time_distributed_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv (RepeatConv)        (None, 10, 50, 50, 1 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_29 (TimeDistri (None, 10, 50, 50, 3 0           time_distributed_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 50, 50, 4 0           repeat_conv[0][0]                \n",
            "                                                                 time_distributed_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_30 (TimeDistri (None, 10, 100, 100, 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_31 (TimeDistri (None, 10, 102, 102, 0           time_distributed_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_32 (TimeDistri (None, 10, 100, 100, 6928        time_distributed_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_33 (TimeDistri (None, 10, 100, 100, 64          time_distributed_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_34 (TimeDistri (None, 10, 100, 100, 0           time_distributed_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_35 (TimeDistri (None, 10, 100, 100, 145         time_distributed_34[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 2,164,113\n",
            "Trainable params: 2,162,385\n",
            "Non-trainable params: 1,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4PDSq-qnONZ",
        "outputId": "66610dce-b177-483f-e85e-edeb871ac93f"
      },
      "source": [
        "depth = 10\n",
        "nr = X_data.shape[0]\n",
        "train_nr = 2250\n",
        "test_nr = 750\n",
        "train_x = np.concatenate([X_data[:train_nr,[0], ...],target_data[:train_nr,[0], ...]], axis = 1)\n",
        "train_y = target_data[:train_nr, ...]\n",
        "\n",
        "test_x = np.concatenate([X_data[nr-test_nr:,[0], ...], target_data[nr-test_nr:, [0], ...]], axis = 1)\n",
        "test_y = target_data[nr-test_nr:,...]\n",
        "\n",
        "\n",
        "train_x = train_x.transpose(0,2,3,1)\n",
        "train_y = train_y[:,:,:,:,None]\n",
        "test_x = test_x.transpose(0,2,3,1)\n",
        "test_y = test_y[:,:,:,:,None]\n",
        "#test_y = test_y.transpose(0,2,3,1)\n",
        "print('train_x shape is ', train_x.shape)\n",
        "print('train_y shape is ', train_y.shape)\n",
        "print('test_x shape is ', test_x.shape)\n",
        "print('test_y shape is ', test_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape is  (2250, 100, 100, 2)\n",
            "train_y shape is  (2250, 10, 100, 100, 1)\n",
            "test_x shape is  (750, 100, 100, 2)\n",
            "test_y shape is  (750, 10, 100, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsuLGQpniCs"
      },
      "source": [
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/saved_models/'\n",
        "epochs = 300\n",
        "batch_size = 8\n",
        "num_batch = int(train_nr/batch_size) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EqIECRbnoTX"
      },
      "source": [
        "def vae_loss(x, t_decoded):\n",
        "    '''Total loss for the plain UAE'''\n",
        "    return K.mean(reconstruction_loss(x, t_decoded))\n",
        "\n",
        "\n",
        "def reconstruction_loss(x, t_decoded):\n",
        "    '''Reconstruction loss for the plain UAE'''\n",
        "\n",
        "    return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
        "\n",
        "def relative_error(x, t_decoded):\n",
        "    return K.mean(K.abs(x - t_decoded) / x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m46Y8eknsmA"
      },
      "source": [
        "opt = Adam(learning_rate=1e-4)\n",
        "vae_model.compile(loss = vae_loss, optimizer = opt, metrics = [relative_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lttghynSnzzT"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "lrScheduler = ReduceLROnPlateau(monitor = 'loss', factor = 0.5, patience = 15, cooldown = 1, verbose = 1, min_lr = 1e-6)\n",
        "filePath = 'saved-model-{epoch:03d}-{val_loss:.2f}.h5'\n",
        "checkPoint = ModelCheckpoint(filePath, monitor = 'val_loss', verbose = 1, save_best_only = False, \\\n",
        "                             save_weights_only = True, mode = 'auto', save_freq = 20)\n",
        "\n",
        "callbacks_list = [lrScheduler, checkPoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClmgwTn9n7Dv",
        "outputId": "6a94ef16-02c4-4c01-8fa9-df93fe15a81e"
      },
      "source": [
        "history = vae_model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, \\\n",
        "                        verbose = 1, validation_data = (test_x, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "282/282 [==============================] - 87s 141ms/step - loss: 8360.2278 - relative_error: -0.6163 - val_loss: 1215.1503 - val_relative_error: -0.2986\n",
            "Epoch 2/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 422.5877 - relative_error: -0.1718 - val_loss: 290.1136 - val_relative_error: -0.1454\n",
            "Epoch 3/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 236.2018 - relative_error: -0.1288 - val_loss: 316.7420 - val_relative_error: -0.1611\n",
            "Epoch 4/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 150.7008 - relative_error: -0.1031 - val_loss: 343.5824 - val_relative_error: -0.1655\n",
            "Epoch 5/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 114.8895 - relative_error: -0.0892 - val_loss: 182.4004 - val_relative_error: -0.1182\n",
            "Epoch 6/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 90.0273 - relative_error: -0.0793 - val_loss: 215.7862 - val_relative_error: -0.1109\n",
            "Epoch 7/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 76.5326 - relative_error: -0.0731 - val_loss: 103.2486 - val_relative_error: -0.0883\n",
            "Epoch 8/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 63.6521 - relative_error: -0.0662 - val_loss: 122.3057 - val_relative_error: -0.0939\n",
            "Epoch 9/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 60.8786 - relative_error: -0.0645 - val_loss: 89.7658 - val_relative_error: -0.0800\n",
            "Epoch 10/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 51.0562 - relative_error: -0.0594 - val_loss: 99.5779 - val_relative_error: -0.0877\n",
            "Epoch 11/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 45.8001 - relative_error: -0.0564 - val_loss: 84.6159 - val_relative_error: -0.0801\n",
            "Epoch 12/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 41.3393 - relative_error: -0.0534 - val_loss: 70.3315 - val_relative_error: -0.0733\n",
            "Epoch 13/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 35.4706 - relative_error: -0.0492 - val_loss: 41.7933 - val_relative_error: -0.0519\n",
            "Epoch 14/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 34.0977 - relative_error: -0.0483 - val_loss: 46.7677 - val_relative_error: -0.0585\n",
            "Epoch 15/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 29.1118 - relative_error: -0.0445 - val_loss: 47.5042 - val_relative_error: -0.0573\n",
            "Epoch 16/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 29.3735 - relative_error: -0.0450 - val_loss: 62.8365 - val_relative_error: -0.0707\n",
            "Epoch 17/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 27.3362 - relative_error: -0.0433 - val_loss: 44.5801 - val_relative_error: -0.0575\n",
            "Epoch 18/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 23.2055 - relative_error: -0.0397 - val_loss: 34.2380 - val_relative_error: -0.0493\n",
            "Epoch 19/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 22.1931 - relative_error: -0.0389 - val_loss: 41.7364 - val_relative_error: -0.0568\n",
            "Epoch 20/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 20.8930 - relative_error: -0.0377 - val_loss: 42.6299 - val_relative_error: -0.0577\n",
            "Epoch 21/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 22.5868 - relative_error: -0.0394 - val_loss: 37.2636 - val_relative_error: -0.0542\n",
            "Epoch 22/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 18.7827 - relative_error: -0.0357 - val_loss: 34.0495 - val_relative_error: -0.0493\n",
            "Epoch 23/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 17.2764 - relative_error: -0.0342 - val_loss: 25.1634 - val_relative_error: -0.0429\n",
            "Epoch 24/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 17.7673 - relative_error: -0.0347 - val_loss: 24.4077 - val_relative_error: -0.0398\n",
            "Epoch 25/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 16.6777 - relative_error: -0.0335 - val_loss: 53.3425 - val_relative_error: -0.0681\n",
            "Epoch 26/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 16.4116 - relative_error: -0.0331 - val_loss: 19.1540 - val_relative_error: -0.0361\n",
            "Epoch 27/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 15.8383 - relative_error: -0.0322 - val_loss: 38.2407 - val_relative_error: -0.0531\n",
            "Epoch 28/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 13.5209 - relative_error: -0.0301 - val_loss: 39.4723 - val_relative_error: -0.0406\n",
            "Epoch 29/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 15.8820 - relative_error: -0.0329 - val_loss: 21.7342 - val_relative_error: -0.0411\n",
            "Epoch 30/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 12.9553 - relative_error: -0.0297 - val_loss: 19.5888 - val_relative_error: -0.0378\n",
            "Epoch 31/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 12.3387 - relative_error: -0.0287 - val_loss: 10.8883 - val_relative_error: -0.0272\n",
            "Epoch 32/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 11.9751 - relative_error: -0.0285 - val_loss: 9.3433 - val_relative_error: -0.0243\n",
            "Epoch 33/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 10.0547 - relative_error: -0.0258 - val_loss: 12.9261 - val_relative_error: -0.0304\n",
            "Epoch 34/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 8.8375 - relative_error: -0.0245 - val_loss: 16.5554 - val_relative_error: -0.0338\n",
            "Epoch 35/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 10.7156 - relative_error: -0.0263 - val_loss: 10.7807 - val_relative_error: -0.0258\n",
            "Epoch 36/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 14.0690 - relative_error: -0.0297 - val_loss: 14.6279 - val_relative_error: -0.0322\n",
            "Epoch 37/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 7.8362 - relative_error: -0.0229 - val_loss: 14.2051 - val_relative_error: -0.0316\n",
            "Epoch 38/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 8.0798 - relative_error: -0.0233 - val_loss: 11.1519 - val_relative_error: -0.0268\n",
            "Epoch 39/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 9.0994 - relative_error: -0.0241 - val_loss: 10.4767 - val_relative_error: -0.0264\n",
            "Epoch 40/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 7.0366 - relative_error: -0.0214 - val_loss: 7.2171 - val_relative_error: -0.0215\n",
            "Epoch 41/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 6.2801 - relative_error: -0.0203 - val_loss: 14.4596 - val_relative_error: -0.0330\n",
            "Epoch 42/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 6.8967 - relative_error: -0.0213 - val_loss: 15.5491 - val_relative_error: -0.0349\n",
            "Epoch 43/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 6.1727 - relative_error: -0.0202 - val_loss: 13.7779 - val_relative_error: -0.0318\n",
            "Epoch 44/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 6.1863 - relative_error: -0.0202 - val_loss: 7.3359 - val_relative_error: -0.0215\n",
            "Epoch 45/300\n",
            "282/282 [==============================] - 37s 133ms/step - loss: 6.0770 - relative_error: -0.0199 - val_loss: 15.3695 - val_relative_error: -0.0341\n",
            "Epoch 46/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 5.3027 - relative_error: -0.0187 - val_loss: 7.2697 - val_relative_error: -0.0226\n",
            "Epoch 47/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 5.4190 - relative_error: -0.0190 - val_loss: 4.8989 - val_relative_error: -0.0175\n",
            "Epoch 48/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 5.1377 - relative_error: -0.0184 - val_loss: 7.9518 - val_relative_error: -0.0233\n",
            "Epoch 49/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 4.5991 - relative_error: -0.0173 - val_loss: 18.3390 - val_relative_error: -0.0396\n",
            "Epoch 50/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 4.8658 - relative_error: -0.0179 - val_loss: 15.1108 - val_relative_error: -0.0333\n",
            "Epoch 51/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 5.0553 - relative_error: -0.0181 - val_loss: 13.1594 - val_relative_error: -0.0303\n",
            "Epoch 52/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 4.3963 - relative_error: -0.0170 - val_loss: 8.7218 - val_relative_error: -0.0246\n",
            "Epoch 53/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 4.5170 - relative_error: -0.0172 - val_loss: 12.1006 - val_relative_error: -0.0309\n",
            "Epoch 54/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 4.7022 - relative_error: -0.0177 - val_loss: 11.0377 - val_relative_error: -0.0282\n",
            "Epoch 55/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 4.5317 - relative_error: -0.0173 - val_loss: 14.2827 - val_relative_error: -0.0335\n",
            "Epoch 56/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 4.2214 - relative_error: -0.0166 - val_loss: 10.5302 - val_relative_error: -0.0278\n",
            "Epoch 57/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 4.5546 - relative_error: -0.0173 - val_loss: 5.6233 - val_relative_error: -0.0188\n",
            "Epoch 58/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.9543 - relative_error: -0.0161 - val_loss: 6.8360 - val_relative_error: -0.0214\n",
            "Epoch 59/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.7980 - relative_error: -0.0157 - val_loss: 5.0950 - val_relative_error: -0.0175\n",
            "Epoch 60/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.5113 - relative_error: -0.0152 - val_loss: 6.7309 - val_relative_error: -0.0213\n",
            "Epoch 61/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.7180 - relative_error: -0.0157 - val_loss: 4.5916 - val_relative_error: -0.0176\n",
            "Epoch 62/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 3.7039 - relative_error: -0.0157 - val_loss: 4.8774 - val_relative_error: -0.0179\n",
            "Epoch 63/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 3.1184 - relative_error: -0.0142 - val_loss: 4.0492 - val_relative_error: -0.0154\n",
            "Epoch 64/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 3.3335 - relative_error: -0.0148 - val_loss: 4.8828 - val_relative_error: -0.0175\n",
            "Epoch 65/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.0052 - relative_error: -0.0139 - val_loss: 5.2004 - val_relative_error: -0.0188\n",
            "Epoch 66/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 3.1367 - relative_error: -0.0143 - val_loss: 4.4365 - val_relative_error: -0.0172\n",
            "Epoch 67/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.0934 - relative_error: -0.0143 - val_loss: 6.2626 - val_relative_error: -0.0197\n",
            "Epoch 68/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 3.4176 - relative_error: -0.0149 - val_loss: 4.4032 - val_relative_error: -0.0168\n",
            "Epoch 69/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 3.2238 - relative_error: -0.0143 - val_loss: 4.3382 - val_relative_error: -0.0167\n",
            "Epoch 70/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.9172 - relative_error: -0.0138 - val_loss: 6.3282 - val_relative_error: -0.0204\n",
            "Epoch 71/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 3.0836 - relative_error: -0.0143 - val_loss: 5.0598 - val_relative_error: -0.0181\n",
            "Epoch 72/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.9358 - relative_error: -0.0138 - val_loss: 7.1046 - val_relative_error: -0.0221\n",
            "Epoch 73/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 2.4951 - relative_error: -0.0126 - val_loss: 3.3817 - val_relative_error: -0.0147\n",
            "Epoch 74/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 3.0576 - relative_error: -0.0139 - val_loss: 3.7905 - val_relative_error: -0.0152\n",
            "Epoch 75/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 2.6699 - relative_error: -0.0131 - val_loss: 7.9523 - val_relative_error: -0.0243\n",
            "Epoch 76/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.4687 - relative_error: -0.0126 - val_loss: 3.5640 - val_relative_error: -0.0147\n",
            "Epoch 77/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.9004 - relative_error: -0.0139 - val_loss: 4.2144 - val_relative_error: -0.0164\n",
            "Epoch 78/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 2.3042 - relative_error: -0.0122 - val_loss: 5.3386 - val_relative_error: -0.0185\n",
            "Epoch 79/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.7060 - relative_error: -0.0134 - val_loss: 5.3264 - val_relative_error: -0.0196\n",
            "Epoch 80/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 2.6560 - relative_error: -0.0132 - val_loss: 3.1576 - val_relative_error: -0.0137\n",
            "Epoch 81/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 2.1684 - relative_error: -0.0118 - val_loss: 11.7248 - val_relative_error: -0.0320\n",
            "Epoch 82/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.4285 - relative_error: -0.0125 - val_loss: 3.8059 - val_relative_error: -0.0156\n",
            "Epoch 83/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.9933 - relative_error: -0.0112 - val_loss: 2.7266 - val_relative_error: -0.0125\n",
            "Epoch 84/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.2379 - relative_error: -0.0120 - val_loss: 5.5824 - val_relative_error: -0.0197\n",
            "Epoch 85/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.0855 - relative_error: -0.0115 - val_loss: 5.0530 - val_relative_error: -0.0181\n",
            "Epoch 86/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.1907 - relative_error: -0.0119 - val_loss: 3.2142 - val_relative_error: -0.0141\n",
            "Epoch 87/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.2100 - relative_error: -0.0120 - val_loss: 3.6625 - val_relative_error: -0.0149\n",
            "Epoch 88/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.0579 - relative_error: -0.0115 - val_loss: 4.4332 - val_relative_error: -0.0170\n",
            "Epoch 89/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.0858 - relative_error: -0.0116 - val_loss: 4.3452 - val_relative_error: -0.0162\n",
            "Epoch 90/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.1293 - relative_error: -0.0118 - val_loss: 5.1721 - val_relative_error: -0.0195\n",
            "Epoch 91/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.0018 - relative_error: -0.0114 - val_loss: 2.7566 - val_relative_error: -0.0131\n",
            "Epoch 92/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.9767 - relative_error: -0.0112 - val_loss: 3.2623 - val_relative_error: -0.0140\n",
            "Epoch 93/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.9076 - relative_error: -0.0111 - val_loss: 2.6201 - val_relative_error: -0.0123\n",
            "Epoch 94/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.8002 - relative_error: -0.0108 - val_loss: 3.2184 - val_relative_error: -0.0137\n",
            "Epoch 95/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.7094 - relative_error: -0.0104 - val_loss: 11.5553 - val_relative_error: -0.0306\n",
            "Epoch 96/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 4.8517 - relative_error: -0.0172 - val_loss: 4.4586 - val_relative_error: -0.0168\n",
            "Epoch 97/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.1704 - relative_error: -0.0116 - val_loss: 2.9332 - val_relative_error: -0.0135\n",
            "Epoch 98/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.5873 - relative_error: -0.0101 - val_loss: 5.1775 - val_relative_error: -0.0191\n",
            "Epoch 99/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.7959 - relative_error: -0.0109 - val_loss: 4.9687 - val_relative_error: -0.0187\n",
            "Epoch 100/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.9011 - relative_error: -0.0112 - val_loss: 3.8077 - val_relative_error: -0.0161\n",
            "Epoch 101/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.6089 - relative_error: -0.0101 - val_loss: 6.2456 - val_relative_error: -0.0215\n",
            "Epoch 102/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.0014 - relative_error: -0.0112 - val_loss: 5.9151 - val_relative_error: -0.0206\n",
            "Epoch 103/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.7865 - relative_error: -0.0107 - val_loss: 6.2582 - val_relative_error: -0.0221\n",
            "Epoch 104/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.5200 - relative_error: -0.0099 - val_loss: 3.1533 - val_relative_error: -0.0137\n",
            "Epoch 105/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.7887 - relative_error: -0.0108 - val_loss: 4.4379 - val_relative_error: -0.0169\n",
            "Epoch 106/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.6170 - relative_error: -0.0102 - val_loss: 5.7397 - val_relative_error: -0.0193\n",
            "Epoch 107/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.2996 - relative_error: -0.0122 - val_loss: 2.4249 - val_relative_error: -0.0125\n",
            "Epoch 108/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 1.3238 - relative_error: -0.0091 - val_loss: 6.4509 - val_relative_error: -0.0222\n",
            "Epoch 109/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.5573 - relative_error: -0.0100 - val_loss: 4.8386 - val_relative_error: -0.0181\n",
            "Epoch 110/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.4610 - relative_error: -0.0097 - val_loss: 8.2227 - val_relative_error: -0.0257\n",
            "Epoch 111/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 1.3820 - relative_error: -0.0094 - val_loss: 3.0152 - val_relative_error: -0.0135\n",
            "Epoch 112/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.5052 - relative_error: -0.0098 - val_loss: 3.8783 - val_relative_error: -0.0154\n",
            "Epoch 113/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.4001 - relative_error: -0.0094 - val_loss: 2.3909 - val_relative_error: -0.0120\n",
            "Epoch 114/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.3770 - relative_error: -0.0094 - val_loss: 2.6635 - val_relative_error: -0.0127\n",
            "Epoch 115/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.6102 - relative_error: -0.0101 - val_loss: 2.4554 - val_relative_error: -0.0124\n",
            "Epoch 116/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.5112 - relative_error: -0.0099 - val_loss: 3.2484 - val_relative_error: -0.0140\n",
            "Epoch 117/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 2.1277 - relative_error: -0.0117 - val_loss: 4.5127 - val_relative_error: -0.0180\n",
            "Epoch 118/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.4362 - relative_error: -0.0095 - val_loss: 2.2101 - val_relative_error: -0.0119\n",
            "Epoch 119/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.4144 - relative_error: -0.0095 - val_loss: 3.0657 - val_relative_error: -0.0139\n",
            "Epoch 120/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.3300 - relative_error: -0.0092 - val_loss: 2.2537 - val_relative_error: -0.0121\n",
            "Epoch 121/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.3882 - relative_error: -0.0095 - val_loss: 2.0691 - val_relative_error: -0.0107\n",
            "Epoch 122/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.2500 - relative_error: -0.0088 - val_loss: 4.5228 - val_relative_error: -0.0170\n",
            "Epoch 123/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 1.7770 - relative_error: -0.0107 - val_loss: 3.9429 - val_relative_error: -0.0166\n",
            "Epoch 124/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.4939 - relative_error: -0.0100 - val_loss: 2.1053 - val_relative_error: -0.0109\n",
            "Epoch 125/300\n",
            "282/282 [==============================] - 35s 124ms/step - loss: 1.2650 - relative_error: -0.0090 - val_loss: 2.1071 - val_relative_error: -0.0112\n",
            "Epoch 126/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.2411 - relative_error: -0.0088 - val_loss: 2.1426 - val_relative_error: -0.0110\n",
            "Epoch 127/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.6906 - relative_error: -0.0103 - val_loss: 2.7000 - val_relative_error: -0.0139\n",
            "Epoch 128/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1009 - relative_error: -0.0083 - val_loss: 2.3158 - val_relative_error: -0.0122\n",
            "Epoch 129/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.1835 - relative_error: -0.0086 - val_loss: 3.0147 - val_relative_error: -0.0133\n",
            "Epoch 130/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.2370 - relative_error: -0.0089 - val_loss: 2.7701 - val_relative_error: -0.0133\n",
            "Epoch 131/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 2.3315 - relative_error: -0.0122 - val_loss: 2.3942 - val_relative_error: -0.0123\n",
            "Epoch 132/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.1037 - relative_error: -0.0083 - val_loss: 3.3960 - val_relative_error: -0.0143\n",
            "Epoch 133/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.2853 - relative_error: -0.0090 - val_loss: 2.5514 - val_relative_error: -0.0130\n",
            "Epoch 134/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.8326 - relative_error: -0.0108 - val_loss: 3.2152 - val_relative_error: -0.0139\n",
            "Epoch 135/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0823 - relative_error: -0.0082 - val_loss: 5.1318 - val_relative_error: -0.0175\n",
            "Epoch 136/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.3495 - relative_error: -0.0092 - val_loss: 2.3360 - val_relative_error: -0.0115\n",
            "Epoch 137/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 1.0674 - relative_error: -0.0083 - val_loss: 5.9523 - val_relative_error: -0.0212\n",
            "Epoch 138/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 2.6801 - relative_error: -0.0129 - val_loss: 2.8400 - val_relative_error: -0.0134\n",
            "Epoch 139/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.2331 - relative_error: -0.0089 - val_loss: 2.1430 - val_relative_error: -0.0113\n",
            "Epoch 140/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0074 - relative_error: -0.0080 - val_loss: 2.3316 - val_relative_error: -0.0118\n",
            "Epoch 141/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0374 - relative_error: -0.0081 - val_loss: 3.3632 - val_relative_error: -0.0146\n",
            "Epoch 142/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.6218 - relative_error: -0.0101 - val_loss: 2.3571 - val_relative_error: -0.0112\n",
            "Epoch 143/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0600 - relative_error: -0.0082 - val_loss: 2.2451 - val_relative_error: -0.0113\n",
            "Epoch 144/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0446 - relative_error: -0.0081 - val_loss: 2.6296 - val_relative_error: -0.0122\n",
            "Epoch 145/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1977 - relative_error: -0.0088 - val_loss: 2.3029 - val_relative_error: -0.0116\n",
            "Epoch 146/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.1168 - relative_error: -0.0085 - val_loss: 2.1548 - val_relative_error: -0.0112\n",
            "Epoch 147/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 1.2971 - relative_error: -0.0092 - val_loss: 2.7990 - val_relative_error: -0.0126\n",
            "Epoch 148/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0523 - relative_error: -0.0082 - val_loss: 2.0180 - val_relative_error: -0.0110\n",
            "Epoch 149/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.9923 - relative_error: -0.0079 - val_loss: 3.3204 - val_relative_error: -0.0147\n",
            "Epoch 150/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1082 - relative_error: -0.0084 - val_loss: 3.6781 - val_relative_error: -0.0143\n",
            "Epoch 151/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1777 - relative_error: -0.0086 - val_loss: 2.0879 - val_relative_error: -0.0113\n",
            "Epoch 152/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9404 - relative_error: -0.0077 - val_loss: 1.6882 - val_relative_error: -0.0098\n",
            "Epoch 153/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1046 - relative_error: -0.0084 - val_loss: 2.7118 - val_relative_error: -0.0124\n",
            "Epoch 154/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0334 - relative_error: -0.0080 - val_loss: 1.8587 - val_relative_error: -0.0105\n",
            "Epoch 155/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.9761 - relative_error: -0.0079 - val_loss: 2.9276 - val_relative_error: -0.0136\n",
            "Epoch 156/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.8592 - relative_error: -0.0109 - val_loss: 18.9416 - val_relative_error: -0.0246\n",
            "Epoch 157/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 6.2323 - relative_error: -0.0194 - val_loss: 3.0620 - val_relative_error: -0.0137\n",
            "Epoch 158/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.2498 - relative_error: -0.0090 - val_loss: 3.0732 - val_relative_error: -0.0141\n",
            "Epoch 159/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0582 - relative_error: -0.0082 - val_loss: 3.2105 - val_relative_error: -0.0154\n",
            "Epoch 160/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.1666 - relative_error: -0.0085 - val_loss: 3.1469 - val_relative_error: -0.0137\n",
            "Epoch 161/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0205 - relative_error: -0.0079 - val_loss: 2.6760 - val_relative_error: -0.0123\n",
            "Epoch 162/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.9402 - relative_error: -0.0076 - val_loss: 2.8664 - val_relative_error: -0.0135\n",
            "Epoch 163/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 1.0530 - relative_error: -0.0082 - val_loss: 2.3086 - val_relative_error: -0.0122\n",
            "Epoch 164/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8559 - relative_error: -0.0074 - val_loss: 1.6791 - val_relative_error: -0.0095\n",
            "Epoch 165/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9461 - relative_error: -0.0077 - val_loss: 1.6405 - val_relative_error: -0.0094\n",
            "Epoch 166/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9442 - relative_error: -0.0078 - val_loss: 2.6072 - val_relative_error: -0.0127\n",
            "Epoch 167/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9275 - relative_error: -0.0077 - val_loss: 2.9300 - val_relative_error: -0.0137\n",
            "Epoch 168/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.2162 - relative_error: -0.0088 - val_loss: 1.8254 - val_relative_error: -0.0103\n",
            "Epoch 169/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.9139 - relative_error: -0.0077 - val_loss: 2.7191 - val_relative_error: -0.0130\n",
            "Epoch 170/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8778 - relative_error: -0.0074 - val_loss: 1.6250 - val_relative_error: -0.0094\n",
            "Epoch 171/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8232 - relative_error: -0.0072 - val_loss: 2.6586 - val_relative_error: -0.0129\n",
            "Epoch 172/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.2167 - relative_error: -0.0088 - val_loss: 2.2588 - val_relative_error: -0.0119\n",
            "Epoch 173/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9082 - relative_error: -0.0075 - val_loss: 2.6981 - val_relative_error: -0.0137\n",
            "Epoch 174/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0425 - relative_error: -0.0080 - val_loss: 1.9429 - val_relative_error: -0.0108\n",
            "Epoch 175/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8809 - relative_error: -0.0074 - val_loss: 2.4327 - val_relative_error: -0.0128\n",
            "Epoch 176/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8424 - relative_error: -0.0073 - val_loss: 1.9269 - val_relative_error: -0.0102\n",
            "Epoch 177/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.8771 - relative_error: -0.0075 - val_loss: 3.1403 - val_relative_error: -0.0150\n",
            "Epoch 178/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0602 - relative_error: -0.0082 - val_loss: 2.3838 - val_relative_error: -0.0130\n",
            "Epoch 179/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9489 - relative_error: -0.0078 - val_loss: 1.8612 - val_relative_error: -0.0105\n",
            "Epoch 180/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.6218 - relative_error: -0.0097 - val_loss: 2.6187 - val_relative_error: -0.0135\n",
            "Epoch 181/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8884 - relative_error: -0.0076 - val_loss: 2.3354 - val_relative_error: -0.0119\n",
            "Epoch 182/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7312 - relative_error: -0.0068 - val_loss: 1.6034 - val_relative_error: -0.0096\n",
            "Epoch 183/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.9439 - relative_error: -0.0078 - val_loss: 2.1279 - val_relative_error: -0.0110\n",
            "Epoch 184/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7817 - relative_error: -0.0071 - val_loss: 2.8444 - val_relative_error: -0.0139\n",
            "Epoch 185/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.8334 - relative_error: -0.0072 - val_loss: 1.9128 - val_relative_error: -0.0106\n",
            "Epoch 186/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7741 - relative_error: -0.0070 - val_loss: 1.9046 - val_relative_error: -0.0106\n",
            "Epoch 187/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.9263 - relative_error: -0.0077 - val_loss: 2.3931 - val_relative_error: -0.0123\n",
            "Epoch 188/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.7582 - relative_error: -0.0070 - val_loss: 2.4538 - val_relative_error: -0.0119\n",
            "Epoch 189/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.7493 - relative_error: -0.0069 - val_loss: 2.7745 - val_relative_error: -0.0131\n",
            "Epoch 190/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0058 - relative_error: -0.0081 - val_loss: 2.0758 - val_relative_error: -0.0111\n",
            "Epoch 191/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.8939 - relative_error: -0.0076 - val_loss: 5.8807 - val_relative_error: -0.0188\n",
            "Epoch 192/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0939 - relative_error: -0.0084 - val_loss: 2.1301 - val_relative_error: -0.0112\n",
            "Epoch 193/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.8026 - relative_error: -0.0071 - val_loss: 1.7127 - val_relative_error: -0.0097\n",
            "Epoch 194/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9305 - relative_error: -0.0076 - val_loss: 2.1138 - val_relative_error: -0.0118\n",
            "Epoch 195/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.8136 - relative_error: -0.0072 - val_loss: 2.4464 - val_relative_error: -0.0131\n",
            "Epoch 196/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7350 - relative_error: -0.0069 - val_loss: 2.1424 - val_relative_error: -0.0115\n",
            "Epoch 197/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.8877 - relative_error: -0.0075 - val_loss: 2.3674 - val_relative_error: -0.0120\n",
            "Epoch 198/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6499 - relative_error: -0.0063 - val_loss: 1.5916 - val_relative_error: -0.0093\n",
            "Epoch 199/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.7285 - relative_error: -0.0068 - val_loss: 1.8387 - val_relative_error: -0.0106\n",
            "Epoch 200/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9083 - relative_error: -0.0077 - val_loss: 5.4749 - val_relative_error: -0.0207\n",
            "Epoch 201/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.4643 - relative_error: -0.0096 - val_loss: 2.1046 - val_relative_error: -0.0116\n",
            "Epoch 202/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9034 - relative_error: -0.0073 - val_loss: 1.9361 - val_relative_error: -0.0106\n",
            "Epoch 203/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6710 - relative_error: -0.0065 - val_loss: 2.0387 - val_relative_error: -0.0117\n",
            "Epoch 204/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8064 - relative_error: -0.0072 - val_loss: 2.0904 - val_relative_error: -0.0119\n",
            "Epoch 205/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0610 - relative_error: -0.0082 - val_loss: 2.8108 - val_relative_error: -0.0126\n",
            "Epoch 206/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7514 - relative_error: -0.0069 - val_loss: 2.3865 - val_relative_error: -0.0123\n",
            "Epoch 207/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.5752 - relative_error: -0.0100 - val_loss: 2.4889 - val_relative_error: -0.0121\n",
            "Epoch 208/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.0678 - relative_error: -0.0083 - val_loss: 2.2853 - val_relative_error: -0.0119\n",
            "Epoch 209/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6687 - relative_error: -0.0065 - val_loss: 4.2948 - val_relative_error: -0.0143\n",
            "Epoch 210/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.2197 - relative_error: -0.0088 - val_loss: 1.6705 - val_relative_error: -0.0100\n",
            "Epoch 211/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.6864 - relative_error: -0.0066 - val_loss: 1.9604 - val_relative_error: -0.0109\n",
            "Epoch 212/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1897 - relative_error: -0.0086 - val_loss: 1.7142 - val_relative_error: -0.0099\n",
            "Epoch 213/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7303 - relative_error: -0.0069 - val_loss: 6.0440 - val_relative_error: -0.0165\n",
            "Epoch 214/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.9469 - relative_error: -0.0077 - val_loss: 1.7708 - val_relative_error: -0.0103\n",
            "Epoch 215/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6439 - relative_error: -0.0064 - val_loss: 1.6496 - val_relative_error: -0.0099\n",
            "Epoch 216/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6524 - relative_error: -0.0064 - val_loss: 1.5299 - val_relative_error: -0.0092\n",
            "Epoch 217/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6185 - relative_error: -0.0063 - val_loss: 2.0425 - val_relative_error: -0.0109\n",
            "Epoch 218/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7085 - relative_error: -0.0067 - val_loss: 3.7835 - val_relative_error: -0.0150\n",
            "Epoch 219/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7642 - relative_error: -0.0070 - val_loss: 8.4959 - val_relative_error: -0.0267\n",
            "Epoch 220/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 2.4699 - relative_error: -0.0123 - val_loss: 1.9107 - val_relative_error: -0.0103\n",
            "Epoch 221/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7609 - relative_error: -0.0069 - val_loss: 1.6913 - val_relative_error: -0.0100\n",
            "Epoch 222/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.6330 - relative_error: -0.0063 - val_loss: 2.0858 - val_relative_error: -0.0111\n",
            "Epoch 223/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7575 - relative_error: -0.0069 - val_loss: 1.7811 - val_relative_error: -0.0105\n",
            "Epoch 224/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5800 - relative_error: -0.0060 - val_loss: 1.9114 - val_relative_error: -0.0109\n",
            "Epoch 225/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7634 - relative_error: -0.0069 - val_loss: 2.7776 - val_relative_error: -0.0127\n",
            "Epoch 226/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5641 - relative_error: -0.0059 - val_loss: 1.8469 - val_relative_error: -0.0108\n",
            "Epoch 227/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7217 - relative_error: -0.0068 - val_loss: 3.2432 - val_relative_error: -0.0132\n",
            "Epoch 228/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6928 - relative_error: -0.0066 - val_loss: 2.3250 - val_relative_error: -0.0117\n",
            "Epoch 229/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7834 - relative_error: -0.0071 - val_loss: 2.2529 - val_relative_error: -0.0124\n",
            "Epoch 230/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6808 - relative_error: -0.0065 - val_loss: 2.0959 - val_relative_error: -0.0111\n",
            "Epoch 231/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7228 - relative_error: -0.0067 - val_loss: 2.3167 - val_relative_error: -0.0122\n",
            "Epoch 232/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6055 - relative_error: -0.0062 - val_loss: 2.0369 - val_relative_error: -0.0106\n",
            "Epoch 233/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.7366 - relative_error: -0.0069 - val_loss: 2.9562 - val_relative_error: -0.0136\n",
            "Epoch 234/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.8559 - relative_error: -0.0072 - val_loss: 1.8557 - val_relative_error: -0.0103\n",
            "Epoch 235/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.6430 - relative_error: -0.0063 - val_loss: 1.9058 - val_relative_error: -0.0111\n",
            "Epoch 236/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.7144 - relative_error: -0.0068 - val_loss: 1.7264 - val_relative_error: -0.0100\n",
            "Epoch 237/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6146 - relative_error: -0.0063 - val_loss: 1.6302 - val_relative_error: -0.0094\n",
            "Epoch 238/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7676 - relative_error: -0.0071 - val_loss: 1.6286 - val_relative_error: -0.0096\n",
            "Epoch 239/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5896 - relative_error: -0.0061 - val_loss: 1.6889 - val_relative_error: -0.0097\n",
            "Epoch 240/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6774 - relative_error: -0.0066 - val_loss: 2.2850 - val_relative_error: -0.0116\n",
            "Epoch 241/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7672 - relative_error: -0.0070 - val_loss: 1.5773 - val_relative_error: -0.0093\n",
            "Epoch 242/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5733 - relative_error: -0.0060 - val_loss: 1.6921 - val_relative_error: -0.0099\n",
            "Epoch 243/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6952 - relative_error: -0.0066 - val_loss: 12.8687 - val_relative_error: -0.0263\n",
            "Epoch 244/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 1.5975 - relative_error: -0.0099 - val_loss: 3.4584 - val_relative_error: -0.0153\n",
            "Epoch 245/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.1123 - relative_error: -0.0085 - val_loss: 1.6922 - val_relative_error: -0.0100\n",
            "Epoch 246/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5357 - relative_error: -0.0058 - val_loss: 2.5536 - val_relative_error: -0.0128\n",
            "Epoch 247/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.6430 - relative_error: -0.0064 - val_loss: 1.6341 - val_relative_error: -0.0100\n",
            "Epoch 248/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6230 - relative_error: -0.0063 - val_loss: 1.8238 - val_relative_error: -0.0103\n",
            "Epoch 249/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5559 - relative_error: -0.0059 - val_loss: 1.4647 - val_relative_error: -0.0089\n",
            "Epoch 250/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.8205 - relative_error: -0.0072 - val_loss: 2.0804 - val_relative_error: -0.0116\n",
            "Epoch 251/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.6322 - relative_error: -0.0063 - val_loss: 2.5325 - val_relative_error: -0.0122\n",
            "Epoch 252/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.9440 - relative_error: -0.0075 - val_loss: 1.8478 - val_relative_error: -0.0106\n",
            "Epoch 253/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6041 - relative_error: -0.0061 - val_loss: 1.8828 - val_relative_error: -0.0104\n",
            "Epoch 254/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6001 - relative_error: -0.0061 - val_loss: 2.3421 - val_relative_error: -0.0127\n",
            "Epoch 255/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6634 - relative_error: -0.0064 - val_loss: 1.7899 - val_relative_error: -0.0107\n",
            "Epoch 256/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6085 - relative_error: -0.0063 - val_loss: 2.0711 - val_relative_error: -0.0110\n",
            "Epoch 257/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5849 - relative_error: -0.0061 - val_loss: 1.6748 - val_relative_error: -0.0099\n",
            "Epoch 258/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7425 - relative_error: -0.0069 - val_loss: 1.8356 - val_relative_error: -0.0107\n",
            "Epoch 259/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5781 - relative_error: -0.0061 - val_loss: 2.3516 - val_relative_error: -0.0120\n",
            "Epoch 260/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.7127 - relative_error: -0.0068 - val_loss: 1.6129 - val_relative_error: -0.0098\n",
            "Epoch 261/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5345 - relative_error: -0.0058 - val_loss: 2.3181 - val_relative_error: -0.0121\n",
            "Epoch 262/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6015 - relative_error: -0.0061 - val_loss: 2.5151 - val_relative_error: -0.0137\n",
            "Epoch 263/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6253 - relative_error: -0.0063 - val_loss: 1.4907 - val_relative_error: -0.0090\n",
            "Epoch 264/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.4970 - relative_error: -0.0056 - val_loss: 2.7910 - val_relative_error: -0.0145\n",
            "Epoch 265/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5811 - relative_error: -0.0061 - val_loss: 2.3690 - val_relative_error: -0.0129\n",
            "Epoch 266/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6673 - relative_error: -0.0065 - val_loss: 1.6659 - val_relative_error: -0.0099\n",
            "Epoch 267/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.5518 - relative_error: -0.0059 - val_loss: 2.5072 - val_relative_error: -0.0128\n",
            "Epoch 268/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6321 - relative_error: -0.0063 - val_loss: 1.6692 - val_relative_error: -0.0101\n",
            "Epoch 269/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5526 - relative_error: -0.0058 - val_loss: 3.2140 - val_relative_error: -0.0128\n",
            "Epoch 270/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6378 - relative_error: -0.0063 - val_loss: 1.9588 - val_relative_error: -0.0108\n",
            "Epoch 271/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5251 - relative_error: -0.0057 - val_loss: 9.7656 - val_relative_error: -0.0294\n",
            "Epoch 272/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 1.2170 - relative_error: -0.0087 - val_loss: 2.3981 - val_relative_error: -0.0128\n",
            "Epoch 273/300\n",
            "282/282 [==============================] - 35s 125ms/step - loss: 0.5358 - relative_error: -0.0058 - val_loss: 1.8479 - val_relative_error: -0.0108\n",
            "Epoch 274/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6016 - relative_error: -0.0059 - val_loss: 2.6460 - val_relative_error: -0.0126\n",
            "Epoch 275/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.4827 - relative_error: -0.0054 - val_loss: 4.9253 - val_relative_error: -0.0192\n",
            "Epoch 276/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 1.0021 - relative_error: -0.0081 - val_loss: 1.8583 - val_relative_error: -0.0101\n",
            "Epoch 277/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6608 - relative_error: -0.0064 - val_loss: 1.5822 - val_relative_error: -0.0095\n",
            "Epoch 278/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5144 - relative_error: -0.0056 - val_loss: 1.7814 - val_relative_error: -0.0102\n",
            "Epoch 279/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6143 - relative_error: -0.0063 - val_loss: 2.0504 - val_relative_error: -0.0107\n",
            "Epoch 280/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5159 - relative_error: -0.0057 - val_loss: 1.4909 - val_relative_error: -0.0093\n",
            "Epoch 281/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.4441 - relative_error: -0.0052 - val_loss: 1.5577 - val_relative_error: -0.0092\n",
            "Epoch 282/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.5692 - relative_error: -0.0060 - val_loss: 2.2771 - val_relative_error: -0.0128\n",
            "Epoch 283/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.6163 - relative_error: -0.0063 - val_loss: 1.7807 - val_relative_error: -0.0104\n",
            "Epoch 284/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.6173 - relative_error: -0.0062 - val_loss: 2.8308 - val_relative_error: -0.0126\n",
            "Epoch 285/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.5371 - relative_error: -0.0059 - val_loss: 1.8881 - val_relative_error: -0.0111\n",
            "Epoch 286/300\n",
            "282/282 [==============================] - 35s 126ms/step - loss: 0.5027 - relative_error: -0.0056 - val_loss: 1.9244 - val_relative_error: -0.0107\n",
            "Epoch 287/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5320 - relative_error: -0.0058 - val_loss: 2.5980 - val_relative_error: -0.0122\n",
            "Epoch 288/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6089 - relative_error: -0.0062 - val_loss: 1.4886 - val_relative_error: -0.0089\n",
            "Epoch 289/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.4851 - relative_error: -0.0055 - val_loss: 1.9441 - val_relative_error: -0.0106\n",
            "Epoch 290/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5367 - relative_error: -0.0057 - val_loss: 1.4381 - val_relative_error: -0.0091\n",
            "Epoch 291/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5949 - relative_error: -0.0062 - val_loss: 1.9531 - val_relative_error: -0.0107\n",
            "Epoch 292/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.6121 - relative_error: -0.0062 - val_loss: 1.5731 - val_relative_error: -0.0094\n",
            "Epoch 293/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.5288 - relative_error: -0.0058 - val_loss: 1.6067 - val_relative_error: -0.0095\n",
            "Epoch 294/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5573 - relative_error: -0.0060 - val_loss: 1.6086 - val_relative_error: -0.0096\n",
            "Epoch 295/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.5221 - relative_error: -0.0057 - val_loss: 1.4958 - val_relative_error: -0.0090\n",
            "Epoch 296/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.5573 - relative_error: -0.0060 - val_loss: 1.6940 - val_relative_error: -0.0097\n",
            "Epoch 297/300\n",
            "282/282 [==============================] - 36s 128ms/step - loss: 0.5076 - relative_error: -0.0057 - val_loss: 1.6481 - val_relative_error: -0.0099\n",
            "Epoch 298/300\n",
            "282/282 [==============================] - 36s 126ms/step - loss: 0.4220 - relative_error: -0.0051 - val_loss: 1.5851 - val_relative_error: -0.0096\n",
            "Epoch 299/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.4644 - relative_error: -0.0054 - val_loss: 2.3716 - val_relative_error: -0.0116\n",
            "Epoch 300/300\n",
            "282/282 [==============================] - 36s 127ms/step - loss: 0.6067 - relative_error: -0.0062 - val_loss: 2.2492 - val_relative_error: -0.0122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "f5j_VWLCPY0H",
        "outputId": "628da730-483a-4cae-bb3d-2a917f71a3ca"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(np.abs(history.history['relative_error']))\n",
        "plt.plot(np.abs(history.history['val_relative_error']))\n",
        "plt.title('model relative error')\n",
        "plt.ylabel('relative error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'relative_error', 'val_loss', 'val_relative_error'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPXacBAry0S",
        "outputId": "d39e870e-bafa-440c-f542-9cfcca002861"
      },
      "source": [
        "vae_model.save(\"/content/gdrive/MyDrive/saved_models/pressure_le1-4.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
