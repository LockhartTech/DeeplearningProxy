{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pressure_Grid_Search_lr1e-4_batch32.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP3nlDbwWP2pMUxDa2LUCDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-srm3018/DeeplearningProxy/blob/main/Gridsearch/Pressure_Grid_Search_lr1e_4_batch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9PQE47vGAl_",
        "outputId": "d2bbcf24-61b2-45c8-d8a1-4bb2892571ec"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 25 19:01:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi27NRCrGRba",
        "outputId": "ac26dc04-6e44-4613-bab8-9c920c5e4b11"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "9D0wgpehaCVX",
        "outputId": "70a48c5a-c8f2-4eaa-a706-f2db85313118"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-537d2bfd-9158-463e-9345-5886ca00c0b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-537d2bfd-9158-463e-9345-5886ca00c0b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving layers.py to layers.py\n",
            "Saving unet_uae.py to unet_uae.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers.py': b'\"\"\"Import required libraries and modules.\"\"\"\\r\\n\\r\\nimport tensorflow as tf\\r\\nfrom keras import backend as K\\r\\nfrom keras.engine.topology import Layer\\r\\nfrom keras.layers.merge import add\\r\\n# from keras.engine import InputSpec\\r\\nfrom keras.layers import InputSpec\\r\\nfrom keras.layers.core import Activation\\r\\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\\r\\nfrom keras.layers import BatchNormalization, ConvLSTM2D\\r\\nfrom keras.layers import TimeDistributed, Reshape, RepeatVector\\r\\nfrom keras import regularizers\\r\\n\\r\\n\\r\\nreg_weights = 0.00001\\r\\n\\r\\n\\r\\ndef conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride,\\r\\n                   padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(\"relu\")(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef time_conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                            strides=stride,\\r\\n                            padding=\\'same\\',\\r\\n                            kernel_regularizer=regularizers.\\r\\n                            l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(\"relu\"))(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    *default = (1,1)\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        a = BatchNormalization()(a)\\r\\n        a = Activation(\"relu\")(a)\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(a)\\r\\n        y = BatchNormalization()(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef time_res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        a = TimeDistributed(BatchNormalization())(a)\\r\\n        a = TimeDistributed(Activation(\"relu\"))(a)\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        y = TimeDistributed(BatchNormalization())(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef dconv_bn_nolinear(nb_filter, nb_row, nb_col, stride=(2, 2),\\r\\n                      activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = UnPooling2D(size=stride)(x)\\r\\n        x = ReflectionPadding2D(padding=(int(nb_row/2), int(nb_col/2)))(x)\\r\\n        x = Conv2D(nb_filter, (nb_row, nb_col), padding=\\'valid\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(activation)(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\ndef time_dconv_bn_nolinear(nb_filter, nb_row, nb_col,\\r\\n                           stride=(2, 2), activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create time convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = TimeDistributed(UnPooling2D(size=stride))(x)\\r\\n        x = TimeDistributed(ReflectionPadding2D(padding=(int(nb_row/2),\\r\\n                            int(nb_col/2))))(x)\\r\\n        x = TimeDistributed(Conv2D(nb_filter, (nb_row, nb_col),\\r\\n                                   padding=\\'valid\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(activation))(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\nclass ReflectionPadding2D(Layer):\\r\\n    \"\"\"class for reflectionPadding2D.\"\"\"\\r\\n\\r\\n    def __init__(self, padding=(1, 1), data_format=\"channels_last\", **kwargs):\\r\\n        \"\"\"\\r\\n        Construct class parameters.\\r\\n\\r\\n        parameters:\\r\\n        -------\\r\\n        padding\\r\\n        dim_ordering\\r\\n        \"\"\"\\r\\n        super(ReflectionPadding2D, self).__init__(**kwargs)\\r\\n\\r\\n        if data_format == \\'channels_last\\':\\r\\n            dim_ordering = K.image_data_format()\\r\\n\\r\\n        self.padding = padding\\r\\n        if isinstance(padding, dict):\\r\\n            if set(padding.keys()) <= {\\'top_pad\\', \\'bottom_pad\\',\\r\\n                                       \\'left_pad\\', \\'right_pad\\'}:\\r\\n                self.top_pad = padding.get(\\'top_pad\\', 0)\\r\\n                self.bottom_pad = padding.get(\\'bottom_pad\\', 0)\\r\\n                self.left_pad = padding.get(\\'left_pad\\', 0)\\r\\n                self.right_pad = padding.get(\\'right_pad\\', 0)\\r\\n            else:\\r\\n                raise ValueError(\\'Unexpected key\\'\\r\\n                                 \\'found in `padding` dictionary.\\'\\r\\n                                 \\'Keys have to be in {\"top_pad\", \"bottom_pad\",\\'\\r\\n                                 \\'\"left_pad\", \"right_pad\"}.\\'\\r\\n                                 \\'Found: \\' + str(padding.keys()))\\r\\n        else:\\r\\n            padding = tuple(padding)\\r\\n            if len(padding) == 2:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[0]\\r\\n                self.left_pad = padding[1]\\r\\n                self.right_pad = padding[1]\\r\\n            elif len(padding) == 4:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[1]\\r\\n                self.left_pad = padding[2]\\r\\n                self.right_pad = padding[3]\\r\\n            else:\\r\\n                raise TypeError(\\'`padding` should be tuple of int \\'\\r\\n                                \\'of length 2 or 4, or dict. \\'\\r\\n                                \\'Found: \\' + str(padding))\\r\\n\\r\\n        # if data_format not in {\\'channels_last\\'}:\\r\\n        #     raise ValueError(\\'data_format must be in {\"channels_last\"}.\\')\\r\\n        self.data_format = data_format\\r\\n        self.input_spec = [InputSpec(ndim=4)]\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call x to apply padding.\"\"\"\\r\\n        top_pad = self.top_pad\\r\\n        bottom_pad = self.bottom_pad\\r\\n        left_pad = self.left_pad\\r\\n        right_pad = self.right_pad\\r\\n\\r\\n        paddings = [[0, 0], [left_pad, right_pad],\\r\\n                    [top_pad, bottom_pad], [0, 0]]\\r\\n\\r\\n        return tf.pad(x, paddings, mode=\\'REFLECT\\', name=None)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"\\r\\n        Compute the shape of output.\\r\\n\\r\\n        Parameters:\\r\\n        --------\\r\\n        input_shape: Tuple\\r\\n        shape of input\\r\\n        \"\"\"\\r\\n        if self.data_format == \\'channels_last\\':\\r\\n            rows = input_shape[1] + self.top_pad + self.bottom_pad\\r\\n            cols = input_shape[2] + self.left_pad + self.right_pad\\r\\n\\r\\n            return (input_shape[0],\\r\\n                    rows,\\r\\n                    cols,\\r\\n                    input_shape[3])\\r\\n        else:\\r\\n            raise ValueError(\\'Invalid data_format:\\', self.data_format)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get the Configure.\"\"\"\\r\\n        config = {\\'padding\\': self.padding}\\r\\n        base_config = super(ReflectionPadding2D, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n\\r\\n\\r\\nclass UnPooling2D(UpSampling2D):\\r\\n    \"\"\"Unpool 2D from 2D upsampling.\"\"\"\\r\\n\\r\\n    def __init__(self, size=(2, 2)):\\r\\n        \"\"\"Construct size.\"\"\"\\r\\n        super(UnPooling2D, self).__init__(size)\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call th x data.\"\"\"\\r\\n        shapes = x.get_shape().as_list()\\r\\n        w = self.size[0] * shapes[1]\\r\\n        h = self.size[1] * shapes[2]\\r\\n        return tf.image.resize(x, (w, h))\\r\\n\\r\\n\\r\\nclass InstanceNormalize(Layer):\\r\\n    \"\"\"Normalization Instance of class.\"\"\"\\r\\n\\r\\n    def __init__(self, **kwargs):\\r\\n        \"\"\"Initialize the keyaarguments.\"\"\"\\r\\n        super(InstanceNormalize, self).__init__(**kwargs)\\r\\n        self.epsilon = 1e-3\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call mean and variance for normalization.\"\"\"\\r\\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\\r\\n        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute the shape of output.\"\"\"\\r\\n        return input_shape\\r\\n\\r\\n\\r\\nclass RepeatConv(Layer):\\r\\n    \"\"\"\\r\\n    Repeats the input n times.\\r\\n\\r\\n    Example:\\r\\n    -------\\r\\n        model = Sequential()\\r\\n        model.add(Dense(32, input_dim=32))\\r\\n        now: model.output_shape == (None, 32)\\r\\n        note: `None` is the batch dimension\\r\\n        model.add(RepeatVector(3))\\r\\n        now: model.output_shape == (None, 3, 32)\\r\\n\\r\\n    Arguments\\r\\n    ---------\\r\\n        n: integer, repetition factor.\\r\\n    Input shape\\r\\n    ----------\\r\\n        4D tensor of shape `(num_samples, w, h, c)`.\\r\\n    Output shape\\r\\n    -----------\\r\\n        5D tensor of shape `(num_samples, n, w, h, c)`.\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, n, **kwargs):\\r\\n        \"\"\"Initialize the class parameters.\"\"\"\\r\\n        super(RepeatConv, self).__init__(**kwargs)\\r\\n        self.n = n\\r\\n        self.input_spec = InputSpec(ndim=4)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute output shape.\"\"\"\\r\\n        return (input_shape[0], self.n, input_shape[1],\\r\\n                input_shape[2], input_shape[3])\\r\\n\\r\\n    def call(self, inputs):\\r\\n        \"\"\"Call the inputs.\"\"\"\\r\\n        x = K.expand_dims(inputs, 1)\\r\\n        pattern = tf.stack([1, self.n, 1, 1, 1])\\r\\n        return K.tile(x, pattern)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get configure.\"\"\"\\r\\n        config = {\\'n\\': self.n}\\r\\n        base_config = super(RepeatConv, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n',\n",
              " 'unet_uae.py': b'\"\"\"Path hack to make tests work.\"\"\"\\r\\nfrom layers import *\\r\\nfrom keras import backend as K\\r\\nfrom keras.layers import Input, Flatten, Dense, Lambda, Reshape\\r\\nfrom keras.layers import concatenate, TimeDistributed, RepeatVector, ConvLSTM2D\\r\\nfrom keras.models import Model\\r\\nimport numpy as np\\r\\n\\r\\n\\r\\ndef create_vae(input_shape, depth):\\r\\n    \"\"\"\\r\\n    Create VAE to create something new.\\r\\n\\r\\n    Parameters:\\r\\n    -------\\r\\n    input_shape : Tuple\\r\\n    depth : int\\r\\n    Returns:\\r\\n    -------\\r\\n    encoder: encoder\\r\\n    model : recurrnet R-UNET model\\r\\n\\r\\n    \"\"\"\\r\\n    # Encoder\\r\\n    input = Input(shape=input_shape, name=\\'image\\')\\r\\n\\r\\n    enc1 = conv_bn_relu(16, 3, 3, stride=(2, 2))(input)\\r\\n    time_enc1 = RepeatConv(depth)(enc1)\\r\\n    enc2 = conv_bn_relu(32, 3, 3, stride=(1, 1))(enc1)\\r\\n    time_enc2 = RepeatConv(depth)(enc2)\\r\\n    enc3 = conv_bn_relu(64, 3, 3, stride=(2, 2))(enc2)\\r\\n    time_enc3 = RepeatConv(depth)(enc3)\\r\\n    enc4 = conv_bn_relu(128, 3, 3, stride=(1, 1))(enc3)\\r\\n    time_enc4 = RepeatConv(depth)(enc4)\\r\\n\\r\\n    x = res_conv(128, 3, 3)(enc4)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n\\r\\n    encoder = Model(input, x, name=\\'encoder\\')\\r\\n\\r\\n    x = RepeatConv(depth)(enc4)\\r\\n    x = ConvLSTM2D(128, (3, 3), strides=(1, 1),\\r\\n                   padding=\\'same\\', activation=\\'relu\\',\\r\\n                   return_sequences=True)(x)\\r\\n    # x = ConvLSTM2D(64, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    # x = ConvLSTM2D(128, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    dec4 = time_res_conv(128, 3, 3)(x)\\r\\n\\r\\n    merge4 = concatenate([time_enc4, dec4], axis=4)\\r\\n    dec3 = time_dconv_bn_nolinear(128, 3, 3, stride=(1, 1))(merge4)\\r\\n    merge3 = concatenate([time_enc3, dec3], axis=4)\\r\\n    dec2 = time_dconv_bn_nolinear(64, 3, 3, stride=(2, 2))(merge3)\\r\\n    merge2 = concatenate([time_enc2, dec2], axis=4)\\r\\n    dec1 = time_dconv_bn_nolinear(32, 3, 3, stride=(1, 1))(merge2)\\r\\n    merge1 = concatenate([time_enc1, dec1], axis=4)\\r\\n    dec0 = time_dconv_bn_nolinear(16, 3, 3, stride=(2, 2))(merge1)\\r\\n\\r\\n    output = TimeDistributed(Conv2D(1, (3, 3), padding=\\'same\\',\\r\\n                                    activation=None))(dec0)\\r\\n    print(\\'output shape is \\', K.int_shape(output))\\r\\n    # Full net\\r\\n    full_model = Model(input, output)\\r\\n\\r\\n    return full_model, encoder\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrWDL1qVfMw"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import unet_uae as vae_util\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf \n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zr0u9vQzf5R",
        "outputId": "cc65b1fb-0d91-4af3-f0e0-78965b0e9a74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlbNGMtBA_Fs"
      },
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Function to load datasets in format .NPY\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    path : string\n",
        "        The absolute path of where data saved in local system\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    loaded_data : ndarray\n",
        "        The data which was loaded\n",
        "    \"\"\"\n",
        "    loaded_data = np.load(path)\n",
        "    return loaded_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cErfklo76F21"
      },
      "source": [
        "# Two common methods for feature scaling is : 1-Normalization & 2-Standardaisation\n",
        "\n",
        "def normalize(data):\n",
        "    \"\"\"\n",
        "    this function used for Max-Min Normalization (Min-Max scaling) by re-scaling\n",
        "    features with a distribution value between 0 and 1. For every feature,the minimum\n",
        "    value of that feature gets transformed into 0, and the maximum value \n",
        "    gets transformed into 1\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    norm_data : ndarray\n",
        "        The normalized data which transformed into 0 and 1\n",
        "    \"\"\"\n",
        "    max_p = np.max(data[:, :, :, :])\n",
        "    min_p = np.min(data[:, :, :, :])\n",
        "    norm_data = (data - min_p)/(max_p - min_p)\n",
        "    return norm_data\n",
        "\n",
        "def standardize(data):\n",
        "    \"\"\"\n",
        "    this function used for rescaling faetures to ensure the mean\n",
        "    and the standard deviation to be 0 and 1, respectively.\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The standardized data which the mean\n",
        "    and the standard deviation to be 0 and 1\n",
        "    \"\"\"\n",
        "    data_mean = np.mean(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    data_std = np.std(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    std_data = (data - data_mean)/(data_std)\n",
        "    return std_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uviUO--fC_pp"
      },
      "source": [
        "# define the absolute path of training datatsat\n",
        "path_perm = '/content/gdrive/MyDrive/perm.npy'\n",
        "path_press = '/content/gdrive/MyDrive/pressure.npy'\n",
        "# use load_data function nd above path to loading data\n",
        "X_data= load_data(path_perm)\n",
        "target_data = load_data(path_press)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5s3_Zex6sQs"
      },
      "source": [
        "# Normalize data using abov normalize function\n",
        "train_nr = 2250\n",
        "test_nr = 750"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yq6Ouns_EIN",
        "outputId": "d13e1da9-091d-4750-baca-f9e6bb2d4d80"
      },
      "source": [
        "p_t_mean = np.mean(target_data[:train_nr, ...], axis = 0, keepdims = True)\n",
        "target_data = target_data - p_t_mean\n",
        "print('max p is ', np.max(target_data[:train_nr, ...]), ', min p is ', np.min(target_data[:train_nr, ...]))\n",
        "max_p = np.max(target_data[:train_nr, ...])\n",
        "min_p = np.min(target_data[:train_nr, ...])\n",
        "target_data = (target_data - min_p)/(max_p -min_p) - 0.5\n",
        "print('max p is ', np.max(target_data), ', min p is ', np.min(target_data))\n",
        "print('max p train is ', np.max(target_data[:train_nr, ...]), ', min p train is ', np.min(target_data[:train_nr, ...]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max p is  516.6753418240017 , min p is  -135.6267126736111\n",
            "max p is  0.5039995270719013 , min p is  -0.500166926837037\n",
            "max p train is  0.5 , min p train is  -0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q5C0284nFxJ",
        "outputId": "235fa13b-7dd5-46b1-d345-fc11d89d89fc"
      },
      "source": [
        "input_shape=(100, 100, 2)\n",
        "depth = 10\n",
        "vae_model,_ = vae_util.create_vae(input_shape, depth)\n",
        "vae_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output shape is  (None, 10, 100, 100, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 100, 100, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 50, 50, 16)   304         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 50, 50, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 50, 50, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 50, 50, 32)   4640        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 50, 50, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 50, 50, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 25, 25, 64)   18496       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 25, 25, 128)  73856       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 25, 25, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 25, 25, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_4 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)       (None, 10, 25, 25, 1 1180160     repeat_conv_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 10, 25, 25, 1 147584      conv_lst_m2d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 10, 25, 25, 1 0           conv_lst_m2d[0][0]               \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 10, 25, 25, 1 147584      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 10, 25, 25, 1 0           add_3[0][0]                      \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_13 (TimeDistri (None, 10, 25, 25, 1 147584      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_14 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_3 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 10, 25, 25, 1 0           add_4[0][0]                      \n",
            "                                                                 time_distributed_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 25, 25, 2 0           repeat_conv_3[0][0]              \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_15 (TimeDistri (None, 10, 25, 25, 2 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 10, 27, 27, 2 0           time_distributed_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 10, 25, 25, 1 295040      time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_2 (RepeatConv)      (None, 10, 25, 25, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_19 (TimeDistri (None, 10, 25, 25, 1 0           time_distributed_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 25, 25, 1 0           repeat_conv_2[0][0]              \n",
            "                                                                 time_distributed_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_20 (TimeDistri (None, 10, 50, 50, 1 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_21 (TimeDistri (None, 10, 52, 52, 1 0           time_distributed_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_22 (TimeDistri (None, 10, 50, 50, 6 110656      time_distributed_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_23 (TimeDistri (None, 10, 50, 50, 6 256         time_distributed_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_1 (RepeatConv)      (None, 10, 50, 50, 3 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_24 (TimeDistri (None, 10, 50, 50, 6 0           time_distributed_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 50, 50, 9 0           repeat_conv_1[0][0]              \n",
            "                                                                 time_distributed_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_25 (TimeDistri (None, 10, 50, 50, 9 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_26 (TimeDistri (None, 10, 52, 52, 9 0           time_distributed_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_27 (TimeDistri (None, 10, 50, 50, 3 27680       time_distributed_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_28 (TimeDistri (None, 10, 50, 50, 3 128         time_distributed_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv (RepeatConv)        (None, 10, 50, 50, 1 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_29 (TimeDistri (None, 10, 50, 50, 3 0           time_distributed_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 50, 50, 4 0           repeat_conv[0][0]                \n",
            "                                                                 time_distributed_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_30 (TimeDistri (None, 10, 100, 100, 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_31 (TimeDistri (None, 10, 102, 102, 0           time_distributed_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_32 (TimeDistri (None, 10, 100, 100, 6928        time_distributed_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_33 (TimeDistri (None, 10, 100, 100, 64          time_distributed_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_34 (TimeDistri (None, 10, 100, 100, 0           time_distributed_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_35 (TimeDistri (None, 10, 100, 100, 145         time_distributed_34[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 2,164,113\n",
            "Trainable params: 2,162,385\n",
            "Non-trainable params: 1,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4PDSq-qnONZ",
        "outputId": "6df3f293-1a48-4e7a-b608-fbfbfd44f6d3"
      },
      "source": [
        "depth = 10\n",
        "nr = X_data.shape[0]\n",
        "train_nr = 2250\n",
        "test_nr = 750\n",
        "train_x = np.concatenate([X_data[:train_nr,[0], ...],target_data[:train_nr,[0], ...]], axis = 1)\n",
        "train_y = target_data[:train_nr, ...]\n",
        "\n",
        "test_x = np.concatenate([X_data[nr-test_nr:,[0], ...], target_data[nr-test_nr:, [0], ...]], axis = 1)\n",
        "test_y = target_data[nr-test_nr:,...]\n",
        "\n",
        "\n",
        "train_x = train_x.transpose(0,2,3,1)\n",
        "train_y = train_y[:,:,:,:,None]\n",
        "test_x = test_x.transpose(0,2,3,1)\n",
        "test_y = test_y[:,:,:,:,None]\n",
        "#test_y = test_y.transpose(0,2,3,1)\n",
        "print('train_x shape is ', train_x.shape)\n",
        "print('train_y shape is ', train_y.shape)\n",
        "print('test_x shape is ', test_x.shape)\n",
        "print('test_y shape is ', test_y.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape is  (2250, 100, 100, 2)\n",
            "train_y shape is  (2250, 10, 100, 100, 1)\n",
            "test_x shape is  (750, 100, 100, 2)\n",
            "test_y shape is  (750, 10, 100, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsuLGQpniCs"
      },
      "source": [
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/saved_models/'\n",
        "epochs = 300\n",
        "batch_size = 4\n",
        "num_batch = int(train_nr/batch_size) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EqIECRbnoTX"
      },
      "source": [
        "def vae_loss(x, t_decoded):\n",
        "    '''Total loss for the plain UAE'''\n",
        "    return K.mean(reconstruction_loss(x, t_decoded))\n",
        "\n",
        "\n",
        "def reconstruction_loss(x, t_decoded):\n",
        "    '''Reconstruction loss for the plain UAE'''\n",
        "\n",
        "    return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
        "\n",
        "def relative_error(x, t_decoded):\n",
        "    return K.mean(K.abs(x - t_decoded) / x)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m46Y8eknsmA"
      },
      "source": [
        "opt = Adam(learning_rate=1e-4)\n",
        "vae_model.compile(loss = vae_loss, optimizer = opt, metrics = [relative_error])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lttghynSnzzT"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "lrScheduler = ReduceLROnPlateau(monitor = 'loss', factor = 0.5, patience = 15, cooldown = 1, verbose = 1, min_lr = 1e-6)\n",
        "filePath = 'saved-model-{epoch:03d}-{val_loss:.2f}.h5'\n",
        "checkPoint = ModelCheckpoint(filePath, monitor = 'val_loss', verbose = 1, save_best_only = False, \\\n",
        "                             save_weights_only = True, mode = 'auto', save_freq = 20)\n",
        "\n",
        "callbacks_list = [lrScheduler, checkPoint]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClmgwTn9n7Dv",
        "outputId": "90f09308-e47b-40b2-f9a8-50fa95422d7b"
      },
      "source": [
        "history = vae_model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, \\\n",
        "                        verbose = 1, validation_data = (test_x, test_y))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "563/563 [==============================] - 71s 125ms/step - loss: 1764.9927 - relative_error: -0.2973 - val_loss: 517.7518 - val_relative_error: -0.1923\n",
            "Epoch 2/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 309.1549 - relative_error: -0.1460 - val_loss: 254.2977 - val_relative_error: -0.1321\n",
            "Epoch 3/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 197.3738 - relative_error: -0.1182 - val_loss: 175.8051 - val_relative_error: -0.1127\n",
            "Epoch 4/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 147.4164 - relative_error: -0.1026 - val_loss: 136.0072 - val_relative_error: -0.0994\n",
            "Epoch 5/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 118.7958 - relative_error: -0.0923 - val_loss: 117.2762 - val_relative_error: -0.0918\n",
            "Epoch 6/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 133.1320 - relative_error: -0.0962 - val_loss: 115.5355 - val_relative_error: -0.0879\n",
            "Epoch 7/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 101.2775 - relative_error: -0.0851 - val_loss: 92.9674 - val_relative_error: -0.0823\n",
            "Epoch 8/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 86.5444 - relative_error: -0.0789 - val_loss: 82.0123 - val_relative_error: -0.0737\n",
            "Epoch 9/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 79.3368 - relative_error: -0.0756 - val_loss: 72.9383 - val_relative_error: -0.0732\n",
            "Epoch 10/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 72.9241 - relative_error: -0.0724 - val_loss: 77.2213 - val_relative_error: -0.0706\n",
            "Epoch 11/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 67.0579 - relative_error: -0.0692 - val_loss: 57.1839 - val_relative_error: -0.0620\n",
            "Epoch 12/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 61.6558 - relative_error: -0.0658 - val_loss: 73.6813 - val_relative_error: -0.0730\n",
            "Epoch 13/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 59.2939 - relative_error: -0.0645 - val_loss: 49.3093 - val_relative_error: -0.0595\n",
            "Epoch 14/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 47.7503 - relative_error: -0.0577 - val_loss: 50.5514 - val_relative_error: -0.0622\n",
            "Epoch 15/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 40.5051 - relative_error: -0.0534 - val_loss: 58.2902 - val_relative_error: -0.0666\n",
            "Epoch 16/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 36.3388 - relative_error: -0.0506 - val_loss: 73.4279 - val_relative_error: -0.0768\n",
            "Epoch 17/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 35.8250 - relative_error: -0.0505 - val_loss: 32.3213 - val_relative_error: -0.0502\n",
            "Epoch 18/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 32.6579 - relative_error: -0.0476 - val_loss: 33.9856 - val_relative_error: -0.0508\n",
            "Epoch 19/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 26.5137 - relative_error: -0.0431 - val_loss: 23.6759 - val_relative_error: -0.0359\n",
            "Epoch 20/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 25.4575 - relative_error: -0.0418 - val_loss: 23.5226 - val_relative_error: -0.0408\n",
            "Epoch 21/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 20.7419 - relative_error: -0.0378 - val_loss: 20.3129 - val_relative_error: -0.0382\n",
            "Epoch 22/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 18.3305 - relative_error: -0.0356 - val_loss: 22.7362 - val_relative_error: -0.0412\n",
            "Epoch 23/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 16.2039 - relative_error: -0.0333 - val_loss: 18.8267 - val_relative_error: -0.0371\n",
            "Epoch 24/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 14.7275 - relative_error: -0.0319 - val_loss: 18.2648 - val_relative_error: -0.0347\n",
            "Epoch 25/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 14.2323 - relative_error: -0.0311 - val_loss: 19.9550 - val_relative_error: -0.0375\n",
            "Epoch 26/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 13.0475 - relative_error: -0.0299 - val_loss: 10.2536 - val_relative_error: -0.0257\n",
            "Epoch 27/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 12.4865 - relative_error: -0.0294 - val_loss: 17.0774 - val_relative_error: -0.0348\n",
            "Epoch 28/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 11.4245 - relative_error: -0.0279 - val_loss: 11.6494 - val_relative_error: -0.0286\n",
            "Epoch 29/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 10.2977 - relative_error: -0.0264 - val_loss: 684.0181 - val_relative_error: -0.0927\n",
            "Epoch 30/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 13.0288 - relative_error: -0.0293 - val_loss: 12.0742 - val_relative_error: -0.0262\n",
            "Epoch 31/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 9.7687 - relative_error: -0.0258 - val_loss: 9.3733 - val_relative_error: -0.0252\n",
            "Epoch 32/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 9.0769 - relative_error: -0.0248 - val_loss: 16.0482 - val_relative_error: -0.0358\n",
            "Epoch 33/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 8.4192 - relative_error: -0.0240 - val_loss: 10.9756 - val_relative_error: -0.0275\n",
            "Epoch 34/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 8.2172 - relative_error: -0.0236 - val_loss: 19.0676 - val_relative_error: -0.0387\n",
            "Epoch 35/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 8.0786 - relative_error: -0.0235 - val_loss: 7.2101 - val_relative_error: -0.0213\n",
            "Epoch 36/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 7.7505 - relative_error: -0.0229 - val_loss: 12.4456 - val_relative_error: -0.0284\n",
            "Epoch 37/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 7.6496 - relative_error: -0.0227 - val_loss: 7.5255 - val_relative_error: -0.0217\n",
            "Epoch 38/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 6.8434 - relative_error: -0.0216 - val_loss: 31.2494 - val_relative_error: -0.0450\n",
            "Epoch 39/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 6.8055 - relative_error: -0.0212 - val_loss: 7.5365 - val_relative_error: -0.0218\n",
            "Epoch 40/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 6.5567 - relative_error: -0.0209 - val_loss: 9.2227 - val_relative_error: -0.0251\n",
            "Epoch 41/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 6.0668 - relative_error: -0.0203 - val_loss: 5.5535 - val_relative_error: -0.0183\n",
            "Epoch 42/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 7.5813 - relative_error: -0.0223 - val_loss: 10.0899 - val_relative_error: -0.0274\n",
            "Epoch 43/300\n",
            "563/563 [==============================] - 66s 118ms/step - loss: 6.1209 - relative_error: -0.0202 - val_loss: 8.7683 - val_relative_error: -0.0243\n",
            "Epoch 44/300\n",
            "563/563 [==============================] - 66s 118ms/step - loss: 5.6952 - relative_error: -0.0195 - val_loss: 6.2930 - val_relative_error: -0.0202\n",
            "Epoch 45/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 5.5731 - relative_error: -0.0194 - val_loss: 12.7897 - val_relative_error: -0.0305\n",
            "Epoch 46/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 5.7317 - relative_error: -0.0196 - val_loss: 7.0098 - val_relative_error: -0.0214\n",
            "Epoch 47/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 5.3272 - relative_error: -0.0189 - val_loss: 6.5117 - val_relative_error: -0.0217\n",
            "Epoch 48/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 5.1008 - relative_error: -0.0185 - val_loss: 5.1183 - val_relative_error: -0.0179\n",
            "Epoch 49/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.8791 - relative_error: -0.0180 - val_loss: 5.7567 - val_relative_error: -0.0188\n",
            "Epoch 50/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 6.0839 - relative_error: -0.0200 - val_loss: 5.1782 - val_relative_error: -0.0175\n",
            "Epoch 51/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.7395 - relative_error: -0.0179 - val_loss: 6.5998 - val_relative_error: -0.0203\n",
            "Epoch 52/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.7432 - relative_error: -0.0179 - val_loss: 5.8163 - val_relative_error: -0.0192\n",
            "Epoch 53/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.4886 - relative_error: -0.0173 - val_loss: 19.6027 - val_relative_error: -0.0299\n",
            "Epoch 54/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.7424 - relative_error: -0.0178 - val_loss: 6.2713 - val_relative_error: -0.0207\n",
            "Epoch 55/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 4.1923 - relative_error: -0.0168 - val_loss: 6.4570 - val_relative_error: -0.0211\n",
            "Epoch 56/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.1710 - relative_error: -0.0166 - val_loss: 9.1566 - val_relative_error: -0.0218\n",
            "Epoch 57/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.9892 - relative_error: -0.0163 - val_loss: 8.0123 - val_relative_error: -0.0209\n",
            "Epoch 58/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.9734 - relative_error: -0.0161 - val_loss: 6.9158 - val_relative_error: -0.0207\n",
            "Epoch 59/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.8851 - relative_error: -0.0161 - val_loss: 4.1961 - val_relative_error: -0.0162\n",
            "Epoch 60/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.8093 - relative_error: -0.0158 - val_loss: 4.2276 - val_relative_error: -0.0163\n",
            "Epoch 61/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.6043 - relative_error: -0.0155 - val_loss: 5.0385 - val_relative_error: -0.0177\n",
            "Epoch 62/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.8212 - relative_error: -0.0159 - val_loss: 6.2545 - val_relative_error: -0.0199\n",
            "Epoch 63/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.6070 - relative_error: -0.0155 - val_loss: 5.1585 - val_relative_error: -0.0191\n",
            "Epoch 64/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.6347 - relative_error: -0.0155 - val_loss: 6.7487 - val_relative_error: -0.0178\n",
            "Epoch 65/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 5.6025 - relative_error: -0.0191 - val_loss: 8.3405 - val_relative_error: -0.0245\n",
            "Epoch 66/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.6048 - relative_error: -0.0155 - val_loss: 5.3058 - val_relative_error: -0.0193\n",
            "Epoch 67/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.2472 - relative_error: -0.0147 - val_loss: 4.4792 - val_relative_error: -0.0173\n",
            "Epoch 68/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.3027 - relative_error: -0.0149 - val_loss: 4.6459 - val_relative_error: -0.0169\n",
            "Epoch 69/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.6534 - relative_error: -0.0154 - val_loss: 4.3113 - val_relative_error: -0.0161\n",
            "Epoch 70/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 3.1801 - relative_error: -0.0144 - val_loss: 7.8424 - val_relative_error: -0.0182\n",
            "Epoch 71/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 2.8591 - relative_error: -0.0138 - val_loss: 4.1221 - val_relative_error: -0.0167\n",
            "Epoch 72/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.0542 - relative_error: -0.0143 - val_loss: 5.1672 - val_relative_error: -0.0179\n",
            "Epoch 73/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.0421 - relative_error: -0.0142 - val_loss: 4.4543 - val_relative_error: -0.0178\n",
            "Epoch 74/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.9975 - relative_error: -0.0140 - val_loss: 7.4042 - val_relative_error: -0.0225\n",
            "Epoch 75/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.0207 - relative_error: -0.0142 - val_loss: 4.6177 - val_relative_error: -0.0176\n",
            "Epoch 76/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.7490 - relative_error: -0.0135 - val_loss: 7.6721 - val_relative_error: -0.0222\n",
            "Epoch 77/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.5241 - relative_error: -0.0152 - val_loss: 5.2355 - val_relative_error: -0.0189\n",
            "Epoch 78/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 2.7474 - relative_error: -0.0135 - val_loss: 3.7388 - val_relative_error: -0.0152\n",
            "Epoch 79/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.8091 - relative_error: -0.0136 - val_loss: 7.1235 - val_relative_error: -0.0205\n",
            "Epoch 80/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 2.8004 - relative_error: -0.0136 - val_loss: 3.7022 - val_relative_error: -0.0151\n",
            "Epoch 81/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 2.8001 - relative_error: -0.0136 - val_loss: 5.3197 - val_relative_error: -0.0149\n",
            "Epoch 82/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.7691 - relative_error: -0.0135 - val_loss: 4.4478 - val_relative_error: -0.0171\n",
            "Epoch 83/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.6770 - relative_error: -0.0133 - val_loss: 3.7791 - val_relative_error: -0.0156\n",
            "Epoch 84/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.5612 - relative_error: -0.0130 - val_loss: 3.5862 - val_relative_error: -0.0150\n",
            "Epoch 85/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.5869 - relative_error: -0.0130 - val_loss: 5.4714 - val_relative_error: -0.0174\n",
            "Epoch 86/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.1004 - relative_error: -0.0141 - val_loss: 3.1520 - val_relative_error: -0.0132\n",
            "Epoch 87/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.5108 - relative_error: -0.0128 - val_loss: 3.6442 - val_relative_error: -0.0151\n",
            "Epoch 88/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.2078 - relative_error: -0.0121 - val_loss: 5.1851 - val_relative_error: -0.0178\n",
            "Epoch 89/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.3772 - relative_error: -0.0125 - val_loss: 5.5582 - val_relative_error: -0.0201\n",
            "Epoch 90/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.5134 - relative_error: -0.0129 - val_loss: 4.4282 - val_relative_error: -0.0174\n",
            "Epoch 91/300\n",
            "563/563 [==============================] - 67s 120ms/step - loss: 2.2217 - relative_error: -0.0121 - val_loss: 4.2041 - val_relative_error: -0.0163\n",
            "Epoch 92/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.3117 - relative_error: -0.0124 - val_loss: 5.8014 - val_relative_error: -0.0194\n",
            "Epoch 93/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.1513 - relative_error: -0.0120 - val_loss: 3.5112 - val_relative_error: -0.0149\n",
            "Epoch 94/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.5968 - relative_error: -0.0159 - val_loss: 18.8636 - val_relative_error: -0.0378\n",
            "Epoch 95/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.1820 - relative_error: -0.0143 - val_loss: 3.9600 - val_relative_error: -0.0158\n",
            "Epoch 96/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.3843 - relative_error: -0.0125 - val_loss: 2.9913 - val_relative_error: -0.0136\n",
            "Epoch 97/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.1678 - relative_error: -0.0119 - val_loss: 3.4009 - val_relative_error: -0.0146\n",
            "Epoch 98/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.0490 - relative_error: -0.0115 - val_loss: 2.7699 - val_relative_error: -0.0135\n",
            "Epoch 99/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.0616 - relative_error: -0.0117 - val_loss: 3.3701 - val_relative_error: -0.0148\n",
            "Epoch 100/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.9318 - relative_error: -0.0112 - val_loss: 3.0377 - val_relative_error: -0.0131\n",
            "Epoch 101/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.0686 - relative_error: -0.0117 - val_loss: 2.7554 - val_relative_error: -0.0127\n",
            "Epoch 102/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.9731 - relative_error: -0.0114 - val_loss: 4.4045 - val_relative_error: -0.0162\n",
            "Epoch 103/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.9732 - relative_error: -0.0114 - val_loss: 2.7042 - val_relative_error: -0.0129\n",
            "Epoch 104/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.8770 - relative_error: -0.0111 - val_loss: 5.8819 - val_relative_error: -0.0201\n",
            "Epoch 105/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.8532 - relative_error: -0.0111 - val_loss: 2.9029 - val_relative_error: -0.0130\n",
            "Epoch 106/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.1212 - relative_error: -0.0116 - val_loss: 3.3815 - val_relative_error: -0.0140\n",
            "Epoch 107/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.0279 - relative_error: -0.0115 - val_loss: 2.9514 - val_relative_error: -0.0138\n",
            "Epoch 108/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.8759 - relative_error: -0.0112 - val_loss: 2.8508 - val_relative_error: -0.0124\n",
            "Epoch 109/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7848 - relative_error: -0.0108 - val_loss: 2.9474 - val_relative_error: -0.0126\n",
            "Epoch 110/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.8541 - relative_error: -0.0111 - val_loss: 3.8492 - val_relative_error: -0.0158\n",
            "Epoch 111/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7482 - relative_error: -0.0107 - val_loss: 2.9590 - val_relative_error: -0.0138\n",
            "Epoch 112/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.8608 - relative_error: -0.0111 - val_loss: 4.2895 - val_relative_error: -0.0173\n",
            "Epoch 113/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 2.0330 - relative_error: -0.0114 - val_loss: 3.1296 - val_relative_error: -0.0128\n",
            "Epoch 114/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7358 - relative_error: -0.0107 - val_loss: 5.3755 - val_relative_error: -0.0186\n",
            "Epoch 115/300\n",
            "563/563 [==============================] - 67s 120ms/step - loss: 1.8990 - relative_error: -0.0111 - val_loss: 3.6973 - val_relative_error: -0.0159\n",
            "Epoch 116/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7988 - relative_error: -0.0108 - val_loss: 3.6608 - val_relative_error: -0.0145\n",
            "Epoch 117/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7848 - relative_error: -0.0108 - val_loss: 3.6215 - val_relative_error: -0.0146\n",
            "Epoch 118/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7414 - relative_error: -0.0107 - val_loss: 3.6101 - val_relative_error: -0.0155\n",
            "Epoch 119/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.6857 - relative_error: -0.0105 - val_loss: 3.6290 - val_relative_error: -0.0150\n",
            "Epoch 120/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.6625 - relative_error: -0.0104 - val_loss: 3.6413 - val_relative_error: -0.0155\n",
            "Epoch 121/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.9279 - relative_error: -0.0114 - val_loss: 5.5262 - val_relative_error: -0.0187\n",
            "Epoch 122/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7982 - relative_error: -0.0108 - val_loss: 3.0164 - val_relative_error: -0.0131\n",
            "Epoch 123/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.6295 - relative_error: -0.0104 - val_loss: 2.7857 - val_relative_error: -0.0133\n",
            "Epoch 124/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.5839 - relative_error: -0.0101 - val_loss: 3.0495 - val_relative_error: -0.0145\n",
            "Epoch 125/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.6109 - relative_error: -0.0102 - val_loss: 3.1822 - val_relative_error: -0.0138\n",
            "Epoch 126/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.7566 - relative_error: -0.0108 - val_loss: 2.6344 - val_relative_error: -0.0128\n",
            "Epoch 127/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4797 - relative_error: -0.0098 - val_loss: 3.0975 - val_relative_error: -0.0140\n",
            "Epoch 128/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.5001 - relative_error: -0.0098 - val_loss: 2.6144 - val_relative_error: -0.0126\n",
            "Epoch 129/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.6284 - relative_error: -0.0104 - val_loss: 4.6201 - val_relative_error: -0.0158\n",
            "Epoch 130/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.5253 - relative_error: -0.0099 - val_loss: 3.0486 - val_relative_error: -0.0132\n",
            "Epoch 131/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.5012 - relative_error: -0.0099 - val_loss: 3.5189 - val_relative_error: -0.0144\n",
            "Epoch 132/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.6901 - relative_error: -0.0104 - val_loss: 2.7343 - val_relative_error: -0.0130\n",
            "Epoch 133/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.5093 - relative_error: -0.0099 - val_loss: 4.0276 - val_relative_error: -0.0160\n",
            "Epoch 134/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.4224 - relative_error: -0.0096 - val_loss: 3.1418 - val_relative_error: -0.0142\n",
            "Epoch 135/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4817 - relative_error: -0.0098 - val_loss: 2.9448 - val_relative_error: -0.0136\n",
            "Epoch 136/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.5694 - relative_error: -0.0101 - val_loss: 3.0668 - val_relative_error: -0.0132\n",
            "Epoch 137/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4439 - relative_error: -0.0096 - val_loss: 3.2621 - val_relative_error: -0.0140\n",
            "Epoch 138/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.3131 - relative_error: -0.0092 - val_loss: 2.7311 - val_relative_error: -0.0127\n",
            "Epoch 139/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4019 - relative_error: -0.0095 - val_loss: 2.9165 - val_relative_error: -0.0138\n",
            "Epoch 140/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2578 - relative_error: -0.0090 - val_loss: 2.6163 - val_relative_error: -0.0122\n",
            "Epoch 141/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4165 - relative_error: -0.0096 - val_loss: 3.0414 - val_relative_error: -0.0136\n",
            "Epoch 142/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.8309 - relative_error: -0.0105 - val_loss: 3.0035 - val_relative_error: -0.0137\n",
            "Epoch 143/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2929 - relative_error: -0.0091 - val_loss: 2.7689 - val_relative_error: -0.0124\n",
            "Epoch 144/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4805 - relative_error: -0.0099 - val_loss: 2.6229 - val_relative_error: -0.0121\n",
            "Epoch 145/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3491 - relative_error: -0.0094 - val_loss: 3.4365 - val_relative_error: -0.0150\n",
            "Epoch 146/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2886 - relative_error: -0.0091 - val_loss: 3.5010 - val_relative_error: -0.0137\n",
            "Epoch 147/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.4392 - relative_error: -0.0096 - val_loss: 2.3942 - val_relative_error: -0.0118\n",
            "Epoch 148/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3425 - relative_error: -0.0093 - val_loss: 4.5222 - val_relative_error: -0.0170\n",
            "Epoch 149/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3772 - relative_error: -0.0094 - val_loss: 2.4365 - val_relative_error: -0.0129\n",
            "Epoch 150/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.2754 - relative_error: -0.0091 - val_loss: 2.8430 - val_relative_error: -0.0132\n",
            "Epoch 151/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2255 - relative_error: -0.0089 - val_loss: 3.9725 - val_relative_error: -0.0160\n",
            "Epoch 152/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.2650 - relative_error: -0.0090 - val_loss: 5.2822 - val_relative_error: -0.0187\n",
            "Epoch 153/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2123 - relative_error: -0.0089 - val_loss: 2.4351 - val_relative_error: -0.0122\n",
            "Epoch 154/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2475 - relative_error: -0.0090 - val_loss: 3.2897 - val_relative_error: -0.0142\n",
            "Epoch 155/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2289 - relative_error: -0.0090 - val_loss: 2.2575 - val_relative_error: -0.0120\n",
            "Epoch 156/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3204 - relative_error: -0.0091 - val_loss: 3.0149 - val_relative_error: -0.0146\n",
            "Epoch 157/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2267 - relative_error: -0.0089 - val_loss: 2.2430 - val_relative_error: -0.0118\n",
            "Epoch 158/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1724 - relative_error: -0.0087 - val_loss: 2.4821 - val_relative_error: -0.0116\n",
            "Epoch 159/300\n",
            "563/563 [==============================] - 67s 120ms/step - loss: 1.2859 - relative_error: -0.0091 - val_loss: 2.4845 - val_relative_error: -0.0122\n",
            "Epoch 160/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1562 - relative_error: -0.0086 - val_loss: 2.9564 - val_relative_error: -0.0123\n",
            "Epoch 161/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1389 - relative_error: -0.0086 - val_loss: 2.1615 - val_relative_error: -0.0107\n",
            "Epoch 162/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3007 - relative_error: -0.0090 - val_loss: 2.5814 - val_relative_error: -0.0129\n",
            "Epoch 163/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2224 - relative_error: -0.0089 - val_loss: 2.4807 - val_relative_error: -0.0119\n",
            "Epoch 164/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1883 - relative_error: -0.0087 - val_loss: 3.0384 - val_relative_error: -0.0135\n",
            "Epoch 165/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0924 - relative_error: -0.0084 - val_loss: 2.7371 - val_relative_error: -0.0129\n",
            "Epoch 166/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1855 - relative_error: -0.0087 - val_loss: 2.0898 - val_relative_error: -0.0106\n",
            "Epoch 167/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.2994 - relative_error: -0.0091 - val_loss: 2.2565 - val_relative_error: -0.0114\n",
            "Epoch 168/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0464 - relative_error: -0.0082 - val_loss: 3.1290 - val_relative_error: -0.0139\n",
            "Epoch 169/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1056 - relative_error: -0.0085 - val_loss: 2.2319 - val_relative_error: -0.0112\n",
            "Epoch 170/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1524 - relative_error: -0.0086 - val_loss: 2.2825 - val_relative_error: -0.0111\n",
            "Epoch 171/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1125 - relative_error: -0.0085 - val_loss: 2.4909 - val_relative_error: -0.0128\n",
            "Epoch 172/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3547 - relative_error: -0.0087 - val_loss: 23.2400 - val_relative_error: -0.0211\n",
            "Epoch 173/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 4.1336 - relative_error: -0.0153 - val_loss: 3.3376 - val_relative_error: -0.0154\n",
            "Epoch 174/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3865 - relative_error: -0.0094 - val_loss: 2.3138 - val_relative_error: -0.0116\n",
            "Epoch 175/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.2077 - relative_error: -0.0088 - val_loss: 2.4881 - val_relative_error: -0.0117\n",
            "Epoch 176/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0217 - relative_error: -0.0081 - val_loss: 2.5089 - val_relative_error: -0.0123\n",
            "Epoch 177/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0492 - relative_error: -0.0082 - val_loss: 2.8102 - val_relative_error: -0.0135\n",
            "Epoch 178/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9784 - relative_error: -0.0079 - val_loss: 2.5804 - val_relative_error: -0.0125\n",
            "Epoch 179/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1758 - relative_error: -0.0086 - val_loss: 2.4680 - val_relative_error: -0.0121\n",
            "Epoch 180/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9598 - relative_error: -0.0078 - val_loss: 2.3823 - val_relative_error: -0.0122\n",
            "Epoch 181/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0197 - relative_error: -0.0081 - val_loss: 2.4314 - val_relative_error: -0.0122\n",
            "Epoch 182/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3872 - relative_error: -0.0093 - val_loss: 2.9207 - val_relative_error: -0.0148\n",
            "Epoch 183/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8780 - relative_error: -0.0074 - val_loss: 2.7807 - val_relative_error: -0.0130\n",
            "Epoch 184/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1918 - relative_error: -0.0085 - val_loss: 2.5296 - val_relative_error: -0.0121\n",
            "Epoch 185/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8957 - relative_error: -0.0076 - val_loss: 2.5980 - val_relative_error: -0.0132\n",
            "Epoch 186/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0977 - relative_error: -0.0083 - val_loss: 3.8247 - val_relative_error: -0.0173\n",
            "Epoch 187/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0346 - relative_error: -0.0081 - val_loss: 3.1682 - val_relative_error: -0.0131\n",
            "Epoch 188/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0673 - relative_error: -0.0083 - val_loss: 2.2678 - val_relative_error: -0.0113\n",
            "Epoch 189/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0483 - relative_error: -0.0082 - val_loss: 2.5125 - val_relative_error: -0.0124\n",
            "Epoch 190/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9276 - relative_error: -0.0077 - val_loss: 2.3533 - val_relative_error: -0.0116\n",
            "Epoch 191/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0490 - relative_error: -0.0082 - val_loss: 2.3835 - val_relative_error: -0.0122\n",
            "Epoch 192/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0467 - relative_error: -0.0082 - val_loss: 2.3028 - val_relative_error: -0.0125\n",
            "Epoch 193/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0149 - relative_error: -0.0081 - val_loss: 3.0709 - val_relative_error: -0.0150\n",
            "Epoch 194/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9770 - relative_error: -0.0079 - val_loss: 2.2854 - val_relative_error: -0.0108\n",
            "Epoch 195/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0234 - relative_error: -0.0080 - val_loss: 2.3236 - val_relative_error: -0.0123\n",
            "Epoch 196/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8895 - relative_error: -0.0075 - val_loss: 2.5068 - val_relative_error: -0.0120\n",
            "Epoch 197/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0701 - relative_error: -0.0083 - val_loss: 2.6631 - val_relative_error: -0.0124\n",
            "Epoch 198/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8877 - relative_error: -0.0075 - val_loss: 2.6192 - val_relative_error: -0.0120\n",
            "Epoch 199/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.0042 - relative_error: -0.0080 - val_loss: 2.2448 - val_relative_error: -0.0113\n",
            "Epoch 200/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9414 - relative_error: -0.0077 - val_loss: 2.0047 - val_relative_error: -0.0107\n",
            "Epoch 201/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9442 - relative_error: -0.0078 - val_loss: 2.4357 - val_relative_error: -0.0123\n",
            "Epoch 202/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9494 - relative_error: -0.0078 - val_loss: 2.5195 - val_relative_error: -0.0119\n",
            "Epoch 203/300\n",
            "563/563 [==============================] - 68s 120ms/step - loss: 1.0094 - relative_error: -0.0080 - val_loss: 2.3991 - val_relative_error: -0.0112\n",
            "Epoch 204/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9782 - relative_error: -0.0079 - val_loss: 2.4515 - val_relative_error: -0.0123\n",
            "Epoch 205/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8741 - relative_error: -0.0074 - val_loss: 2.5860 - val_relative_error: -0.0120\n",
            "Epoch 206/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9074 - relative_error: -0.0075 - val_loss: 1.9202 - val_relative_error: -0.0110\n",
            "Epoch 207/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 3.0118 - relative_error: -0.0121 - val_loss: 2.3957 - val_relative_error: -0.0119\n",
            "Epoch 208/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.3284 - relative_error: -0.0090 - val_loss: 2.3286 - val_relative_error: -0.0117\n",
            "Epoch 209/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 1.0555 - relative_error: -0.0081 - val_loss: 2.2395 - val_relative_error: -0.0118\n",
            "Epoch 210/300\n",
            "563/563 [==============================] - 66s 118ms/step - loss: 0.9613 - relative_error: -0.0077 - val_loss: 2.1666 - val_relative_error: -0.0105\n",
            "Epoch 211/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8659 - relative_error: -0.0074 - val_loss: 2.4598 - val_relative_error: -0.0120\n",
            "Epoch 212/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8844 - relative_error: -0.0074 - val_loss: 2.5527 - val_relative_error: -0.0112\n",
            "Epoch 213/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8879 - relative_error: -0.0074 - val_loss: 2.0904 - val_relative_error: -0.0117\n",
            "Epoch 214/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8508 - relative_error: -0.0074 - val_loss: 1.9794 - val_relative_error: -0.0107\n",
            "Epoch 215/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8263 - relative_error: -0.0072 - val_loss: 2.8801 - val_relative_error: -0.0125\n",
            "Epoch 216/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9619 - relative_error: -0.0078 - val_loss: 1.9272 - val_relative_error: -0.0106\n",
            "Epoch 217/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8653 - relative_error: -0.0074 - val_loss: 2.3671 - val_relative_error: -0.0121\n",
            "Epoch 218/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8062 - relative_error: -0.0071 - val_loss: 2.5251 - val_relative_error: -0.0128\n",
            "Epoch 219/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9105 - relative_error: -0.0076 - val_loss: 2.6114 - val_relative_error: -0.0123\n",
            "Epoch 220/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8291 - relative_error: -0.0072 - val_loss: 2.5263 - val_relative_error: -0.0122\n",
            "Epoch 221/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9055 - relative_error: -0.0075 - val_loss: 2.2688 - val_relative_error: -0.0109\n",
            "Epoch 222/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8628 - relative_error: -0.0074 - val_loss: 2.3096 - val_relative_error: -0.0123\n",
            "Epoch 223/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8230 - relative_error: -0.0071 - val_loss: 2.1117 - val_relative_error: -0.0113\n",
            "Epoch 224/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7975 - relative_error: -0.0071 - val_loss: 1.8549 - val_relative_error: -0.0103\n",
            "Epoch 225/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9212 - relative_error: -0.0076 - val_loss: 2.6262 - val_relative_error: -0.0133\n",
            "Epoch 226/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7921 - relative_error: -0.0070 - val_loss: 2.2328 - val_relative_error: -0.0101\n",
            "Epoch 227/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9309 - relative_error: -0.0077 - val_loss: 2.1741 - val_relative_error: -0.0110\n",
            "Epoch 228/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8635 - relative_error: -0.0073 - val_loss: 2.3808 - val_relative_error: -0.0116\n",
            "Epoch 229/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7645 - relative_error: -0.0070 - val_loss: 1.9825 - val_relative_error: -0.0111\n",
            "Epoch 230/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9759 - relative_error: -0.0078 - val_loss: 2.6896 - val_relative_error: -0.0117\n",
            "Epoch 231/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8363 - relative_error: -0.0072 - val_loss: 3.4271 - val_relative_error: -0.0157\n",
            "Epoch 232/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.9115 - relative_error: -0.0076 - val_loss: 2.7008 - val_relative_error: -0.0131\n",
            "Epoch 233/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9060 - relative_error: -0.0075 - val_loss: 2.4620 - val_relative_error: -0.0122\n",
            "Epoch 234/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7442 - relative_error: -0.0068 - val_loss: 2.4704 - val_relative_error: -0.0123\n",
            "Epoch 235/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7662 - relative_error: -0.0070 - val_loss: 2.3844 - val_relative_error: -0.0117\n",
            "Epoch 236/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7744 - relative_error: -0.0069 - val_loss: 2.3098 - val_relative_error: -0.0112\n",
            "Epoch 237/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9014 - relative_error: -0.0075 - val_loss: 3.5507 - val_relative_error: -0.0126\n",
            "Epoch 238/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8323 - relative_error: -0.0072 - val_loss: 1.9027 - val_relative_error: -0.0107\n",
            "Epoch 239/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8244 - relative_error: -0.0072 - val_loss: 2.0807 - val_relative_error: -0.0106\n",
            "Epoch 240/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7108 - relative_error: -0.0067 - val_loss: 2.5456 - val_relative_error: -0.0121\n",
            "Epoch 241/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8703 - relative_error: -0.0074 - val_loss: 2.2880 - val_relative_error: -0.0119\n",
            "Epoch 242/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8128 - relative_error: -0.0071 - val_loss: 2.1750 - val_relative_error: -0.0108\n",
            "Epoch 243/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7692 - relative_error: -0.0070 - val_loss: 2.3985 - val_relative_error: -0.0122\n",
            "Epoch 244/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8718 - relative_error: -0.0075 - val_loss: 2.3239 - val_relative_error: -0.0125\n",
            "Epoch 245/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7721 - relative_error: -0.0070 - val_loss: 2.3337 - val_relative_error: -0.0117\n",
            "Epoch 246/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6805 - relative_error: -0.0066 - val_loss: 2.1646 - val_relative_error: -0.0120\n",
            "Epoch 247/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8136 - relative_error: -0.0072 - val_loss: 2.5984 - val_relative_error: -0.0130\n",
            "Epoch 248/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.8195 - relative_error: -0.0072 - val_loss: 2.9210 - val_relative_error: -0.0133\n",
            "Epoch 249/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8193 - relative_error: -0.0071 - val_loss: 2.2482 - val_relative_error: -0.0115\n",
            "Epoch 250/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7982 - relative_error: -0.0071 - val_loss: 2.2056 - val_relative_error: -0.0123\n",
            "Epoch 251/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7489 - relative_error: -0.0068 - val_loss: 2.0859 - val_relative_error: -0.0113\n",
            "Epoch 252/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.7226 - relative_error: -0.0067 - val_loss: 2.6198 - val_relative_error: -0.0138\n",
            "Epoch 253/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8152 - relative_error: -0.0071 - val_loss: 3.5043 - val_relative_error: -0.0149\n",
            "Epoch 254/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7590 - relative_error: -0.0069 - val_loss: 2.0708 - val_relative_error: -0.0110\n",
            "Epoch 255/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6570 - relative_error: -0.0064 - val_loss: 3.7079 - val_relative_error: -0.0171\n",
            "Epoch 256/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7946 - relative_error: -0.0071 - val_loss: 1.8856 - val_relative_error: -0.0102\n",
            "Epoch 257/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7209 - relative_error: -0.0066 - val_loss: 2.2253 - val_relative_error: -0.0112\n",
            "Epoch 258/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8338 - relative_error: -0.0072 - val_loss: 2.0248 - val_relative_error: -0.0104\n",
            "Epoch 259/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 1.1878 - relative_error: -0.0084 - val_loss: 2.2534 - val_relative_error: -0.0119\n",
            "Epoch 260/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7859 - relative_error: -0.0069 - val_loss: 2.5314 - val_relative_error: -0.0118\n",
            "Epoch 261/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7201 - relative_error: -0.0067 - val_loss: 2.1994 - val_relative_error: -0.0117\n",
            "Epoch 262/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6586 - relative_error: -0.0064 - val_loss: 1.8521 - val_relative_error: -0.0097\n",
            "Epoch 263/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6547 - relative_error: -0.0064 - val_loss: 2.3261 - val_relative_error: -0.0120\n",
            "Epoch 264/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7150 - relative_error: -0.0065 - val_loss: 8.2414 - val_relative_error: -0.0253\n",
            "Epoch 265/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.9538 - relative_error: -0.0076 - val_loss: 2.2499 - val_relative_error: -0.0119\n",
            "Epoch 266/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6241 - relative_error: -0.0062 - val_loss: 1.8953 - val_relative_error: -0.0107\n",
            "Epoch 267/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7745 - relative_error: -0.0069 - val_loss: 2.2128 - val_relative_error: -0.0118\n",
            "Epoch 268/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6386 - relative_error: -0.0063 - val_loss: 2.5275 - val_relative_error: -0.0124\n",
            "Epoch 269/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8270 - relative_error: -0.0071 - val_loss: 5.1311 - val_relative_error: -0.0189\n",
            "Epoch 270/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6814 - relative_error: -0.0065 - val_loss: 2.4668 - val_relative_error: -0.0131\n",
            "Epoch 271/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7447 - relative_error: -0.0068 - val_loss: 2.3857 - val_relative_error: -0.0127\n",
            "Epoch 272/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6766 - relative_error: -0.0065 - val_loss: 2.0268 - val_relative_error: -0.0109\n",
            "Epoch 273/300\n",
            "563/563 [==============================] - 67s 118ms/step - loss: 0.6948 - relative_error: -0.0066 - val_loss: 2.0700 - val_relative_error: -0.0116\n",
            "Epoch 274/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7386 - relative_error: -0.0068 - val_loss: 2.0310 - val_relative_error: -0.0109\n",
            "Epoch 275/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6270 - relative_error: -0.0062 - val_loss: 3.4833 - val_relative_error: -0.0146\n",
            "Epoch 276/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.8592 - relative_error: -0.0073 - val_loss: 1.7855 - val_relative_error: -0.0101\n",
            "Epoch 277/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6082 - relative_error: -0.0062 - val_loss: 1.8503 - val_relative_error: -0.0106\n",
            "Epoch 278/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6524 - relative_error: -0.0063 - val_loss: 1.8176 - val_relative_error: -0.0103\n",
            "Epoch 279/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6672 - relative_error: -0.0064 - val_loss: 2.2438 - val_relative_error: -0.0118\n",
            "Epoch 280/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6319 - relative_error: -0.0063 - val_loss: 2.3284 - val_relative_error: -0.0119\n",
            "Epoch 281/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6601 - relative_error: -0.0064 - val_loss: 1.8757 - val_relative_error: -0.0105\n",
            "Epoch 282/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6693 - relative_error: -0.0065 - val_loss: 2.1979 - val_relative_error: -0.0116\n",
            "Epoch 283/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7773 - relative_error: -0.0069 - val_loss: 2.0488 - val_relative_error: -0.0112\n",
            "Epoch 284/300\n",
            "563/563 [==============================] - 67s 120ms/step - loss: 0.5824 - relative_error: -0.0060 - val_loss: 1.7587 - val_relative_error: -0.0100\n",
            "Epoch 285/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.5919 - relative_error: -0.0060 - val_loss: 2.0430 - val_relative_error: -0.0108\n",
            "Epoch 286/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.7022 - relative_error: -0.0066 - val_loss: 2.3862 - val_relative_error: -0.0122\n",
            "Epoch 287/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6516 - relative_error: -0.0063 - val_loss: 2.1320 - val_relative_error: -0.0112\n",
            "Epoch 288/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.5779 - relative_error: -0.0059 - val_loss: 1.9600 - val_relative_error: -0.0107\n",
            "Epoch 289/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6744 - relative_error: -0.0065 - val_loss: 2.8308 - val_relative_error: -0.0126\n",
            "Epoch 290/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6551 - relative_error: -0.0063 - val_loss: 2.3498 - val_relative_error: -0.0127\n",
            "Epoch 291/300\n",
            "563/563 [==============================] - 68s 120ms/step - loss: 0.7691 - relative_error: -0.0069 - val_loss: 2.2657 - val_relative_error: -0.0115\n",
            "Epoch 292/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6630 - relative_error: -0.0064 - val_loss: 2.3333 - val_relative_error: -0.0118\n",
            "Epoch 293/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6999 - relative_error: -0.0065 - val_loss: 2.3289 - val_relative_error: -0.0113\n",
            "Epoch 294/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6454 - relative_error: -0.0064 - val_loss: 1.6342 - val_relative_error: -0.0097\n",
            "Epoch 295/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6564 - relative_error: -0.0064 - val_loss: 2.2187 - val_relative_error: -0.0112\n",
            "Epoch 296/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6643 - relative_error: -0.0064 - val_loss: 2.2703 - val_relative_error: -0.0119\n",
            "Epoch 297/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6445 - relative_error: -0.0063 - val_loss: 3.1695 - val_relative_error: -0.0143\n",
            "Epoch 298/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6119 - relative_error: -0.0061 - val_loss: 2.4518 - val_relative_error: -0.0121\n",
            "Epoch 299/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6164 - relative_error: -0.0061 - val_loss: 1.8151 - val_relative_error: -0.0107\n",
            "Epoch 300/300\n",
            "563/563 [==============================] - 67s 119ms/step - loss: 0.6897 - relative_error: -0.0063 - val_loss: 1.9197 - val_relative_error: -0.0101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5j_VWLCPY0H",
        "outputId": "4a452982-7016-40c6-8458-7917154f2179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(np.abs(history.history['relative_error']))\n",
        "plt.plot(np.abs(history.history['val_relative_error']))\n",
        "plt.title('model relative error')\n",
        "plt.ylabel('relative error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/MyDrive/Gridsearch_images/pressure_lr1-4_300epochs_adam_batch4.png')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'relative_error', 'val_loss', 'val_relative_error'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDs1BchKTfh7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPXacBAry0S",
        "outputId": "3bb6186a-7ab0-4916-f6d0-709db325de5d"
      },
      "source": [
        "#vae_model.save(\"/content/gdrive/MyDrive/saved_models/pressure_le1-2.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}