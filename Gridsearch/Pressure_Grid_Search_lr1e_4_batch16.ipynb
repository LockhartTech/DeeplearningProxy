{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pressure_Grid_Search_lr1e-4_batch16.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyONpxfBWoR5drLmfK61ixSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-srm3018/DeeplearningProxy/blob/main/Gridsearch/Pressure_Grid_Search_lr1e_4_batch16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9PQE47vGAl_",
        "outputId": "5958ed38-0c0b-42cd-d17c-0ea6f619feb8"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 25 15:58:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi27NRCrGRba",
        "outputId": "532130ff-096f-4d18-d5c2-5835d8693bb4"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "9D0wgpehaCVX",
        "outputId": "080dd82c-23cd-48d8-8f17-ca5be0431dee"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a84a778-13b5-48be-92bd-ef2b3b4a6dda\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a84a778-13b5-48be-92bd-ef2b3b4a6dda\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving layers.py to layers (2).py\n",
            "Saving unet_uae.py to unet_uae (2).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers.py': b'\"\"\"Import required libraries and modules.\"\"\"\\r\\n\\r\\nimport tensorflow as tf\\r\\nfrom keras import backend as K\\r\\nfrom keras.engine.topology import Layer\\r\\nfrom keras.layers.merge import add\\r\\n# from keras.engine import InputSpec\\r\\nfrom keras.layers import InputSpec\\r\\nfrom keras.layers.core import Activation\\r\\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\\r\\nfrom keras.layers import BatchNormalization, ConvLSTM2D\\r\\nfrom keras.layers import TimeDistributed, Reshape, RepeatVector\\r\\nfrom keras import regularizers\\r\\n\\r\\n\\r\\nreg_weights = 0.00001\\r\\n\\r\\n\\r\\ndef conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride,\\r\\n                   padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(\"relu\")(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef time_conv_bn_relu(filter_num, row_num, col_num, stride):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def conv_func(x):\\r\\n        x = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                            strides=stride,\\r\\n                            padding=\\'same\\',\\r\\n                            kernel_regularizer=regularizers.\\r\\n                            l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(\"relu\"))(x)\\r\\n        return x\\r\\n\\r\\n    return conv_func\\r\\n\\r\\n\\r\\ndef res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    *default = (1,1)\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        a = BatchNormalization()(a)\\r\\n        a = Activation(\"relu\")(a)\\r\\n        a = Conv2D(filter_num, (row_num, col_num),\\r\\n                   strides=stride, padding=\\'same\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(a)\\r\\n        y = BatchNormalization()(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef time_res_conv(filter_num, row_num, col_num, stride=(1, 1)):\\r\\n    \"\"\"\\r\\n    Create Convolutional Batch Norm layer.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    conv_func\\r\\n    \"\"\"\\r\\n    def _res_func(x):\\r\\n        identity = x\\r\\n\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        a = TimeDistributed(BatchNormalization())(a)\\r\\n        a = TimeDistributed(Activation(\"relu\"))(a)\\r\\n        a = TimeDistributed(Conv2D(filter_num, (row_num, col_num),\\r\\n                                   strides=stride, padding=\\'same\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        y = TimeDistributed(BatchNormalization())(a)\\r\\n\\r\\n        return add([identity, y])\\r\\n\\r\\n    return _res_func\\r\\n\\r\\n\\r\\ndef dconv_bn_nolinear(nb_filter, nb_row, nb_col, stride=(2, 2),\\r\\n                      activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = UnPooling2D(size=stride)(x)\\r\\n        x = ReflectionPadding2D(padding=(int(nb_row/2), int(nb_col/2)))(x)\\r\\n        x = Conv2D(nb_filter, (nb_row, nb_col), padding=\\'valid\\',\\r\\n                   kernel_regularizer=regularizers.l2(reg_weights))(x)\\r\\n        x = BatchNormalization()(x)\\r\\n        x = Activation(activation)(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\ndef time_dconv_bn_nolinear(nb_filter, nb_row, nb_col,\\r\\n                           stride=(2, 2), activation=\"relu\"):\\r\\n    \"\"\"\\r\\n    Create time convolutional Batch Norm layer in decoders.\\r\\n\\r\\n    Parameters:\\r\\n    ---------\\r\\n    filter_num : int\\r\\n    number of filters to use in convolution layer.\\r\\n    row_num : int\\r\\n    number of row\\r\\n    col_num : int\\r\\n    number of column\\r\\n    stride : int\\r\\n    size of stride\\r\\n    Returns:\\r\\n    ---------\\r\\n    dconv_bn\\r\\n    \"\"\"\\r\\n    def _dconv_bn(x):\\r\\n        x = TimeDistributed(UnPooling2D(size=stride))(x)\\r\\n        x = TimeDistributed(ReflectionPadding2D(padding=(int(nb_row/2),\\r\\n                            int(nb_col/2))))(x)\\r\\n        x = TimeDistributed(Conv2D(nb_filter, (nb_row, nb_col),\\r\\n                                   padding=\\'valid\\',\\r\\n                                   kernel_regularizer=regularizers.\\r\\n                                   l2(reg_weights)))(x)\\r\\n        x = TimeDistributed(BatchNormalization())(x)\\r\\n        x = TimeDistributed(Activation(activation))(x)\\r\\n        return x\\r\\n\\r\\n    return _dconv_bn\\r\\n\\r\\n\\r\\nclass ReflectionPadding2D(Layer):\\r\\n    \"\"\"class for reflectionPadding2D.\"\"\"\\r\\n\\r\\n    def __init__(self, padding=(1, 1), data_format=\"channels_last\", **kwargs):\\r\\n        \"\"\"\\r\\n        Construct class parameters.\\r\\n\\r\\n        parameters:\\r\\n        -------\\r\\n        padding\\r\\n        dim_ordering\\r\\n        \"\"\"\\r\\n        super(ReflectionPadding2D, self).__init__(**kwargs)\\r\\n\\r\\n        if data_format == \\'channels_last\\':\\r\\n            dim_ordering = K.image_data_format()\\r\\n\\r\\n        self.padding = padding\\r\\n        if isinstance(padding, dict):\\r\\n            if set(padding.keys()) <= {\\'top_pad\\', \\'bottom_pad\\',\\r\\n                                       \\'left_pad\\', \\'right_pad\\'}:\\r\\n                self.top_pad = padding.get(\\'top_pad\\', 0)\\r\\n                self.bottom_pad = padding.get(\\'bottom_pad\\', 0)\\r\\n                self.left_pad = padding.get(\\'left_pad\\', 0)\\r\\n                self.right_pad = padding.get(\\'right_pad\\', 0)\\r\\n            else:\\r\\n                raise ValueError(\\'Unexpected key\\'\\r\\n                                 \\'found in `padding` dictionary.\\'\\r\\n                                 \\'Keys have to be in {\"top_pad\", \"bottom_pad\",\\'\\r\\n                                 \\'\"left_pad\", \"right_pad\"}.\\'\\r\\n                                 \\'Found: \\' + str(padding.keys()))\\r\\n        else:\\r\\n            padding = tuple(padding)\\r\\n            if len(padding) == 2:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[0]\\r\\n                self.left_pad = padding[1]\\r\\n                self.right_pad = padding[1]\\r\\n            elif len(padding) == 4:\\r\\n                self.top_pad = padding[0]\\r\\n                self.bottom_pad = padding[1]\\r\\n                self.left_pad = padding[2]\\r\\n                self.right_pad = padding[3]\\r\\n            else:\\r\\n                raise TypeError(\\'`padding` should be tuple of int \\'\\r\\n                                \\'of length 2 or 4, or dict. \\'\\r\\n                                \\'Found: \\' + str(padding))\\r\\n\\r\\n        # if data_format not in {\\'channels_last\\'}:\\r\\n        #     raise ValueError(\\'data_format must be in {\"channels_last\"}.\\')\\r\\n        self.data_format = data_format\\r\\n        self.input_spec = [InputSpec(ndim=4)]\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call x to apply padding.\"\"\"\\r\\n        top_pad = self.top_pad\\r\\n        bottom_pad = self.bottom_pad\\r\\n        left_pad = self.left_pad\\r\\n        right_pad = self.right_pad\\r\\n\\r\\n        paddings = [[0, 0], [left_pad, right_pad],\\r\\n                    [top_pad, bottom_pad], [0, 0]]\\r\\n\\r\\n        return tf.pad(x, paddings, mode=\\'REFLECT\\', name=None)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"\\r\\n        Compute the shape of output.\\r\\n\\r\\n        Parameters:\\r\\n        --------\\r\\n        input_shape: Tuple\\r\\n        shape of input\\r\\n        \"\"\"\\r\\n        if self.data_format == \\'channels_last\\':\\r\\n            rows = input_shape[1] + self.top_pad + self.bottom_pad\\r\\n            cols = input_shape[2] + self.left_pad + self.right_pad\\r\\n\\r\\n            return (input_shape[0],\\r\\n                    rows,\\r\\n                    cols,\\r\\n                    input_shape[3])\\r\\n        else:\\r\\n            raise ValueError(\\'Invalid data_format:\\', self.data_format)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get the Configure.\"\"\"\\r\\n        config = {\\'padding\\': self.padding}\\r\\n        base_config = super(ReflectionPadding2D, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n\\r\\n\\r\\nclass UnPooling2D(UpSampling2D):\\r\\n    \"\"\"Unpool 2D from 2D upsampling.\"\"\"\\r\\n\\r\\n    def __init__(self, size=(2, 2)):\\r\\n        \"\"\"Construct size.\"\"\"\\r\\n        super(UnPooling2D, self).__init__(size)\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call th x data.\"\"\"\\r\\n        shapes = x.get_shape().as_list()\\r\\n        w = self.size[0] * shapes[1]\\r\\n        h = self.size[1] * shapes[2]\\r\\n        return tf.image.resize(x, (w, h))\\r\\n\\r\\n\\r\\nclass InstanceNormalize(Layer):\\r\\n    \"\"\"Normalization Instance of class.\"\"\"\\r\\n\\r\\n    def __init__(self, **kwargs):\\r\\n        \"\"\"Initialize the keyaarguments.\"\"\"\\r\\n        super(InstanceNormalize, self).__init__(**kwargs)\\r\\n        self.epsilon = 1e-3\\r\\n\\r\\n    def call(self, x, mask=None):\\r\\n        \"\"\"Call mean and variance for normalization.\"\"\"\\r\\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\\r\\n        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute the shape of output.\"\"\"\\r\\n        return input_shape\\r\\n\\r\\n\\r\\nclass RepeatConv(Layer):\\r\\n    \"\"\"\\r\\n    Repeats the input n times.\\r\\n\\r\\n    Example:\\r\\n    -------\\r\\n        model = Sequential()\\r\\n        model.add(Dense(32, input_dim=32))\\r\\n        now: model.output_shape == (None, 32)\\r\\n        note: `None` is the batch dimension\\r\\n        model.add(RepeatVector(3))\\r\\n        now: model.output_shape == (None, 3, 32)\\r\\n\\r\\n    Arguments\\r\\n    ---------\\r\\n        n: integer, repetition factor.\\r\\n    Input shape\\r\\n    ----------\\r\\n        4D tensor of shape `(num_samples, w, h, c)`.\\r\\n    Output shape\\r\\n    -----------\\r\\n        5D tensor of shape `(num_samples, n, w, h, c)`.\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, n, **kwargs):\\r\\n        \"\"\"Initialize the class parameters.\"\"\"\\r\\n        super(RepeatConv, self).__init__(**kwargs)\\r\\n        self.n = n\\r\\n        self.input_spec = InputSpec(ndim=4)\\r\\n\\r\\n    def compute_output_shape(self, input_shape):\\r\\n        \"\"\"Compute output shape.\"\"\"\\r\\n        return (input_shape[0], self.n, input_shape[1],\\r\\n                input_shape[2], input_shape[3])\\r\\n\\r\\n    def call(self, inputs):\\r\\n        \"\"\"Call the inputs.\"\"\"\\r\\n        x = K.expand_dims(inputs, 1)\\r\\n        pattern = tf.stack([1, self.n, 1, 1, 1])\\r\\n        return K.tile(x, pattern)\\r\\n\\r\\n    def get_config(self):\\r\\n        \"\"\"Get configure.\"\"\"\\r\\n        config = {\\'n\\': self.n}\\r\\n        base_config = super(RepeatConv, self).get_config()\\r\\n        return dict(list(base_config.items()) + list(config.items()))\\r\\n',\n",
              " 'unet_uae.py': b'\"\"\"Path hack to make tests work.\"\"\"\\r\\nfrom layers import *\\r\\nfrom keras import backend as K\\r\\nfrom keras.layers import Input, Flatten, Dense, Lambda, Reshape\\r\\nfrom keras.layers import concatenate, TimeDistributed, RepeatVector, ConvLSTM2D\\r\\nfrom keras.models import Model\\r\\nimport numpy as np\\r\\n\\r\\n\\r\\ndef create_vae(input_shape, depth):\\r\\n    \"\"\"\\r\\n    Create VAE to create something new.\\r\\n\\r\\n    Parameters:\\r\\n    -------\\r\\n    input_shape : Tuple\\r\\n    depth : int\\r\\n    Returns:\\r\\n    -------\\r\\n    encoder: encoder\\r\\n    model : recurrnet R-UNET model\\r\\n\\r\\n    \"\"\"\\r\\n    # Encoder\\r\\n    input = Input(shape=input_shape, name=\\'image\\')\\r\\n\\r\\n    enc1 = conv_bn_relu(16, 3, 3, stride=(2, 2))(input)\\r\\n    time_enc1 = RepeatConv(depth)(enc1)\\r\\n    enc2 = conv_bn_relu(32, 3, 3, stride=(1, 1))(enc1)\\r\\n    time_enc2 = RepeatConv(depth)(enc2)\\r\\n    enc3 = conv_bn_relu(64, 3, 3, stride=(2, 2))(enc2)\\r\\n    time_enc3 = RepeatConv(depth)(enc3)\\r\\n    enc4 = conv_bn_relu(128, 3, 3, stride=(1, 1))(enc3)\\r\\n    time_enc4 = RepeatConv(depth)(enc4)\\r\\n\\r\\n    x = res_conv(128, 3, 3)(enc4)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n    x = res_conv(128, 3, 3)(x)\\r\\n\\r\\n    encoder = Model(input, x, name=\\'encoder\\')\\r\\n\\r\\n    x = RepeatConv(depth)(enc4)\\r\\n    x = ConvLSTM2D(128, (3, 3), strides=(1, 1),\\r\\n                   padding=\\'same\\', activation=\\'relu\\',\\r\\n                   return_sequences=True)(x)\\r\\n    # x = ConvLSTM2D(64, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    # x = ConvLSTM2D(128, (3, 3), strides=(1, 1), padding = \\'same\\',\\r\\n    # activation=\\'relu\\', return_sequences = True)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    x = time_res_conv(128, 3, 3)(x)\\r\\n    dec4 = time_res_conv(128, 3, 3)(x)\\r\\n\\r\\n    merge4 = concatenate([time_enc4, dec4], axis=4)\\r\\n    dec3 = time_dconv_bn_nolinear(128, 3, 3, stride=(1, 1))(merge4)\\r\\n    merge3 = concatenate([time_enc3, dec3], axis=4)\\r\\n    dec2 = time_dconv_bn_nolinear(64, 3, 3, stride=(2, 2))(merge3)\\r\\n    merge2 = concatenate([time_enc2, dec2], axis=4)\\r\\n    dec1 = time_dconv_bn_nolinear(32, 3, 3, stride=(1, 1))(merge2)\\r\\n    merge1 = concatenate([time_enc1, dec1], axis=4)\\r\\n    dec0 = time_dconv_bn_nolinear(16, 3, 3, stride=(2, 2))(merge1)\\r\\n\\r\\n    output = TimeDistributed(Conv2D(1, (3, 3), padding=\\'same\\',\\r\\n                                    activation=None))(dec0)\\r\\n    print(\\'output shape is \\', K.int_shape(output))\\r\\n    # Full net\\r\\n    full_model = Model(input, output)\\r\\n\\r\\n    return full_model, encoder\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrWDL1qVfMw"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import unet_uae as vae_util\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf \n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zr0u9vQzf5R",
        "outputId": "a488064e-c86a-491b-8421-0221fc7f8785"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlbNGMtBA_Fs"
      },
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Function to load datasets in format .NPY\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    path : string\n",
        "        The absolute path of where data saved in local system\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    loaded_data : ndarray\n",
        "        The data which was loaded\n",
        "    \"\"\"\n",
        "    loaded_data = np.load(path)\n",
        "    return loaded_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cErfklo76F21"
      },
      "source": [
        "# Two common methods for feature scaling is : 1-Normalization & 2-Standardaisation\n",
        "\n",
        "def normalize(data):\n",
        "    \"\"\"\n",
        "    this function used for Max-Min Normalization (Min-Max scaling) by re-scaling\n",
        "    features with a distribution value between 0 and 1. For every feature,the minimum\n",
        "    value of that feature gets transformed into 0, and the maximum value \n",
        "    gets transformed into 1\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    norm_data : ndarray\n",
        "        The normalized data which transformed into 0 and 1\n",
        "    \"\"\"\n",
        "    max_p = np.max(data[:, :, :, :])\n",
        "    min_p = np.min(data[:, :, :, :])\n",
        "    norm_data = (data - min_p)/(max_p - min_p)\n",
        "    return norm_data\n",
        "\n",
        "def standardize(data):\n",
        "    \"\"\"\n",
        "    this function used for rescaling faetures to ensure the mean\n",
        "    and the standard deviation to be 0 and 1, respectively.\n",
        "    \n",
        "    Parameter:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The numpy array which we want to normalize\n",
        "        \n",
        "    Return:\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        The standardized data which the mean\n",
        "    and the standard deviation to be 0 and 1\n",
        "    \"\"\"\n",
        "    data_mean = np.mean(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    data_std = np.std(data[:, :, :, :], axis = 0, keepdims = True)\n",
        "    std_data = (data - data_mean)/(data_std)\n",
        "    return std_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uviUO--fC_pp"
      },
      "source": [
        "# define the absolute path of training datatsat\n",
        "path_perm = '/content/gdrive/MyDrive/perm.npy'\n",
        "path_press = '/content/gdrive/MyDrive/pressure.npy'\n",
        "# use load_data function nd above path to loading data\n",
        "X_data= load_data(path_perm)\n",
        "target_data = load_data(path_press)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5s3_Zex6sQs"
      },
      "source": [
        "# Normalize data using abov normalize function\n",
        "train_nr = 2250\n",
        "test_nr = 750"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yq6Ouns_EIN",
        "outputId": "c91155a9-0587-48c2-810c-26b016ea5a3d"
      },
      "source": [
        "p_t_mean = np.mean(target_data[:train_nr, ...], axis = 0, keepdims = True)\n",
        "target_data = target_data - p_t_mean\n",
        "print('max p is ', np.max(target_data[:train_nr, ...]), ', min p is ', np.min(target_data[:train_nr, ...]))\n",
        "max_p = np.max(target_data[:train_nr, ...])\n",
        "min_p = np.min(target_data[:train_nr, ...])\n",
        "target_data = (target_data - min_p)/(max_p -min_p) - 0.5\n",
        "print('max p is ', np.max(target_data), ', min p is ', np.min(target_data))\n",
        "print('max p train is ', np.max(target_data[:train_nr, ...]), ', min p train is ', np.min(target_data[:train_nr, ...]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max p is  516.6753418240017 , min p is  -135.6267126736111\n",
            "max p is  0.5039995270719013 , min p is  -0.500166926837037\n",
            "max p train is  0.5 , min p train is  -0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q5C0284nFxJ",
        "outputId": "7b4f35a3-a6c4-43c6-dc8f-be0104e47673"
      },
      "source": [
        "input_shape=(100, 100, 2)\n",
        "depth = 10\n",
        "vae_model,_ = vae_util.create_vae(input_shape, depth)\n",
        "vae_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output shape is  (None, 10, 100, 100, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 100, 100, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 50, 50, 16)   304         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 50, 50, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 50, 50, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 50, 50, 32)   4640        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 50, 50, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 50, 50, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 25, 25, 64)   18496       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 25, 25, 128)  73856       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 25, 25, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 25, 25, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_4 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)       (None, 10, 25, 25, 1 1180160     repeat_conv_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 10, 25, 25, 1 147584      conv_lst_m2d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 10, 25, 25, 1 0           conv_lst_m2d[0][0]               \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 10, 25, 25, 1 147584      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 10, 25, 25, 1 512         time_distributed_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 10, 25, 25, 1 0           add_3[0][0]                      \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_13 (TimeDistri (None, 10, 25, 25, 1 147584      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_14 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_3 (RepeatConv)      (None, 10, 25, 25, 1 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 10, 25, 25, 1 0           add_4[0][0]                      \n",
            "                                                                 time_distributed_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 25, 25, 2 0           repeat_conv_3[0][0]              \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_15 (TimeDistri (None, 10, 25, 25, 2 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 10, 27, 27, 2 0           time_distributed_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 10, 25, 25, 1 295040      time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 10, 25, 25, 1 512         time_distributed_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_2 (RepeatConv)      (None, 10, 25, 25, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_19 (TimeDistri (None, 10, 25, 25, 1 0           time_distributed_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 25, 25, 1 0           repeat_conv_2[0][0]              \n",
            "                                                                 time_distributed_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_20 (TimeDistri (None, 10, 50, 50, 1 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_21 (TimeDistri (None, 10, 52, 52, 1 0           time_distributed_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_22 (TimeDistri (None, 10, 50, 50, 6 110656      time_distributed_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_23 (TimeDistri (None, 10, 50, 50, 6 256         time_distributed_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv_1 (RepeatConv)      (None, 10, 50, 50, 3 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_24 (TimeDistri (None, 10, 50, 50, 6 0           time_distributed_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 50, 50, 9 0           repeat_conv_1[0][0]              \n",
            "                                                                 time_distributed_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_25 (TimeDistri (None, 10, 50, 50, 9 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_26 (TimeDistri (None, 10, 52, 52, 9 0           time_distributed_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_27 (TimeDistri (None, 10, 50, 50, 3 27680       time_distributed_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_28 (TimeDistri (None, 10, 50, 50, 3 128         time_distributed_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "repeat_conv (RepeatConv)        (None, 10, 50, 50, 1 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_29 (TimeDistri (None, 10, 50, 50, 3 0           time_distributed_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 50, 50, 4 0           repeat_conv[0][0]                \n",
            "                                                                 time_distributed_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_30 (TimeDistri (None, 10, 100, 100, 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_31 (TimeDistri (None, 10, 102, 102, 0           time_distributed_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_32 (TimeDistri (None, 10, 100, 100, 6928        time_distributed_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_33 (TimeDistri (None, 10, 100, 100, 64          time_distributed_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_34 (TimeDistri (None, 10, 100, 100, 0           time_distributed_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_35 (TimeDistri (None, 10, 100, 100, 145         time_distributed_34[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 2,164,113\n",
            "Trainable params: 2,162,385\n",
            "Non-trainable params: 1,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4PDSq-qnONZ",
        "outputId": "f966b0f9-17f9-481b-a76d-ce53879fd003"
      },
      "source": [
        "depth = 10\n",
        "nr = X_data.shape[0]\n",
        "train_nr = 2250\n",
        "test_nr = 750\n",
        "train_x = np.concatenate([X_data[:train_nr,[0], ...],target_data[:train_nr,[0], ...]], axis = 1)\n",
        "train_y = target_data[:train_nr, ...]\n",
        "\n",
        "test_x = np.concatenate([X_data[nr-test_nr:,[0], ...], target_data[nr-test_nr:, [0], ...]], axis = 1)\n",
        "test_y = target_data[nr-test_nr:,...]\n",
        "\n",
        "\n",
        "train_x = train_x.transpose(0,2,3,1)\n",
        "train_y = train_y[:,:,:,:,None]\n",
        "test_x = test_x.transpose(0,2,3,1)\n",
        "test_y = test_y[:,:,:,:,None]\n",
        "#test_y = test_y.transpose(0,2,3,1)\n",
        "print('train_x shape is ', train_x.shape)\n",
        "print('train_y shape is ', train_y.shape)\n",
        "print('test_x shape is ', test_x.shape)\n",
        "print('test_y shape is ', test_y.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape is  (2250, 100, 100, 2)\n",
            "train_y shape is  (2250, 10, 100, 100, 1)\n",
            "test_x shape is  (750, 100, 100, 2)\n",
            "test_y shape is  (750, 10, 100, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsuLGQpniCs"
      },
      "source": [
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/saved_models/'\n",
        "epochs = 300\n",
        "batch_size = 16\n",
        "num_batch = int(train_nr/batch_size) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EqIECRbnoTX"
      },
      "source": [
        "def vae_loss(x, t_decoded):\n",
        "    '''Total loss for the plain UAE'''\n",
        "    return K.mean(reconstruction_loss(x, t_decoded))\n",
        "\n",
        "\n",
        "def reconstruction_loss(x, t_decoded):\n",
        "    '''Reconstruction loss for the plain UAE'''\n",
        "\n",
        "    return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
        "\n",
        "def relative_error(x, t_decoded):\n",
        "    return K.mean(K.abs(x - t_decoded) / x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m46Y8eknsmA"
      },
      "source": [
        "opt = Adam(learning_rate=1e-4)\n",
        "vae_model.compile(loss = vae_loss, optimizer = opt, metrics = [relative_error])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lttghynSnzzT"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "lrScheduler = ReduceLROnPlateau(monitor = 'loss', factor = 0.5, patience = 15, cooldown = 1, verbose = 1, min_lr = 1e-6)\n",
        "filePath = 'saved-model-{epoch:03d}-{val_loss:.2f}.h5'\n",
        "checkPoint = ModelCheckpoint(filePath, monitor = 'val_loss', verbose = 1, save_best_only = False, \\\n",
        "                             save_weights_only = True, mode = 'auto', save_freq = 20)\n",
        "\n",
        "callbacks_list = [lrScheduler, checkPoint]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClmgwTn9n7Dv",
        "outputId": "e765d37d-b422-466d-ffde-eab276c34c57"
      },
      "source": [
        "history = vae_model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, \\\n",
        "                        verbose = 1, validation_data = (test_x, test_y))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "141/141 [==============================] - 67s 228ms/step - loss: 66388.1646 - relative_error: -2.1327 - val_loss: 14349.9482 - val_relative_error: -1.2721\n",
            "Epoch 2/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 1690.8959 - relative_error: -0.3449 - val_loss: 4058.1567 - val_relative_error: -0.6280\n",
            "Epoch 3/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 776.7769 - relative_error: -0.2255 - val_loss: 937.4716 - val_relative_error: -0.2652\n",
            "Epoch 4/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 551.7735 - relative_error: -0.1896 - val_loss: 738.5228 - val_relative_error: -0.1896\n",
            "Epoch 5/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 446.6452 - relative_error: -0.1705 - val_loss: 1052.2975 - val_relative_error: -0.1759\n",
            "Epoch 6/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 373.2837 - relative_error: -0.1557 - val_loss: 561.4374 - val_relative_error: -0.1581\n",
            "Epoch 7/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 326.0126 - relative_error: -0.1461 - val_loss: 524.6495 - val_relative_error: -0.1485\n",
            "Epoch 8/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 294.9877 - relative_error: -0.1394 - val_loss: 397.8847 - val_relative_error: -0.1461\n",
            "Epoch 9/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 275.5292 - relative_error: -0.1355 - val_loss: 257.7114 - val_relative_error: -0.1304\n",
            "Epoch 10/300\n",
            "141/141 [==============================] - 28s 195ms/step - loss: 242.1049 - relative_error: -0.1269 - val_loss: 285.8790 - val_relative_error: -0.1264\n",
            "Epoch 11/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 222.9824 - relative_error: -0.1230 - val_loss: 244.4081 - val_relative_error: -0.1202\n",
            "Epoch 12/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 202.6341 - relative_error: -0.1174 - val_loss: 209.5627 - val_relative_error: -0.1143\n",
            "Epoch 13/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 191.3992 - relative_error: -0.1150 - val_loss: 200.9563 - val_relative_error: -0.1109\n",
            "Epoch 14/300\n",
            "141/141 [==============================] - 27s 192ms/step - loss: 178.3626 - relative_error: -0.1110 - val_loss: 179.5079 - val_relative_error: -0.1071\n",
            "Epoch 15/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 163.1189 - relative_error: -0.1061 - val_loss: 170.2746 - val_relative_error: -0.1046\n",
            "Epoch 16/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 154.6919 - relative_error: -0.1041 - val_loss: 192.5861 - val_relative_error: -0.1031\n",
            "Epoch 17/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 142.9471 - relative_error: -0.0996 - val_loss: 613.8911 - val_relative_error: -0.1132\n",
            "Epoch 18/300\n",
            "141/141 [==============================] - 27s 194ms/step - loss: 142.5745 - relative_error: -0.1007 - val_loss: 180.6602 - val_relative_error: -0.1022\n",
            "Epoch 19/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 129.6635 - relative_error: -0.0954 - val_loss: 140.2157 - val_relative_error: -0.1001\n",
            "Epoch 20/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 125.7059 - relative_error: -0.0940 - val_loss: 141.4931 - val_relative_error: -0.1008\n",
            "Epoch 21/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 115.2004 - relative_error: -0.0897 - val_loss: 151.8701 - val_relative_error: -0.1082\n",
            "Epoch 22/300\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 111.0957 - relative_error: -0.0882 - val_loss: 1071.5436 - val_relative_error: -0.1397\n",
            "Epoch 23/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 175.1326 - relative_error: -0.1078 - val_loss: 132.1662 - val_relative_error: -0.0926\n",
            "Epoch 24/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 113.3763 - relative_error: -0.0889 - val_loss: 111.9556 - val_relative_error: -0.0903\n",
            "Epoch 25/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 103.4492 - relative_error: -0.0853 - val_loss: 143.7441 - val_relative_error: -0.1066\n",
            "Epoch 26/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 97.5939 - relative_error: -0.0830 - val_loss: 133.9928 - val_relative_error: -0.0943\n",
            "Epoch 27/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 94.6019 - relative_error: -0.0819 - val_loss: 106.5422 - val_relative_error: -0.0890\n",
            "Epoch 28/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 85.1901 - relative_error: -0.0770 - val_loss: 134.9935 - val_relative_error: -0.1050\n",
            "Epoch 29/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 81.3467 - relative_error: -0.0754 - val_loss: 108.0604 - val_relative_error: -0.0894\n",
            "Epoch 30/300\n",
            "141/141 [==============================] - 27s 195ms/step - loss: 78.3542 - relative_error: -0.0743 - val_loss: 96.2595 - val_relative_error: -0.0844\n",
            "Epoch 31/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 75.5929 - relative_error: -0.0732 - val_loss: 85.4419 - val_relative_error: -0.0783\n",
            "Epoch 32/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 69.4606 - relative_error: -0.0699 - val_loss: 70.7091 - val_relative_error: -0.0696\n",
            "Epoch 33/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 68.6125 - relative_error: -0.0698 - val_loss: 76.1896 - val_relative_error: -0.0739\n",
            "Epoch 34/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 63.7316 - relative_error: -0.0666 - val_loss: 83.9853 - val_relative_error: -0.0712\n",
            "Epoch 35/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 61.8278 - relative_error: -0.0661 - val_loss: 75.8923 - val_relative_error: -0.0733\n",
            "Epoch 36/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 60.4022 - relative_error: -0.0653 - val_loss: 73.7926 - val_relative_error: -0.0716\n",
            "Epoch 37/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 51.6889 - relative_error: -0.0601 - val_loss: 82.3935 - val_relative_error: -0.0764\n",
            "Epoch 38/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 52.4383 - relative_error: -0.0604 - val_loss: 80.2998 - val_relative_error: -0.0753\n",
            "Epoch 39/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 47.2017 - relative_error: -0.0575 - val_loss: 82.2396 - val_relative_error: -0.0776\n",
            "Epoch 40/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 43.5445 - relative_error: -0.0548 - val_loss: 58.2197 - val_relative_error: -0.0628\n",
            "Epoch 41/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 44.3323 - relative_error: -0.0558 - val_loss: 60.5779 - val_relative_error: -0.0659\n",
            "Epoch 42/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 41.2580 - relative_error: -0.0536 - val_loss: 40.2984 - val_relative_error: -0.0529\n",
            "Epoch 43/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 40.3552 - relative_error: -0.0526 - val_loss: 117.2188 - val_relative_error: -0.0754\n",
            "Epoch 44/300\n",
            "141/141 [==============================] - 28s 195ms/step - loss: 37.6262 - relative_error: -0.0512 - val_loss: 67.4077 - val_relative_error: -0.0691\n",
            "Epoch 45/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 33.2858 - relative_error: -0.0477 - val_loss: 40.5425 - val_relative_error: -0.0533\n",
            "Epoch 46/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 36.5679 - relative_error: -0.0505 - val_loss: 36.8831 - val_relative_error: -0.0508\n",
            "Epoch 47/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 48.3469 - relative_error: -0.0577 - val_loss: 62.6026 - val_relative_error: -0.0660\n",
            "Epoch 48/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 34.9414 - relative_error: -0.0496 - val_loss: 50.1680 - val_relative_error: -0.0620\n",
            "Epoch 49/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 32.9230 - relative_error: -0.0482 - val_loss: 37.0740 - val_relative_error: -0.0513\n",
            "Epoch 50/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 29.9338 - relative_error: -0.0456 - val_loss: 51.3247 - val_relative_error: -0.0612\n",
            "Epoch 51/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 29.6074 - relative_error: -0.0456 - val_loss: 31.6310 - val_relative_error: -0.0470\n",
            "Epoch 52/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 26.9427 - relative_error: -0.0429 - val_loss: 47.5286 - val_relative_error: -0.0596\n",
            "Epoch 53/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 25.8380 - relative_error: -0.0423 - val_loss: 73.0111 - val_relative_error: -0.0750\n",
            "Epoch 54/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 25.9510 - relative_error: -0.0425 - val_loss: 38.0864 - val_relative_error: -0.0525\n",
            "Epoch 55/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 30.4799 - relative_error: -0.0458 - val_loss: 82.0959 - val_relative_error: -0.0793\n",
            "Epoch 56/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 28.0205 - relative_error: -0.0442 - val_loss: 41.3709 - val_relative_error: -0.0554\n",
            "Epoch 57/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 27.0680 - relative_error: -0.0432 - val_loss: 46.1220 - val_relative_error: -0.0553\n",
            "Epoch 58/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 28.2937 - relative_error: -0.0445 - val_loss: 35.3562 - val_relative_error: -0.0515\n",
            "Epoch 59/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 24.4993 - relative_error: -0.0413 - val_loss: 25.8070 - val_relative_error: -0.0414\n",
            "Epoch 60/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 23.3888 - relative_error: -0.0402 - val_loss: 17.9828 - val_relative_error: -0.0348\n",
            "Epoch 61/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 21.8492 - relative_error: -0.0389 - val_loss: 37.6966 - val_relative_error: -0.0531\n",
            "Epoch 62/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 24.1067 - relative_error: -0.0412 - val_loss: 47.8932 - val_relative_error: -0.0589\n",
            "Epoch 63/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 23.7427 - relative_error: -0.0407 - val_loss: 43.9911 - val_relative_error: -0.0558\n",
            "Epoch 64/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 23.8259 - relative_error: -0.0410 - val_loss: 23.7919 - val_relative_error: -0.0401\n",
            "Epoch 65/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 24.6045 - relative_error: -0.0411 - val_loss: 24.1803 - val_relative_error: -0.0411\n",
            "Epoch 66/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 19.3052 - relative_error: -0.0364 - val_loss: 24.7797 - val_relative_error: -0.0420\n",
            "Epoch 67/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 18.7821 - relative_error: -0.0363 - val_loss: 36.4016 - val_relative_error: -0.0472\n",
            "Epoch 68/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 19.4012 - relative_error: -0.0367 - val_loss: 20.8699 - val_relative_error: -0.0385\n",
            "Epoch 69/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 17.7543 - relative_error: -0.0352 - val_loss: 215.3100 - val_relative_error: -0.0654\n",
            "Epoch 70/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 20.6321 - relative_error: -0.0375 - val_loss: 16.3223 - val_relative_error: -0.0329\n",
            "Epoch 71/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 19.4675 - relative_error: -0.0370 - val_loss: 23.0035 - val_relative_error: -0.0383\n",
            "Epoch 72/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 18.8972 - relative_error: -0.0365 - val_loss: 60.0340 - val_relative_error: -0.0712\n",
            "Epoch 73/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 17.5762 - relative_error: -0.0348 - val_loss: 20.2217 - val_relative_error: -0.0374\n",
            "Epoch 74/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 18.1928 - relative_error: -0.0357 - val_loss: 14.4321 - val_relative_error: -0.0313\n",
            "Epoch 75/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 15.1944 - relative_error: -0.0323 - val_loss: 29.6496 - val_relative_error: -0.0454\n",
            "Epoch 76/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 17.8313 - relative_error: -0.0356 - val_loss: 19.2400 - val_relative_error: -0.0382\n",
            "Epoch 77/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 16.4103 - relative_error: -0.0333 - val_loss: 47.6988 - val_relative_error: -0.0489\n",
            "Epoch 78/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 20.3328 - relative_error: -0.0373 - val_loss: 27.4598 - val_relative_error: -0.0454\n",
            "Epoch 79/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 19.5952 - relative_error: -0.0372 - val_loss: 27.3379 - val_relative_error: -0.0437\n",
            "Epoch 80/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 15.5283 - relative_error: -0.0328 - val_loss: 21.6028 - val_relative_error: -0.0389\n",
            "Epoch 81/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 13.9031 - relative_error: -0.0310 - val_loss: 23.6179 - val_relative_error: -0.0412\n",
            "Epoch 82/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 13.7931 - relative_error: -0.0310 - val_loss: 16.1343 - val_relative_error: -0.0341\n",
            "Epoch 83/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 12.7745 - relative_error: -0.0297 - val_loss: 26.0263 - val_relative_error: -0.0433\n",
            "Epoch 84/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.3014 - relative_error: -0.0290 - val_loss: 19.4261 - val_relative_error: -0.0369\n",
            "Epoch 85/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.3236 - relative_error: -0.0291 - val_loss: 22.5050 - val_relative_error: -0.0409\n",
            "Epoch 86/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.7997 - relative_error: -0.0296 - val_loss: 16.7631 - val_relative_error: -0.0342\n",
            "Epoch 87/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.1682 - relative_error: -0.0291 - val_loss: 15.5795 - val_relative_error: -0.0342\n",
            "Epoch 88/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 12.1767 - relative_error: -0.0290 - val_loss: 16.4604 - val_relative_error: -0.0344\n",
            "Epoch 89/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 11.5738 - relative_error: -0.0284 - val_loss: 13.4604 - val_relative_error: -0.0308\n",
            "Epoch 90/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.0100 - relative_error: -0.0286 - val_loss: 12.7069 - val_relative_error: -0.0294\n",
            "Epoch 91/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 11.7610 - relative_error: -0.0288 - val_loss: 15.6492 - val_relative_error: -0.0333\n",
            "Epoch 92/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 10.3615 - relative_error: -0.0264 - val_loss: 11.2265 - val_relative_error: -0.0277\n",
            "Epoch 93/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 10.2540 - relative_error: -0.0267 - val_loss: 25.5788 - val_relative_error: -0.0432\n",
            "Epoch 94/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 10.1840 - relative_error: -0.0265 - val_loss: 9.4577 - val_relative_error: -0.0254\n",
            "Epoch 95/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 10.4679 - relative_error: -0.0268 - val_loss: 12.6589 - val_relative_error: -0.0299\n",
            "Epoch 96/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 9.0045 - relative_error: -0.0249 - val_loss: 12.9539 - val_relative_error: -0.0289\n",
            "Epoch 97/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 8.9613 - relative_error: -0.0248 - val_loss: 14.8382 - val_relative_error: -0.0326\n",
            "Epoch 98/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 14.4694 - relative_error: -0.0314 - val_loss: 10.9831 - val_relative_error: -0.0268\n",
            "Epoch 99/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 9.3469 - relative_error: -0.0251 - val_loss: 11.3138 - val_relative_error: -0.0252\n",
            "Epoch 100/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 16.4986 - relative_error: -0.0316 - val_loss: 24.8622 - val_relative_error: -0.0366\n",
            "Epoch 101/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 17.4172 - relative_error: -0.0344 - val_loss: 18.6527 - val_relative_error: -0.0350\n",
            "Epoch 102/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 10.5786 - relative_error: -0.0269 - val_loss: 14.0545 - val_relative_error: -0.0305\n",
            "Epoch 103/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 9.7741 - relative_error: -0.0257 - val_loss: 10.7734 - val_relative_error: -0.0271\n",
            "Epoch 104/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 9.6826 - relative_error: -0.0260 - val_loss: 10.4975 - val_relative_error: -0.0263\n",
            "Epoch 105/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 9.1206 - relative_error: -0.0252 - val_loss: 16.0267 - val_relative_error: -0.0333\n",
            "Epoch 106/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 8.3663 - relative_error: -0.0240 - val_loss: 15.9840 - val_relative_error: -0.0337\n",
            "Epoch 107/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 8.0876 - relative_error: -0.0235 - val_loss: 8.9279 - val_relative_error: -0.0243\n",
            "Epoch 108/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 8.6887 - relative_error: -0.0247 - val_loss: 6.9656 - val_relative_error: -0.0212\n",
            "Epoch 109/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 7.8181 - relative_error: -0.0232 - val_loss: 8.7635 - val_relative_error: -0.0244\n",
            "Epoch 110/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 7.3485 - relative_error: -0.0226 - val_loss: 8.0460 - val_relative_error: -0.0226\n",
            "Epoch 111/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 7.2992 - relative_error: -0.0224 - val_loss: 9.1834 - val_relative_error: -0.0248\n",
            "Epoch 112/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 7.0462 - relative_error: -0.0220 - val_loss: 6.2723 - val_relative_error: -0.0199\n",
            "Epoch 113/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 6.6481 - relative_error: -0.0214 - val_loss: 7.2803 - val_relative_error: -0.0214\n",
            "Epoch 114/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.7737 - relative_error: -0.0214 - val_loss: 11.6242 - val_relative_error: -0.0287\n",
            "Epoch 115/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.6305 - relative_error: -0.0215 - val_loss: 8.4037 - val_relative_error: -0.0239\n",
            "Epoch 116/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 7.1737 - relative_error: -0.0223 - val_loss: 8.1559 - val_relative_error: -0.0235\n",
            "Epoch 117/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 7.2297 - relative_error: -0.0223 - val_loss: 8.5484 - val_relative_error: -0.0232\n",
            "Epoch 118/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.7994 - relative_error: -0.0215 - val_loss: 17.9091 - val_relative_error: -0.0363\n",
            "Epoch 119/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.5095 - relative_error: -0.0211 - val_loss: 8.3461 - val_relative_error: -0.0229\n",
            "Epoch 120/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 6.0924 - relative_error: -0.0205 - val_loss: 8.9971 - val_relative_error: -0.0248\n",
            "Epoch 121/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.4584 - relative_error: -0.0213 - val_loss: 10.5926 - val_relative_error: -0.0272\n",
            "Epoch 122/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.1738 - relative_error: -0.0205 - val_loss: 5.2902 - val_relative_error: -0.0184\n",
            "Epoch 123/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 6.1294 - relative_error: -0.0204 - val_loss: 14.2329 - val_relative_error: -0.0328\n",
            "Epoch 124/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.6220 - relative_error: -0.0197 - val_loss: 6.8216 - val_relative_error: -0.0210\n",
            "Epoch 125/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.7322 - relative_error: -0.0196 - val_loss: 7.6241 - val_relative_error: -0.0226\n",
            "Epoch 126/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.4689 - relative_error: -0.0194 - val_loss: 6.2498 - val_relative_error: -0.0199\n",
            "Epoch 127/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.0455 - relative_error: -0.0202 - val_loss: 8.7582 - val_relative_error: -0.0249\n",
            "Epoch 128/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 6.1172 - relative_error: -0.0207 - val_loss: 7.3346 - val_relative_error: -0.0222\n",
            "Epoch 129/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.3846 - relative_error: -0.0193 - val_loss: 13.5455 - val_relative_error: -0.0310\n",
            "Epoch 130/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.8502 - relative_error: -0.0201 - val_loss: 8.9947 - val_relative_error: -0.0254\n",
            "Epoch 131/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.7864 - relative_error: -0.0198 - val_loss: 5.6479 - val_relative_error: -0.0190\n",
            "Epoch 132/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.5783 - relative_error: -0.0198 - val_loss: 7.0784 - val_relative_error: -0.0214\n",
            "Epoch 133/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.2010 - relative_error: -0.0190 - val_loss: 8.7261 - val_relative_error: -0.0245\n",
            "Epoch 134/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.9318 - relative_error: -0.0183 - val_loss: 5.1669 - val_relative_error: -0.0171\n",
            "Epoch 135/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 5.0048 - relative_error: -0.0183 - val_loss: 7.1035 - val_relative_error: -0.0221\n",
            "Epoch 136/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.8898 - relative_error: -0.0182 - val_loss: 6.0858 - val_relative_error: -0.0198\n",
            "Epoch 137/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.2532 - relative_error: -0.0190 - val_loss: 5.6455 - val_relative_error: -0.0184\n",
            "Epoch 138/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.5183 - relative_error: -0.0174 - val_loss: 41.1049 - val_relative_error: -0.0233\n",
            "Epoch 139/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.6846 - relative_error: -0.0195 - val_loss: 5.8704 - val_relative_error: -0.0195\n",
            "Epoch 140/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.0544 - relative_error: -0.0187 - val_loss: 7.3062 - val_relative_error: -0.0222\n",
            "Epoch 141/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 5.0758 - relative_error: -0.0185 - val_loss: 6.7399 - val_relative_error: -0.0210\n",
            "Epoch 142/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.8428 - relative_error: -0.0182 - val_loss: 4.6243 - val_relative_error: -0.0164\n",
            "Epoch 143/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.9356 - relative_error: -0.0187 - val_loss: 7.8929 - val_relative_error: -0.0235\n",
            "Epoch 144/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.9145 - relative_error: -0.0184 - val_loss: 4.6281 - val_relative_error: -0.0172\n",
            "Epoch 145/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.2657 - relative_error: -0.0170 - val_loss: 4.4469 - val_relative_error: -0.0166\n",
            "Epoch 146/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 4.0126 - relative_error: -0.0165 - val_loss: 5.4451 - val_relative_error: -0.0191\n",
            "Epoch 147/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.5382 - relative_error: -0.0178 - val_loss: 5.6963 - val_relative_error: -0.0195\n",
            "Epoch 148/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.5766 - relative_error: -0.0175 - val_loss: 6.8281 - val_relative_error: -0.0213\n",
            "Epoch 149/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.0680 - relative_error: -0.0166 - val_loss: 6.2583 - val_relative_error: -0.0209\n",
            "Epoch 150/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.9743 - relative_error: -0.0163 - val_loss: 7.0966 - val_relative_error: -0.0218\n",
            "Epoch 151/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.3281 - relative_error: -0.0171 - val_loss: 5.0760 - val_relative_error: -0.0184\n",
            "Epoch 152/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.5757 - relative_error: -0.0177 - val_loss: 7.6642 - val_relative_error: -0.0225\n",
            "Epoch 153/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.0976 - relative_error: -0.0167 - val_loss: 6.6848 - val_relative_error: -0.0212\n",
            "Epoch 154/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 4.1037 - relative_error: -0.0166 - val_loss: 5.9322 - val_relative_error: -0.0191\n",
            "Epoch 155/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 18.7659 - relative_error: -0.0301 - val_loss: 25.6349 - val_relative_error: -0.0414\n",
            "Epoch 156/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 17.8931 - relative_error: -0.0343 - val_loss: 15.7429 - val_relative_error: -0.0322\n",
            "Epoch 157/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 11.4384 - relative_error: -0.0276 - val_loss: 161.2739 - val_relative_error: -0.1059\n",
            "Epoch 158/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 32.3570 - relative_error: -0.0457 - val_loss: 20.3306 - val_relative_error: -0.0369\n",
            "Epoch 159/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 18.8804 - relative_error: -0.0351 - val_loss: 14.0522 - val_relative_error: -0.0301\n",
            "Epoch 160/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 13.4219 - relative_error: -0.0297 - val_loss: 12.2174 - val_relative_error: -0.0285\n",
            "Epoch 161/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 10.0931 - relative_error: -0.0258 - val_loss: 9.2959 - val_relative_error: -0.0247\n",
            "Epoch 162/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 8.2352 - relative_error: -0.0237 - val_loss: 11.1604 - val_relative_error: -0.0274\n",
            "Epoch 163/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 6.7278 - relative_error: -0.0212 - val_loss: 5.9948 - val_relative_error: -0.0195\n",
            "Epoch 164/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 6.0686 - relative_error: -0.0201 - val_loss: 5.5815 - val_relative_error: -0.0191\n",
            "Epoch 165/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.5691 - relative_error: -0.0191 - val_loss: 5.5742 - val_relative_error: -0.0193\n",
            "Epoch 166/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.5722 - relative_error: -0.0194 - val_loss: 4.9001 - val_relative_error: -0.0176\n",
            "Epoch 167/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.4017 - relative_error: -0.0188 - val_loss: 4.4509 - val_relative_error: -0.0167\n",
            "Epoch 168/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.7573 - relative_error: -0.0177 - val_loss: 4.5620 - val_relative_error: -0.0169\n",
            "Epoch 169/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.1354 - relative_error: -0.0185 - val_loss: 4.9608 - val_relative_error: -0.0183\n",
            "Epoch 170/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.3040 - relative_error: -0.0169 - val_loss: 5.1513 - val_relative_error: -0.0183\n",
            "Epoch 171/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.8108 - relative_error: -0.0193 - val_loss: 6.1339 - val_relative_error: -0.0197\n",
            "Epoch 172/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.3953 - relative_error: -0.0171 - val_loss: 7.0153 - val_relative_error: -0.0213\n",
            "Epoch 173/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 5.0247 - relative_error: -0.0183 - val_loss: 4.1255 - val_relative_error: -0.0159\n",
            "Epoch 174/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.3553 - relative_error: -0.0171 - val_loss: 4.5799 - val_relative_error: -0.0175\n",
            "Epoch 175/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.8327 - relative_error: -0.0161 - val_loss: 3.8477 - val_relative_error: -0.0155\n",
            "Epoch 176/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.8097 - relative_error: -0.0159 - val_loss: 5.0408 - val_relative_error: -0.0178\n",
            "Epoch 177/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.0130 - relative_error: -0.0163 - val_loss: 4.9351 - val_relative_error: -0.0168\n",
            "Epoch 178/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.0783 - relative_error: -0.0164 - val_loss: 5.3361 - val_relative_error: -0.0190\n",
            "Epoch 179/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.7463 - relative_error: -0.0157 - val_loss: 4.1380 - val_relative_error: -0.0164\n",
            "Epoch 180/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.6898 - relative_error: -0.0156 - val_loss: 4.6961 - val_relative_error: -0.0172\n",
            "Epoch 181/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.6524 - relative_error: -0.0155 - val_loss: 4.8556 - val_relative_error: -0.0175\n",
            "Epoch 182/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.7089 - relative_error: -0.0158 - val_loss: 3.8823 - val_relative_error: -0.0157\n",
            "Epoch 183/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.3656 - relative_error: -0.0150 - val_loss: 4.3166 - val_relative_error: -0.0172\n",
            "Epoch 184/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.5215 - relative_error: -0.0179 - val_loss: 4.2774 - val_relative_error: -0.0167\n",
            "Epoch 185/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.7054 - relative_error: -0.0158 - val_loss: 3.3316 - val_relative_error: -0.0143\n",
            "Epoch 186/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.1539 - relative_error: -0.0143 - val_loss: 3.6164 - val_relative_error: -0.0151\n",
            "Epoch 187/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.6618 - relative_error: -0.0156 - val_loss: 6.0094 - val_relative_error: -0.0189\n",
            "Epoch 188/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.3863 - relative_error: -0.0150 - val_loss: 5.5928 - val_relative_error: -0.0196\n",
            "Epoch 189/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.4237 - relative_error: -0.0152 - val_loss: 3.5881 - val_relative_error: -0.0149\n",
            "Epoch 190/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.4059 - relative_error: -0.0151 - val_loss: 3.5125 - val_relative_error: -0.0147\n",
            "Epoch 191/300\n",
            "141/141 [==============================] - 28s 196ms/step - loss: 3.2799 - relative_error: -0.0149 - val_loss: 4.6202 - val_relative_error: -0.0172\n",
            "Epoch 192/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 2.9847 - relative_error: -0.0140 - val_loss: 3.8816 - val_relative_error: -0.0165\n",
            "Epoch 193/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.3774 - relative_error: -0.0151 - val_loss: 5.8510 - val_relative_error: -0.0205\n",
            "Epoch 194/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.2058 - relative_error: -0.0146 - val_loss: 3.5668 - val_relative_error: -0.0146\n",
            "Epoch 195/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.2323 - relative_error: -0.0148 - val_loss: 4.9307 - val_relative_error: -0.0172\n",
            "Epoch 196/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.1549 - relative_error: -0.0147 - val_loss: 3.7740 - val_relative_error: -0.0150\n",
            "Epoch 197/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.0144 - relative_error: -0.0143 - val_loss: 5.5220 - val_relative_error: -0.0191\n",
            "Epoch 198/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.0965 - relative_error: -0.0143 - val_loss: 4.3613 - val_relative_error: -0.0152\n",
            "Epoch 199/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.0237 - relative_error: -0.0143 - val_loss: 3.4845 - val_relative_error: -0.0147\n",
            "Epoch 200/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.1709 - relative_error: -0.0146 - val_loss: 3.7403 - val_relative_error: -0.0155\n",
            "Epoch 201/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.4409 - relative_error: -0.0154 - val_loss: 4.3866 - val_relative_error: -0.0171\n",
            "Epoch 202/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.1056 - relative_error: -0.0144 - val_loss: 3.3621 - val_relative_error: -0.0145\n",
            "Epoch 203/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.1446 - relative_error: -0.0146 - val_loss: 4.5739 - val_relative_error: -0.0174\n",
            "Epoch 204/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 2.7352 - relative_error: -0.0135 - val_loss: 5.9046 - val_relative_error: -0.0203\n",
            "Epoch 205/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.0072 - relative_error: -0.0141 - val_loss: 5.8520 - val_relative_error: -0.0192\n",
            "Epoch 206/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.6472 - relative_error: -0.0159 - val_loss: 4.8142 - val_relative_error: -0.0175\n",
            "Epoch 207/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.1037 - relative_error: -0.0145 - val_loss: 4.2995 - val_relative_error: -0.0163\n",
            "Epoch 208/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.7807 - relative_error: -0.0137 - val_loss: 3.9289 - val_relative_error: -0.0159\n",
            "Epoch 209/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.6721 - relative_error: -0.0132 - val_loss: 2.9052 - val_relative_error: -0.0133\n",
            "Epoch 210/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.7257 - relative_error: -0.0137 - val_loss: 3.3338 - val_relative_error: -0.0146\n",
            "Epoch 211/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.6173 - relative_error: -0.0131 - val_loss: 2.9328 - val_relative_error: -0.0134\n",
            "Epoch 212/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.2913 - relative_error: -0.0150 - val_loss: 3.5462 - val_relative_error: -0.0151\n",
            "Epoch 213/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.6586 - relative_error: -0.0133 - val_loss: 4.7806 - val_relative_error: -0.0168\n",
            "Epoch 214/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.1054 - relative_error: -0.0144 - val_loss: 5.4391 - val_relative_error: -0.0192\n",
            "Epoch 215/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 3.1365 - relative_error: -0.0146 - val_loss: 3.4478 - val_relative_error: -0.0146\n",
            "Epoch 216/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.7445 - relative_error: -0.0135 - val_loss: 4.5859 - val_relative_error: -0.0169\n",
            "Epoch 217/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 2.6360 - relative_error: -0.0133 - val_loss: 5.6390 - val_relative_error: -0.0168\n",
            "Epoch 218/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 3.4554 - relative_error: -0.0152 - val_loss: 170.1911 - val_relative_error: -0.0569\n",
            "Epoch 219/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 12.1787 - relative_error: -0.0269 - val_loss: 7.1919 - val_relative_error: -0.0216\n",
            "Epoch 220/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 4.0678 - relative_error: -0.0166 - val_loss: 4.3888 - val_relative_error: -0.0170\n",
            "Epoch 221/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 3.0861 - relative_error: -0.0142 - val_loss: 3.6258 - val_relative_error: -0.0148\n",
            "Epoch 222/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.7821 - relative_error: -0.0136 - val_loss: 3.4955 - val_relative_error: -0.0147\n",
            "Epoch 223/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.8675 - relative_error: -0.0139 - val_loss: 3.2040 - val_relative_error: -0.0139\n",
            "Epoch 224/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.5924 - relative_error: -0.0131 - val_loss: 3.1096 - val_relative_error: -0.0138\n",
            "Epoch 225/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.6591 - relative_error: -0.0133 - val_loss: 3.4107 - val_relative_error: -0.0143\n",
            "Epoch 226/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.7304 - relative_error: -0.0136 - val_loss: 3.3350 - val_relative_error: -0.0145\n",
            "Epoch 227/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.6179 - relative_error: -0.0133 - val_loss: 3.2755 - val_relative_error: -0.0143\n",
            "Epoch 228/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.7832 - relative_error: -0.0137 - val_loss: 3.6482 - val_relative_error: -0.0150\n",
            "Epoch 229/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.4631 - relative_error: -0.0130 - val_loss: 3.7504 - val_relative_error: -0.0155\n",
            "Epoch 230/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.7882 - relative_error: -0.0138 - val_loss: 4.2581 - val_relative_error: -0.0166\n",
            "Epoch 231/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.4110 - relative_error: -0.0126 - val_loss: 3.0662 - val_relative_error: -0.0139\n",
            "Epoch 232/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.3891 - relative_error: -0.0126 - val_loss: 3.3283 - val_relative_error: -0.0148\n",
            "Epoch 233/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.3245 - relative_error: -0.0124 - val_loss: 3.0681 - val_relative_error: -0.0133\n",
            "Epoch 234/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.6741 - relative_error: -0.0134 - val_loss: 2.8902 - val_relative_error: -0.0133\n",
            "Epoch 235/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.2827 - relative_error: -0.0124 - val_loss: 3.2888 - val_relative_error: -0.0141\n",
            "Epoch 236/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.3489 - relative_error: -0.0126 - val_loss: 3.6898 - val_relative_error: -0.0155\n",
            "Epoch 237/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.7508 - relative_error: -0.0135 - val_loss: 3.8384 - val_relative_error: -0.0157\n",
            "Epoch 238/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 2.6566 - relative_error: -0.0135 - val_loss: 6.4685 - val_relative_error: -0.0213\n",
            "Epoch 239/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.2608 - relative_error: -0.0122 - val_loss: 3.3663 - val_relative_error: -0.0143\n",
            "Epoch 240/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.1799 - relative_error: -0.0119 - val_loss: 3.4282 - val_relative_error: -0.0141\n",
            "Epoch 241/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.3828 - relative_error: -0.0128 - val_loss: 3.1440 - val_relative_error: -0.0144\n",
            "Epoch 242/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.1582 - relative_error: -0.0120 - val_loss: 4.7446 - val_relative_error: -0.0181\n",
            "Epoch 243/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.5497 - relative_error: -0.0130 - val_loss: 3.1889 - val_relative_error: -0.0141\n",
            "Epoch 244/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.3162 - relative_error: -0.0125 - val_loss: 2.5165 - val_relative_error: -0.0121\n",
            "Epoch 245/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.3925 - relative_error: -0.0127 - val_loss: 2.9392 - val_relative_error: -0.0133\n",
            "Epoch 246/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.9371 - relative_error: -0.0112 - val_loss: 4.0397 - val_relative_error: -0.0161\n",
            "Epoch 247/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.1419 - relative_error: -0.0120 - val_loss: 4.4484 - val_relative_error: -0.0172\n",
            "Epoch 248/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.4508 - relative_error: -0.0128 - val_loss: 3.0901 - val_relative_error: -0.0133\n",
            "Epoch 249/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.3361 - relative_error: -0.0125 - val_loss: 3.0583 - val_relative_error: -0.0133\n",
            "Epoch 250/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.7774 - relative_error: -0.0139 - val_loss: 3.6428 - val_relative_error: -0.0151\n",
            "Epoch 251/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.2627 - relative_error: -0.0123 - val_loss: 2.9448 - val_relative_error: -0.0130\n",
            "Epoch 252/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.1295 - relative_error: -0.0119 - val_loss: 2.9594 - val_relative_error: -0.0133\n",
            "Epoch 253/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.1545 - relative_error: -0.0121 - val_loss: 4.2056 - val_relative_error: -0.0161\n",
            "Epoch 254/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.9572 - relative_error: -0.0114 - val_loss: 3.6909 - val_relative_error: -0.0152\n",
            "Epoch 255/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.0900 - relative_error: -0.0117 - val_loss: 2.5897 - val_relative_error: -0.0123\n",
            "Epoch 256/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.1941 - relative_error: -0.0120 - val_loss: 2.5860 - val_relative_error: -0.0124\n",
            "Epoch 257/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.2214 - relative_error: -0.0123 - val_loss: 2.5716 - val_relative_error: -0.0125\n",
            "Epoch 258/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.7937 - relative_error: -0.0140 - val_loss: 3.0747 - val_relative_error: -0.0139\n",
            "Epoch 259/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.0280 - relative_error: -0.0117 - val_loss: 2.9385 - val_relative_error: -0.0129\n",
            "Epoch 260/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.9557 - relative_error: -0.0113 - val_loss: 2.5645 - val_relative_error: -0.0122\n",
            "Epoch 261/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.0229 - relative_error: -0.0117 - val_loss: 3.3152 - val_relative_error: -0.0141\n",
            "Epoch 262/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.2714 - relative_error: -0.0124 - val_loss: 2.7414 - val_relative_error: -0.0132\n",
            "Epoch 263/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 2.4690 - relative_error: -0.0128 - val_loss: 3.5663 - val_relative_error: -0.0141\n",
            "Epoch 264/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.0563 - relative_error: -0.0117 - val_loss: 2.7809 - val_relative_error: -0.0133\n",
            "Epoch 265/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.2659 - relative_error: -0.0124 - val_loss: 2.7648 - val_relative_error: -0.0129\n",
            "Epoch 266/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 2.0663 - relative_error: -0.0118 - val_loss: 2.6027 - val_relative_error: -0.0123\n",
            "Epoch 267/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.8910 - relative_error: -0.0111 - val_loss: 2.5327 - val_relative_error: -0.0127\n",
            "Epoch 268/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7706 - relative_error: -0.0109 - val_loss: 2.9064 - val_relative_error: -0.0137\n",
            "Epoch 269/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 1.8317 - relative_error: -0.0110 - val_loss: 2.9049 - val_relative_error: -0.0130\n",
            "Epoch 270/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7309 - relative_error: -0.0107 - val_loss: 3.1652 - val_relative_error: -0.0140\n",
            "Epoch 271/300\n",
            "141/141 [==============================] - 28s 200ms/step - loss: 1.9196 - relative_error: -0.0112 - val_loss: 3.2053 - val_relative_error: -0.0137\n",
            "Epoch 272/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.9138 - relative_error: -0.0112 - val_loss: 3.5359 - val_relative_error: -0.0147\n",
            "Epoch 273/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7665 - relative_error: -0.0108 - val_loss: 3.6704 - val_relative_error: -0.0153\n",
            "Epoch 274/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 2.4066 - relative_error: -0.0129 - val_loss: 2.9397 - val_relative_error: -0.0133\n",
            "Epoch 275/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.5928 - relative_error: -0.0102 - val_loss: 2.9615 - val_relative_error: -0.0134\n",
            "Epoch 276/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.8930 - relative_error: -0.0112 - val_loss: 3.5732 - val_relative_error: -0.0146\n",
            "Epoch 277/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.6307 - relative_error: -0.0104 - val_loss: 3.0501 - val_relative_error: -0.0136\n",
            "Epoch 278/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.7392 - relative_error: -0.0108 - val_loss: 2.6291 - val_relative_error: -0.0126\n",
            "Epoch 279/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.6857 - relative_error: -0.0106 - val_loss: 3.8744 - val_relative_error: -0.0154\n",
            "Epoch 280/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.8926 - relative_error: -0.0113 - val_loss: 2.4888 - val_relative_error: -0.0123\n",
            "Epoch 281/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.8533 - relative_error: -0.0112 - val_loss: 2.3174 - val_relative_error: -0.0118\n",
            "Epoch 282/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.6565 - relative_error: -0.0104 - val_loss: 2.3795 - val_relative_error: -0.0117\n",
            "Epoch 283/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.6502 - relative_error: -0.0102 - val_loss: 3.9627 - val_relative_error: -0.0161\n",
            "Epoch 284/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.8500 - relative_error: -0.0112 - val_loss: 2.7851 - val_relative_error: -0.0134\n",
            "Epoch 285/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.7363 - relative_error: -0.0107 - val_loss: 2.4429 - val_relative_error: -0.0125\n",
            "Epoch 286/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7655 - relative_error: -0.0108 - val_loss: 2.5168 - val_relative_error: -0.0125\n",
            "Epoch 287/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.7788 - relative_error: -0.0110 - val_loss: 3.6863 - val_relative_error: -0.0150\n",
            "Epoch 288/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.5995 - relative_error: -0.0103 - val_loss: 2.4748 - val_relative_error: -0.0120\n",
            "Epoch 289/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.5485 - relative_error: -0.0101 - val_loss: 2.6095 - val_relative_error: -0.0132\n",
            "Epoch 290/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7383 - relative_error: -0.0107 - val_loss: 2.7581 - val_relative_error: -0.0129\n",
            "Epoch 291/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.4801 - relative_error: -0.0098 - val_loss: 3.0526 - val_relative_error: -0.0133\n",
            "Epoch 292/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.8110 - relative_error: -0.0109 - val_loss: 3.4240 - val_relative_error: -0.0143\n",
            "Epoch 293/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.4984 - relative_error: -0.0099 - val_loss: 2.7414 - val_relative_error: -0.0124\n",
            "Epoch 294/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.5493 - relative_error: -0.0101 - val_loss: 2.3108 - val_relative_error: -0.0114\n",
            "Epoch 295/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.7619 - relative_error: -0.0109 - val_loss: 2.8036 - val_relative_error: -0.0130\n",
            "Epoch 296/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.8152 - relative_error: -0.0112 - val_loss: 3.5513 - val_relative_error: -0.0152\n",
            "Epoch 297/300\n",
            "141/141 [==============================] - 28s 199ms/step - loss: 1.7661 - relative_error: -0.0109 - val_loss: 2.8331 - val_relative_error: -0.0132\n",
            "Epoch 298/300\n",
            "141/141 [==============================] - 28s 197ms/step - loss: 1.6792 - relative_error: -0.0105 - val_loss: 2.5269 - val_relative_error: -0.0121\n",
            "Epoch 299/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.5932 - relative_error: -0.0103 - val_loss: 2.2620 - val_relative_error: -0.0112\n",
            "Epoch 300/300\n",
            "141/141 [==============================] - 28s 198ms/step - loss: 1.4587 - relative_error: -0.0098 - val_loss: 2.4587 - val_relative_error: -0.0121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "f5j_VWLCPY0H",
        "outputId": "a80477aa-f8fb-4471-a93c-cf51766d3e5a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(np.abs(history.history['relative_error']))\n",
        "plt.plot(np.abs(history.history['val_relative_error']))\n",
        "plt.title('model relative error')\n",
        "plt.ylabel('relative error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/MyDrive/Gridsearch_images/pressure_lr1-4_300epochs_adam_batch16.png')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'relative_error', 'val_loss', 'val_relative_error'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPXacBAry0S",
        "outputId": "3bb6186a-7ab0-4916-f6d0-709db325de5d"
      },
      "source": [
        "#vae_model.save(\"/content/gdrive/MyDrive/saved_models/pressure_le1-2.h5\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}